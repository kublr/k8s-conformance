I1123 11:25:40.459870      23 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-710016297
I1123 11:25:40.459893      23 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1123 11:25:40.460055      23 e2e.go:129] Starting e2e run "46cb2407-ea3a-471f-9b5e-c3c1adc3b604" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1606130738 - Will randomize all specs
Will run 305 of 5233 specs

Nov 23 11:25:40.476: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 11:25:40.479: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 23 11:25:40.505: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 23 11:25:40.557: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 23 11:25:40.557: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Nov 23 11:25:40.557: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 23 11:25:40.572: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Nov 23 11:25:40.572: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Nov 23 11:25:40.572: INFO: e2e test version: v1.19.3
Nov 23 11:25:40.574: INFO: kube-apiserver version: v1.19.3
Nov 23 11:25:40.574: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 11:25:40.580: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:25:40.582: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
Nov 23 11:25:40.613: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Nov 23 11:25:40.619: INFO: PSP annotation exists on dry run pod: "e2e-test-privileged-psp"; assuming PodSecurityPolicy is enabled
Nov 23 11:25:40.625: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 23 11:25:42.762: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6121 PodName:var-expansion-116acff4-4589-4f4e-b148-e2f77c187abe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 11:25:42.762: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: test for file in mounted path
Nov 23 11:25:42.884: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6121 PodName:var-expansion-116acff4-4589-4f4e-b148-e2f77c187abe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 11:25:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: updating the annotation value
Nov 23 11:25:43.508: INFO: Successfully updated pod "var-expansion-116acff4-4589-4f4e-b148-e2f77c187abe"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 23 11:25:43.513: INFO: Deleting pod "var-expansion-116acff4-4589-4f4e-b148-e2f77c187abe" in namespace "var-expansion-6121"
Nov 23 11:25:43.519: INFO: Wait up to 5m0s for pod "var-expansion-116acff4-4589-4f4e-b148-e2f77c187abe" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:26:17.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6121" for this suite.

• [SLOW TEST:36.965 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":1,"skipped":23,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:26:17.547: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5787
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:26:17.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3" in namespace "downward-api-5787" to be "Succeeded or Failed"
Nov 23 11:26:17.714: INFO: Pod "downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650007ms
Nov 23 11:26:19.718: INFO: Pod "downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009046521s
STEP: Saw pod success
Nov 23 11:26:19.718: INFO: Pod "downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3" satisfied condition "Succeeded or Failed"
Nov 23 11:26:19.722: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3 container client-container: <nil>
STEP: delete the pod
Nov 23 11:26:19.760: INFO: Waiting for pod downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3 to disappear
Nov 23 11:26:19.764: INFO: Pod downwardapi-volume-ed96421e-2c26-4e8b-a34f-169a972c9df3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:26:19.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5787" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":50,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:26:19.782: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:26:19.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6666" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":3,"skipped":54,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:26:19.931: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 23 11:26:20.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42121 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:26:20.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42121 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 23 11:26:30.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42177 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:26:30.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42177 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 23 11:26:40.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42207 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:26:40.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42207 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 23 11:26:50.126: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42234 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:26:50.127: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-a b317704a-c734-49fa-95b8-8e9d41f731fd 42234 0 2020-11-23 11:26:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-11-23 11:26:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 23 11:27:00.136: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-b 1440989b-e827-481f-84d8-adf1715ceb7b 42263 0 2020-11-23 11:27:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-23 11:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:27:00.137: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-b 1440989b-e827-481f-84d8-adf1715ceb7b 42263 0 2020-11-23 11:27:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-23 11:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 23 11:27:10.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-b 1440989b-e827-481f-84d8-adf1715ceb7b 42293 0 2020-11-23 11:27:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-23 11:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:27:10.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6802 /api/v1/namespaces/watch-6802/configmaps/e2e-watch-test-configmap-b 1440989b-e827-481f-84d8-adf1715ceb7b 42293 0 2020-11-23 11:27:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-11-23 11:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:27:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6802" for this suite.

• [SLOW TEST:60.228 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":4,"skipped":85,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:27:20.159: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9340
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9274
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:27:26.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-256" for this suite.
STEP: Destroying namespace "nsdeletetest-9340" for this suite.
Nov 23 11:27:26.660: INFO: Namespace nsdeletetest-9340 was already deleted
STEP: Destroying namespace "nsdeletetest-9274" for this suite.

• [SLOW TEST:6.507 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":5,"skipped":86,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:27:26.666: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-45cd853e-7e34-4655-a412-e12ee4b1f2e1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:27:26.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9268" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":6,"skipped":95,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:27:26.822: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 23 11:27:26.983: INFO: Waiting up to 5m0s for pod "pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de" in namespace "emptydir-5762" to be "Succeeded or Failed"
Nov 23 11:27:26.987: INFO: Pod "pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129293ms
Nov 23 11:27:28.991: INFO: Pod "pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008380444s
STEP: Saw pod success
Nov 23 11:27:28.992: INFO: Pod "pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de" satisfied condition "Succeeded or Failed"
Nov 23 11:27:28.996: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de container test-container: <nil>
STEP: delete the pod
Nov 23 11:27:29.017: INFO: Waiting for pod pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de to disappear
Nov 23 11:27:29.021: INFO: Pod pod-a64f73e7-a64f-4dd8-be34-aa8b586ec1de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:27:29.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5762" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":107,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:27:29.031: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5412
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-c65c55e7-d520-48c1-a532-9b1077638c6e
STEP: Creating secret with name s-test-opt-upd-f8d52fd9-d783-453a-91a4-9a9215f42a5b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c65c55e7-d520-48c1-a532-9b1077638c6e
STEP: Updating secret s-test-opt-upd-f8d52fd9-d783-453a-91a4-9a9215f42a5b
STEP: Creating secret with name s-test-opt-create-39ca99ab-6ed8-40e8-bb02-6bcd3e6ba0cd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:27:33.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5412" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":110,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:27:33.359: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-rh2g
STEP: Creating a pod to test atomic-volume-subpath
Nov 23 11:27:33.541: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rh2g" in namespace "subpath-8159" to be "Succeeded or Failed"
Nov 23 11:27:33.546: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Pending", Reason="", readiness=false. Elapsed: 5.502878ms
Nov 23 11:27:35.551: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 2.010521242s
Nov 23 11:27:37.556: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 4.014999226s
Nov 23 11:27:39.560: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 6.01924899s
Nov 23 11:27:41.563: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 8.022593s
Nov 23 11:27:43.567: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 10.026365515s
Nov 23 11:27:45.571: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 12.030095709s
Nov 23 11:27:47.575: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 14.034596012s
Nov 23 11:27:49.579: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 16.038259188s
Nov 23 11:27:51.583: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 18.042574508s
Nov 23 11:27:53.588: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Running", Reason="", readiness=true. Elapsed: 20.047455326s
Nov 23 11:27:55.592: INFO: Pod "pod-subpath-test-configmap-rh2g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050862855s
STEP: Saw pod success
Nov 23 11:27:55.592: INFO: Pod "pod-subpath-test-configmap-rh2g" satisfied condition "Succeeded or Failed"
Nov 23 11:27:55.594: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod pod-subpath-test-configmap-rh2g container test-container-subpath-configmap-rh2g: <nil>
STEP: delete the pod
Nov 23 11:27:55.648: INFO: Waiting for pod pod-subpath-test-configmap-rh2g to disappear
Nov 23 11:27:55.651: INFO: Pod pod-subpath-test-configmap-rh2g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rh2g
Nov 23 11:27:55.651: INFO: Deleting pod "pod-subpath-test-configmap-rh2g" in namespace "subpath-8159"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:27:55.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8159" for this suite.

• [SLOW TEST:22.309 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":9,"skipped":121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:27:55.669: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6452
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 23 11:27:55.820: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 11:28:00.873: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:18.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6452" for this suite.

• [SLOW TEST:22.649 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":10,"skipped":158,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:18.319: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3213
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-1b7f3ae0-b098-41a1-aac6-6f11ef2634e9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:20.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3213" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":11,"skipped":177,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:20.529: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Nov 23 11:28:20.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 api-versions'
Nov 23 11:28:20.770: INFO: stderr: ""
Nov 23 11:28:20.770: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncore.kublr.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nfeature.crd.kublr.com/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:20.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-130" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":12,"skipped":201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:20.781: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-55b6fb96-17d0-4544-ab93-fe55eb0851f2
STEP: Creating a pod to test consume secrets
Nov 23 11:28:20.938: INFO: Waiting up to 5m0s for pod "pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4" in namespace "secrets-4615" to be "Succeeded or Failed"
Nov 23 11:28:20.945: INFO: Pod "pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.27089ms
Nov 23 11:28:22.952: INFO: Pod "pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013337558s
STEP: Saw pod success
Nov 23 11:28:22.952: INFO: Pod "pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4" satisfied condition "Succeeded or Failed"
Nov 23 11:28:22.960: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4 container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 11:28:22.991: INFO: Waiting for pod pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4 to disappear
Nov 23 11:28:22.996: INFO: Pod pod-secrets-8638be9c-8630-49d1-9fba-8e29d2eae0f4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:22.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4615" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:23.008: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-cb714ec6-d2d3-4d91-9886-7fdb8666ea43
STEP: Creating a pod to test consume configMaps
Nov 23 11:28:23.190: INFO: Waiting up to 5m0s for pod "pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50" in namespace "configmap-9339" to be "Succeeded or Failed"
Nov 23 11:28:23.195: INFO: Pod "pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50": Phase="Pending", Reason="", readiness=false. Elapsed: 5.227476ms
Nov 23 11:28:25.201: INFO: Pod "pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010723524s
STEP: Saw pod success
Nov 23 11:28:25.201: INFO: Pod "pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50" satisfied condition "Succeeded or Failed"
Nov 23 11:28:25.206: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 11:28:25.237: INFO: Waiting for pod pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50 to disappear
Nov 23 11:28:25.243: INFO: Pod pod-configmaps-91823b3e-e655-47ae-ac8a-018a2e738a50 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:25.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9339" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":261,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:25.268: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:28:25.430: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 23 11:28:30.443: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 23 11:28:30.443: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 23 11:28:30.474: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2197 /apis/apps/v1/namespaces/deployment-2197/deployments/test-cleanup-deployment 10615413-725e-4a91-8b33-4221bed381d5 42824 1 2020-11-23 11:28:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-11-23 11:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b57a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 23 11:28:30.480: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-2197 /apis/apps/v1/namespaces/deployment-2197/replicasets/test-cleanup-deployment-5d446bdd47 c6c417e3-0966-4c57-83ea-82702ec7aa78 42826 1 2020-11-23 11:28:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 10615413-725e-4a91-8b33-4221bed381d5 0xc00704e357 0xc00704e358}] []  [{kube-controller-manager Update apps/v1 2020-11-23 11:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10615413-725e-4a91-8b33-4221bed381d5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00704e4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 11:28:30.480: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 23 11:28:30.480: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2197 /apis/apps/v1/namespaces/deployment-2197/replicasets/test-cleanup-controller a41869b2-0d0c-4d93-98e7-106663b17ae3 42825 1 2020-11-23 11:28:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 10615413-725e-4a91-8b33-4221bed381d5 0xc00704e187 0xc00704e188}] []  [{e2e.test Update apps/v1 2020-11-23 11:28:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 11:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"10615413-725e-4a91-8b33-4221bed381d5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00704e2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 23 11:28:30.490: INFO: Pod "test-cleanup-controller-fplph" is available:
&Pod{ObjectMeta:{test-cleanup-controller-fplph test-cleanup-controller- deployment-2197 /api/v1/namespaces/deployment-2197/pods/test-cleanup-controller-fplph 8d7787dd-f948-416e-91ea-9ad97da6ddf3 42800 0 2020-11-23 11:28:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:100.96.1.33/32 cni.projectcalico.org/podIPs:100.96.1.33/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller a41869b2-0d0c-4d93-98e7-106663b17ae3 0xc00704ef67 0xc00704ef68}] []  [{kube-controller-manager Update v1 2020-11-23 11:28:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a41869b2-0d0c-4d93-98e7-106663b17ae3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 11:28:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 11:28:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c5q8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c5q8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c5q8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:28:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:28:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:28:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:28:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:100.96.1.33,StartTime:2020-11-23 11:28:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 11:28:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6276859a1889096f50535d8918568e78eb963dad333bb34d7210629e94a15641,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 11:28:30.490: INFO: Pod "test-cleanup-deployment-5d446bdd47-qh4db" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-qh4db test-cleanup-deployment-5d446bdd47- deployment-2197 /api/v1/namespaces/deployment-2197/pods/test-cleanup-deployment-5d446bdd47-qh4db 5112af69-dc6e-4576-aac8-8f23fa938693 42828 0 2020-11-23 11:28:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 c6c417e3-0966-4c57-83ea-82702ec7aa78 0xc00704f177 0xc00704f178}] []  [{kube-controller-manager Update v1 2020-11-23 11:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c417e3-0966-4c57-83ea-82702ec7aa78\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c5q8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c5q8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c5q8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:30.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2197" for this suite.

• [SLOW TEST:5.248 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":15,"skipped":262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:30.517: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-6503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 23 11:28:30.722: INFO: starting watch
STEP: patching
STEP: updating
Nov 23 11:28:30.740: INFO: waiting for watch events with expected annotations
Nov 23 11:28:30.740: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:30.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6503" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":16,"skipped":309,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:30.774: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 23 11:28:34.982: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 23 11:28:34.990: INFO: Pod pod-with-poststart-http-hook still exists
Nov 23 11:28:36.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 23 11:28:36.995: INFO: Pod pod-with-poststart-http-hook still exists
Nov 23 11:28:38.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 23 11:28:38.994: INFO: Pod pod-with-poststart-http-hook still exists
Nov 23 11:28:40.990: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 23 11:28:40.994: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:40.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3528" for this suite.

• [SLOW TEST:10.232 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":315,"failed":0}
SS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:41.006: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Nov 23 11:28:41.169: INFO: created test-event-1
Nov 23 11:28:41.174: INFO: created test-event-2
Nov 23 11:28:41.178: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 23 11:28:41.181: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 23 11:28:41.197: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:28:41.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1953" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":18,"skipped":317,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:28:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6443
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6443
Nov 23 11:28:43.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 23 11:28:43.740: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 23 11:28:43.740: INFO: stdout: "iptables"
Nov 23 11:28:43.740: INFO: proxyMode: iptables
Nov 23 11:28:43.748: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:28:43.751: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:28:45.751: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:28:45.755: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:28:47.751: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:28:47.755: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:28:49.751: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:28:49.754: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6443
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6443
I1123 11:28:49.779297      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6443, replica count: 3
I1123 11:28:52.829698      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 11:28:52.846: INFO: Creating new exec pod
Nov 23 11:28:55.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov 23 11:28:58.165: INFO: rc: 1
Nov 23 11:28:58.165: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-timeout 80
nc: connect to affinity-nodeport-timeout port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 11:28:59.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov 23 11:29:01.385: INFO: rc: 1
Nov 23 11:29:01.385: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-timeout 80
nc: connect to affinity-nodeport-timeout port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 11:29:02.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Nov 23 11:29:02.425: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 23 11:29:02.425: INFO: stdout: ""
Nov 23 11:29:02.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 100.69.140.209 80'
Nov 23 11:29:02.673: INFO: stderr: "+ nc -zv -t -w 2 100.69.140.209 80\nConnection to 100.69.140.209 80 port [tcp/http] succeeded!\n"
Nov 23 11:29:02.673: INFO: stdout: ""
Nov 23 11:29:02.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 31416'
Nov 23 11:29:02.919: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 31416\nConnection to 192.168.8.54 31416 port [tcp/31416] succeeded!\n"
Nov 23 11:29:02.919: INFO: stdout: ""
Nov 23 11:29:02.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 31416'
Nov 23 11:29:03.195: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 31416\nConnection to 192.168.8.34 31416 port [tcp/31416] succeeded!\n"
Nov 23 11:29:03.195: INFO: stdout: ""
Nov 23 11:29:03.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 31416'
Nov 23 11:29:03.451: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 31416\nConnection to 192.168.8.54 31416 port [tcp/31416] succeeded!\n"
Nov 23 11:29:03.451: INFO: stdout: ""
Nov 23 11:29:03.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 31416'
Nov 23 11:29:03.689: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 31416\nConnection to 192.168.8.34 31416 port [tcp/31416] succeeded!\n"
Nov 23 11:29:03.689: INFO: stdout: ""
Nov 23 11:29:03.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.54:31416/ ; done'
Nov 23 11:29:04.026: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n"
Nov 23 11:29:04.026: INFO: stdout: "\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx\naffinity-nodeport-timeout-qvrhx"
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Received response from host: affinity-nodeport-timeout-qvrhx
Nov 23 11:29:04.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.8.54:31416/'
Nov 23 11:29:04.267: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n"
Nov 23 11:29:04.267: INFO: stdout: "affinity-nodeport-timeout-qvrhx"
Nov 23 11:29:19.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6443 execpod-affinityqqp5c -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.8.54:31416/'
Nov 23 11:29:19.500: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.8.54:31416/\n"
Nov 23 11:29:19.500: INFO: stdout: "affinity-nodeport-timeout-69fkk"
Nov 23 11:29:19.500: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6443, will wait for the garbage collector to delete the pods
Nov 23 11:29:19.576: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.732134ms
Nov 23 11:29:20.176: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.19808ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:30.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6443" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:49.737 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":19,"skipped":321,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:30.946: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1282
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:29:31.101: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1282
I1123 11:29:31.122878      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1282, replica count: 1
I1123 11:29:32.173268      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 11:29:32.287: INFO: Created: latency-svc-bdjwh
Nov 23 11:29:32.308: INFO: Got endpoints: latency-svc-bdjwh [34.66954ms]
Nov 23 11:29:32.406: INFO: Created: latency-svc-gtzh6
Nov 23 11:29:32.431: INFO: Got endpoints: latency-svc-gtzh6 [123.097397ms]
Nov 23 11:29:32.438: INFO: Created: latency-svc-8fss5
Nov 23 11:29:32.450: INFO: Created: latency-svc-hmrrx
Nov 23 11:29:32.457: INFO: Got endpoints: latency-svc-8fss5 [147.947493ms]
Nov 23 11:29:32.506: INFO: Got endpoints: latency-svc-hmrrx [196.229654ms]
Nov 23 11:29:32.517: INFO: Created: latency-svc-6cr8z
Nov 23 11:29:32.532: INFO: Got endpoints: latency-svc-6cr8z [222.96799ms]
Nov 23 11:29:32.546: INFO: Created: latency-svc-bl8jf
Nov 23 11:29:32.594: INFO: Got endpoints: latency-svc-bl8jf [284.410109ms]
Nov 23 11:29:32.594: INFO: Created: latency-svc-7m4cj
Nov 23 11:29:32.626: INFO: Got endpoints: latency-svc-7m4cj [316.683269ms]
Nov 23 11:29:32.632: INFO: Created: latency-svc-jc8tf
Nov 23 11:29:32.632: INFO: Got endpoints: latency-svc-jc8tf [323.629221ms]
Nov 23 11:29:32.647: INFO: Created: latency-svc-zxkqq
Nov 23 11:29:32.700: INFO: Got endpoints: latency-svc-zxkqq [390.678711ms]
Nov 23 11:29:32.740: INFO: Created: latency-svc-jr2hj
Nov 23 11:29:32.759: INFO: Got endpoints: latency-svc-jr2hj [450.44598ms]
Nov 23 11:29:32.767: INFO: Created: latency-svc-wz97h
Nov 23 11:29:32.777: INFO: Got endpoints: latency-svc-wz97h [467.57378ms]
Nov 23 11:29:32.794: INFO: Created: latency-svc-4lpgm
Nov 23 11:29:32.809: INFO: Created: latency-svc-x6xg8
Nov 23 11:29:32.811: INFO: Got endpoints: latency-svc-4lpgm [501.662528ms]
Nov 23 11:29:32.852: INFO: Got endpoints: latency-svc-x6xg8 [543.139399ms]
Nov 23 11:29:32.856: INFO: Created: latency-svc-pb5wp
Nov 23 11:29:32.863: INFO: Got endpoints: latency-svc-pb5wp [553.798255ms]
Nov 23 11:29:32.878: INFO: Created: latency-svc-9sqjf
Nov 23 11:29:32.890: INFO: Got endpoints: latency-svc-9sqjf [580.584125ms]
Nov 23 11:29:32.894: INFO: Created: latency-svc-7cs89
Nov 23 11:29:32.911: INFO: Got endpoints: latency-svc-7cs89 [602.042435ms]
Nov 23 11:29:32.928: INFO: Created: latency-svc-v8wrc
Nov 23 11:29:32.947: INFO: Created: latency-svc-r6m6z
Nov 23 11:29:32.948: INFO: Got endpoints: latency-svc-v8wrc [517.146381ms]
Nov 23 11:29:32.976: INFO: Got endpoints: latency-svc-r6m6z [519.242971ms]
Nov 23 11:29:32.983: INFO: Created: latency-svc-jnx8b
Nov 23 11:29:32.999: INFO: Got endpoints: latency-svc-jnx8b [493.776197ms]
Nov 23 11:29:33.000: INFO: Created: latency-svc-kh75w
Nov 23 11:29:33.021: INFO: Created: latency-svc-69wgw
Nov 23 11:29:33.033: INFO: Got endpoints: latency-svc-kh75w [500.491113ms]
Nov 23 11:29:33.057: INFO: Got endpoints: latency-svc-69wgw [463.186208ms]
Nov 23 11:29:33.080: INFO: Created: latency-svc-44kzk
Nov 23 11:29:33.108: INFO: Created: latency-svc-v5gws
Nov 23 11:29:33.108: INFO: Got endpoints: latency-svc-44kzk [481.981315ms]
Nov 23 11:29:33.116: INFO: Got endpoints: latency-svc-v5gws [484.112289ms]
Nov 23 11:29:33.146: INFO: Created: latency-svc-q99jc
Nov 23 11:29:33.152: INFO: Got endpoints: latency-svc-q99jc [452.010325ms]
Nov 23 11:29:33.160: INFO: Created: latency-svc-8g5ch
Nov 23 11:29:33.161: INFO: Got endpoints: latency-svc-8g5ch [401.536343ms]
Nov 23 11:29:33.174: INFO: Created: latency-svc-hw5nn
Nov 23 11:29:33.186: INFO: Got endpoints: latency-svc-hw5nn [409.222766ms]
Nov 23 11:29:33.186: INFO: Created: latency-svc-k72p2
Nov 23 11:29:33.199: INFO: Got endpoints: latency-svc-k72p2 [388.589227ms]
Nov 23 11:29:33.211: INFO: Created: latency-svc-6vbt5
Nov 23 11:29:33.217: INFO: Got endpoints: latency-svc-6vbt5 [364.423438ms]
Nov 23 11:29:33.218: INFO: Created: latency-svc-29h7g
Nov 23 11:29:33.247: INFO: Created: latency-svc-tkrsh
Nov 23 11:29:33.255: INFO: Got endpoints: latency-svc-29h7g [391.68729ms]
Nov 23 11:29:33.266: INFO: Created: latency-svc-8mgkd
Nov 23 11:29:33.276: INFO: Got endpoints: latency-svc-tkrsh [386.510971ms]
Nov 23 11:29:33.284: INFO: Created: latency-svc-rhhw2
Nov 23 11:29:33.293: INFO: Got endpoints: latency-svc-8mgkd [381.321636ms]
Nov 23 11:29:33.305: INFO: Got endpoints: latency-svc-rhhw2 [357.132347ms]
Nov 23 11:29:33.315: INFO: Created: latency-svc-vn525
Nov 23 11:29:33.328: INFO: Got endpoints: latency-svc-vn525 [352.383478ms]
Nov 23 11:29:33.338: INFO: Created: latency-svc-l8dpb
Nov 23 11:29:33.370: INFO: Got endpoints: latency-svc-l8dpb [370.949189ms]
Nov 23 11:29:33.377: INFO: Created: latency-svc-pdnkx
Nov 23 11:29:33.390: INFO: Got endpoints: latency-svc-pdnkx [357.728404ms]
Nov 23 11:29:33.409: INFO: Created: latency-svc-p6jqp
Nov 23 11:29:33.422: INFO: Created: latency-svc-2w6th
Nov 23 11:29:33.429: INFO: Got endpoints: latency-svc-2w6th [320.814006ms]
Nov 23 11:29:33.429: INFO: Got endpoints: latency-svc-p6jqp [371.978065ms]
Nov 23 11:29:33.436: INFO: Created: latency-svc-54kfz
Nov 23 11:29:33.445: INFO: Created: latency-svc-nlzb9
Nov 23 11:29:33.455: INFO: Got endpoints: latency-svc-nlzb9 [303.190259ms]
Nov 23 11:29:33.455: INFO: Got endpoints: latency-svc-54kfz [338.3579ms]
Nov 23 11:29:33.478: INFO: Created: latency-svc-hgwwm
Nov 23 11:29:33.485: INFO: Got endpoints: latency-svc-hgwwm [324.256737ms]
Nov 23 11:29:33.494: INFO: Created: latency-svc-wjwz9
Nov 23 11:29:33.506: INFO: Got endpoints: latency-svc-wjwz9 [320.285091ms]
Nov 23 11:29:33.514: INFO: Created: latency-svc-42x62
Nov 23 11:29:33.522: INFO: Created: latency-svc-jlkxj
Nov 23 11:29:33.524: INFO: Got endpoints: latency-svc-42x62 [323.926915ms]
Nov 23 11:29:33.531: INFO: Got endpoints: latency-svc-jlkxj [314.49735ms]
Nov 23 11:29:33.541: INFO: Created: latency-svc-t89x8
Nov 23 11:29:33.550: INFO: Got endpoints: latency-svc-t89x8 [295.085034ms]
Nov 23 11:29:33.551: INFO: Created: latency-svc-qs77j
Nov 23 11:29:33.568: INFO: Got endpoints: latency-svc-qs77j [291.388329ms]
Nov 23 11:29:33.574: INFO: Created: latency-svc-fh6fr
Nov 23 11:29:33.584: INFO: Got endpoints: latency-svc-fh6fr [291.694408ms]
Nov 23 11:29:33.590: INFO: Created: latency-svc-xphln
Nov 23 11:29:33.609: INFO: Created: latency-svc-fvqg6
Nov 23 11:29:33.610: INFO: Got endpoints: latency-svc-xphln [304.449249ms]
Nov 23 11:29:33.618: INFO: Got endpoints: latency-svc-fvqg6 [289.519081ms]
Nov 23 11:29:33.630: INFO: Created: latency-svc-2xnkq
Nov 23 11:29:33.636: INFO: Got endpoints: latency-svc-2xnkq [265.433054ms]
Nov 23 11:29:33.645: INFO: Created: latency-svc-499gf
Nov 23 11:29:33.654: INFO: Got endpoints: latency-svc-499gf [263.5217ms]
Nov 23 11:29:33.654: INFO: Created: latency-svc-xvfw5
Nov 23 11:29:33.662: INFO: Got endpoints: latency-svc-xvfw5 [233.663493ms]
Nov 23 11:29:33.667: INFO: Created: latency-svc-dfsmh
Nov 23 11:29:33.675: INFO: Got endpoints: latency-svc-dfsmh [245.847573ms]
Nov 23 11:29:33.693: INFO: Created: latency-svc-rxk58
Nov 23 11:29:33.702: INFO: Got endpoints: latency-svc-rxk58 [247.100412ms]
Nov 23 11:29:33.713: INFO: Created: latency-svc-9p6bj
Nov 23 11:29:33.720: INFO: Got endpoints: latency-svc-9p6bj [264.945801ms]
Nov 23 11:29:33.721: INFO: Created: latency-svc-9dtn4
Nov 23 11:29:33.735: INFO: Got endpoints: latency-svc-9dtn4 [249.761028ms]
Nov 23 11:29:33.746: INFO: Created: latency-svc-4k77h
Nov 23 11:29:33.754: INFO: Got endpoints: latency-svc-4k77h [247.708067ms]
Nov 23 11:29:33.757: INFO: Created: latency-svc-kx9cp
Nov 23 11:29:33.765: INFO: Got endpoints: latency-svc-kx9cp [241.279362ms]
Nov 23 11:29:33.770: INFO: Created: latency-svc-cjzwn
Nov 23 11:29:33.778: INFO: Got endpoints: latency-svc-cjzwn [246.951728ms]
Nov 23 11:29:33.789: INFO: Created: latency-svc-z7n58
Nov 23 11:29:33.803: INFO: Created: latency-svc-2zqw5
Nov 23 11:29:33.814: INFO: Created: latency-svc-pzcfs
Nov 23 11:29:33.821: INFO: Got endpoints: latency-svc-z7n58 [271.291969ms]
Nov 23 11:29:33.834: INFO: Created: latency-svc-t74n7
Nov 23 11:29:33.858: INFO: Created: latency-svc-4bb7z
Nov 23 11:29:33.869: INFO: Got endpoints: latency-svc-2zqw5 [301.807115ms]
Nov 23 11:29:33.883: INFO: Created: latency-svc-pvw49
Nov 23 11:29:33.889: INFO: Created: latency-svc-ptv7k
Nov 23 11:29:33.905: INFO: Created: latency-svc-z2cd9
Nov 23 11:29:33.910: INFO: Created: latency-svc-hl4cl
Nov 23 11:29:33.918: INFO: Got endpoints: latency-svc-pzcfs [333.575185ms]
Nov 23 11:29:33.930: INFO: Created: latency-svc-jqxdc
Nov 23 11:29:33.944: INFO: Created: latency-svc-sf68w
Nov 23 11:29:33.951: INFO: Created: latency-svc-d2pm4
Nov 23 11:29:33.965: INFO: Created: latency-svc-wljlp
Nov 23 11:29:33.982: INFO: Got endpoints: latency-svc-t74n7 [371.772764ms]
Nov 23 11:29:33.986: INFO: Created: latency-svc-r5htl
Nov 23 11:29:33.996: INFO: Created: latency-svc-fvg7r
Nov 23 11:29:34.017: INFO: Created: latency-svc-znh2x
Nov 23 11:29:34.022: INFO: Got endpoints: latency-svc-4bb7z [404.476451ms]
Nov 23 11:29:34.036: INFO: Created: latency-svc-ppzxt
Nov 23 11:29:34.046: INFO: Created: latency-svc-5fm2z
Nov 23 11:29:34.052: INFO: Created: latency-svc-gb2q5
Nov 23 11:29:34.056: INFO: Created: latency-svc-9gbq6
Nov 23 11:29:34.072: INFO: Got endpoints: latency-svc-pvw49 [436.027326ms]
Nov 23 11:29:34.086: INFO: Created: latency-svc-v9sj4
Nov 23 11:29:34.125: INFO: Got endpoints: latency-svc-ptv7k [470.971039ms]
Nov 23 11:29:34.143: INFO: Created: latency-svc-prwcz
Nov 23 11:29:34.170: INFO: Got endpoints: latency-svc-z2cd9 [507.579311ms]
Nov 23 11:29:34.180: INFO: Created: latency-svc-mrdpf
Nov 23 11:29:34.229: INFO: Got endpoints: latency-svc-hl4cl [554.26059ms]
Nov 23 11:29:34.241: INFO: Created: latency-svc-s6l4d
Nov 23 11:29:34.271: INFO: Got endpoints: latency-svc-jqxdc [568.600655ms]
Nov 23 11:29:34.283: INFO: Created: latency-svc-2p2j6
Nov 23 11:29:34.320: INFO: Got endpoints: latency-svc-sf68w [600.51389ms]
Nov 23 11:29:34.338: INFO: Created: latency-svc-vzpmq
Nov 23 11:29:34.367: INFO: Got endpoints: latency-svc-d2pm4 [632.2771ms]
Nov 23 11:29:34.384: INFO: Created: latency-svc-5zzqh
Nov 23 11:29:34.419: INFO: Got endpoints: latency-svc-wljlp [664.682217ms]
Nov 23 11:29:34.437: INFO: Created: latency-svc-n26cw
Nov 23 11:29:34.472: INFO: Got endpoints: latency-svc-r5htl [706.737623ms]
Nov 23 11:29:34.487: INFO: Created: latency-svc-ppjnk
Nov 23 11:29:34.521: INFO: Got endpoints: latency-svc-fvg7r [742.887873ms]
Nov 23 11:29:34.551: INFO: Created: latency-svc-bx9nn
Nov 23 11:29:34.572: INFO: Got endpoints: latency-svc-znh2x [750.118634ms]
Nov 23 11:29:34.585: INFO: Created: latency-svc-2tm8n
Nov 23 11:29:34.624: INFO: Got endpoints: latency-svc-ppzxt [754.133176ms]
Nov 23 11:29:34.643: INFO: Created: latency-svc-2c9xl
Nov 23 11:29:34.670: INFO: Got endpoints: latency-svc-5fm2z [647.880557ms]
Nov 23 11:29:34.682: INFO: Created: latency-svc-jkzrr
Nov 23 11:29:34.729: INFO: Got endpoints: latency-svc-gb2q5 [811.173825ms]
Nov 23 11:29:34.747: INFO: Created: latency-svc-w4xx9
Nov 23 11:29:34.776: INFO: Got endpoints: latency-svc-9gbq6 [794.208471ms]
Nov 23 11:29:34.819: INFO: Created: latency-svc-p5mfj
Nov 23 11:29:34.831: INFO: Got endpoints: latency-svc-v9sj4 [759.365761ms]
Nov 23 11:29:34.850: INFO: Created: latency-svc-5pnlp
Nov 23 11:29:34.872: INFO: Got endpoints: latency-svc-prwcz [746.402895ms]
Nov 23 11:29:34.884: INFO: Created: latency-svc-7d5z9
Nov 23 11:29:34.918: INFO: Got endpoints: latency-svc-mrdpf [747.538682ms]
Nov 23 11:29:34.936: INFO: Created: latency-svc-95tpm
Nov 23 11:29:34.967: INFO: Got endpoints: latency-svc-s6l4d [737.703477ms]
Nov 23 11:29:34.980: INFO: Created: latency-svc-px8vc
Nov 23 11:29:35.024: INFO: Got endpoints: latency-svc-2p2j6 [753.058337ms]
Nov 23 11:29:35.038: INFO: Created: latency-svc-tkzph
Nov 23 11:29:35.076: INFO: Got endpoints: latency-svc-vzpmq [755.412986ms]
Nov 23 11:29:35.091: INFO: Created: latency-svc-8pc9p
Nov 23 11:29:35.128: INFO: Got endpoints: latency-svc-5zzqh [760.064014ms]
Nov 23 11:29:35.159: INFO: Created: latency-svc-c4kws
Nov 23 11:29:35.176: INFO: Got endpoints: latency-svc-n26cw [757.29924ms]
Nov 23 11:29:35.235: INFO: Got endpoints: latency-svc-ppjnk [763.236261ms]
Nov 23 11:29:35.240: INFO: Created: latency-svc-wjt89
Nov 23 11:29:35.256: INFO: Created: latency-svc-cd2qw
Nov 23 11:29:35.273: INFO: Got endpoints: latency-svc-bx9nn [751.351219ms]
Nov 23 11:29:35.288: INFO: Created: latency-svc-8h66f
Nov 23 11:29:35.318: INFO: Got endpoints: latency-svc-2tm8n [746.401584ms]
Nov 23 11:29:35.332: INFO: Created: latency-svc-9v742
Nov 23 11:29:35.371: INFO: Got endpoints: latency-svc-2c9xl [747.396128ms]
Nov 23 11:29:35.388: INFO: Created: latency-svc-hl65v
Nov 23 11:29:35.418: INFO: Got endpoints: latency-svc-jkzrr [747.936687ms]
Nov 23 11:29:35.430: INFO: Created: latency-svc-64k8k
Nov 23 11:29:35.481: INFO: Got endpoints: latency-svc-w4xx9 [751.650117ms]
Nov 23 11:29:35.490: INFO: Created: latency-svc-s7pw2
Nov 23 11:29:35.523: INFO: Got endpoints: latency-svc-p5mfj [746.55041ms]
Nov 23 11:29:35.535: INFO: Created: latency-svc-khh42
Nov 23 11:29:35.569: INFO: Got endpoints: latency-svc-5pnlp [737.695802ms]
Nov 23 11:29:35.592: INFO: Created: latency-svc-cg6rt
Nov 23 11:29:35.618: INFO: Got endpoints: latency-svc-7d5z9 [746.578026ms]
Nov 23 11:29:35.630: INFO: Created: latency-svc-rnv2k
Nov 23 11:29:35.671: INFO: Got endpoints: latency-svc-95tpm [753.041226ms]
Nov 23 11:29:35.684: INFO: Created: latency-svc-r7tmx
Nov 23 11:29:35.721: INFO: Got endpoints: latency-svc-px8vc [754.640525ms]
Nov 23 11:29:35.742: INFO: Created: latency-svc-jcpc5
Nov 23 11:29:35.771: INFO: Got endpoints: latency-svc-tkzph [747.198598ms]
Nov 23 11:29:35.786: INFO: Created: latency-svc-hhjst
Nov 23 11:29:35.829: INFO: Got endpoints: latency-svc-8pc9p [752.84744ms]
Nov 23 11:29:35.842: INFO: Created: latency-svc-j77dd
Nov 23 11:29:35.874: INFO: Got endpoints: latency-svc-c4kws [746.731813ms]
Nov 23 11:29:35.888: INFO: Created: latency-svc-wvj4z
Nov 23 11:29:35.923: INFO: Got endpoints: latency-svc-wjt89 [746.800028ms]
Nov 23 11:29:35.942: INFO: Created: latency-svc-gxd7m
Nov 23 11:29:36.000: INFO: Got endpoints: latency-svc-cd2qw [765.437182ms]
Nov 23 11:29:36.023: INFO: Got endpoints: latency-svc-8h66f [749.805507ms]
Nov 23 11:29:36.025: INFO: Created: latency-svc-ftkcw
Nov 23 11:29:36.046: INFO: Created: latency-svc-nx65l
Nov 23 11:29:36.070: INFO: Got endpoints: latency-svc-9v742 [751.821393ms]
Nov 23 11:29:36.091: INFO: Created: latency-svc-dpgxp
Nov 23 11:29:36.123: INFO: Got endpoints: latency-svc-hl65v [752.037523ms]
Nov 23 11:29:36.139: INFO: Created: latency-svc-44sxp
Nov 23 11:29:36.174: INFO: Got endpoints: latency-svc-64k8k [755.160478ms]
Nov 23 11:29:36.193: INFO: Created: latency-svc-mnx6w
Nov 23 11:29:36.224: INFO: Got endpoints: latency-svc-s7pw2 [743.052855ms]
Nov 23 11:29:36.254: INFO: Created: latency-svc-qsjt5
Nov 23 11:29:36.268: INFO: Got endpoints: latency-svc-khh42 [745.079053ms]
Nov 23 11:29:36.281: INFO: Created: latency-svc-q4hbp
Nov 23 11:29:36.320: INFO: Got endpoints: latency-svc-cg6rt [750.99725ms]
Nov 23 11:29:36.337: INFO: Created: latency-svc-z7sjv
Nov 23 11:29:36.370: INFO: Got endpoints: latency-svc-rnv2k [752.062125ms]
Nov 23 11:29:36.381: INFO: Created: latency-svc-vhssm
Nov 23 11:29:36.435: INFO: Got endpoints: latency-svc-r7tmx [763.666491ms]
Nov 23 11:29:36.452: INFO: Created: latency-svc-9qdds
Nov 23 11:29:36.471: INFO: Got endpoints: latency-svc-jcpc5 [749.532787ms]
Nov 23 11:29:36.485: INFO: Created: latency-svc-nz9xh
Nov 23 11:29:36.528: INFO: Got endpoints: latency-svc-hhjst [756.756617ms]
Nov 23 11:29:36.543: INFO: Created: latency-svc-bnbwb
Nov 23 11:29:36.571: INFO: Got endpoints: latency-svc-j77dd [742.642243ms]
Nov 23 11:29:36.593: INFO: Created: latency-svc-sswm6
Nov 23 11:29:36.617: INFO: Got endpoints: latency-svc-wvj4z [742.451258ms]
Nov 23 11:29:36.637: INFO: Created: latency-svc-qrrjb
Nov 23 11:29:36.671: INFO: Got endpoints: latency-svc-gxd7m [747.867682ms]
Nov 23 11:29:36.688: INFO: Created: latency-svc-8wl9k
Nov 23 11:29:36.719: INFO: Got endpoints: latency-svc-ftkcw [718.059539ms]
Nov 23 11:29:36.731: INFO: Created: latency-svc-6rcr5
Nov 23 11:29:36.770: INFO: Got endpoints: latency-svc-nx65l [747.686349ms]
Nov 23 11:29:36.785: INFO: Created: latency-svc-kz8mn
Nov 23 11:29:36.818: INFO: Got endpoints: latency-svc-dpgxp [748.045892ms]
Nov 23 11:29:36.835: INFO: Created: latency-svc-q446d
Nov 23 11:29:36.871: INFO: Got endpoints: latency-svc-44sxp [747.822027ms]
Nov 23 11:29:36.882: INFO: Created: latency-svc-8s6ng
Nov 23 11:29:36.918: INFO: Got endpoints: latency-svc-mnx6w [744.604334ms]
Nov 23 11:29:36.933: INFO: Created: latency-svc-pf7wv
Nov 23 11:29:36.970: INFO: Got endpoints: latency-svc-qsjt5 [746.258262ms]
Nov 23 11:29:36.982: INFO: Created: latency-svc-cdxmd
Nov 23 11:29:37.024: INFO: Got endpoints: latency-svc-q4hbp [756.194231ms]
Nov 23 11:29:37.037: INFO: Created: latency-svc-4brb8
Nov 23 11:29:37.071: INFO: Got endpoints: latency-svc-z7sjv [751.009657ms]
Nov 23 11:29:37.082: INFO: Created: latency-svc-dkdxx
Nov 23 11:29:37.119: INFO: Got endpoints: latency-svc-vhssm [748.333495ms]
Nov 23 11:29:37.131: INFO: Created: latency-svc-mrcdm
Nov 23 11:29:37.171: INFO: Got endpoints: latency-svc-9qdds [736.059514ms]
Nov 23 11:29:37.179: INFO: Created: latency-svc-cgpdg
Nov 23 11:29:37.217: INFO: Got endpoints: latency-svc-nz9xh [746.196831ms]
Nov 23 11:29:37.229: INFO: Created: latency-svc-hp6vz
Nov 23 11:29:37.273: INFO: Got endpoints: latency-svc-bnbwb [745.218722ms]
Nov 23 11:29:37.287: INFO: Created: latency-svc-2c2kn
Nov 23 11:29:37.318: INFO: Got endpoints: latency-svc-sswm6 [746.907528ms]
Nov 23 11:29:37.329: INFO: Created: latency-svc-txv9l
Nov 23 11:29:37.369: INFO: Got endpoints: latency-svc-qrrjb [751.909186ms]
Nov 23 11:29:37.382: INFO: Created: latency-svc-nswkm
Nov 23 11:29:37.419: INFO: Got endpoints: latency-svc-8wl9k [747.788448ms]
Nov 23 11:29:37.435: INFO: Created: latency-svc-s6c4q
Nov 23 11:29:37.471: INFO: Got endpoints: latency-svc-6rcr5 [752.768854ms]
Nov 23 11:29:37.487: INFO: Created: latency-svc-fbqmt
Nov 23 11:29:37.521: INFO: Got endpoints: latency-svc-kz8mn [750.305431ms]
Nov 23 11:29:37.535: INFO: Created: latency-svc-qzxqk
Nov 23 11:29:37.577: INFO: Got endpoints: latency-svc-q446d [758.844671ms]
Nov 23 11:29:37.593: INFO: Created: latency-svc-h7s5g
Nov 23 11:29:37.618: INFO: Got endpoints: latency-svc-8s6ng [746.665869ms]
Nov 23 11:29:37.630: INFO: Created: latency-svc-c9g8d
Nov 23 11:29:37.669: INFO: Got endpoints: latency-svc-pf7wv [750.348706ms]
Nov 23 11:29:37.689: INFO: Created: latency-svc-xmbdv
Nov 23 11:29:37.717: INFO: Got endpoints: latency-svc-cdxmd [746.65669ms]
Nov 23 11:29:37.728: INFO: Created: latency-svc-qgrfn
Nov 23 11:29:37.771: INFO: Got endpoints: latency-svc-4brb8 [747.069354ms]
Nov 23 11:29:37.786: INFO: Created: latency-svc-rfxhz
Nov 23 11:29:37.817: INFO: Got endpoints: latency-svc-dkdxx [746.114267ms]
Nov 23 11:29:37.831: INFO: Created: latency-svc-mxtjv
Nov 23 11:29:37.868: INFO: Got endpoints: latency-svc-mrcdm [749.2784ms]
Nov 23 11:29:37.879: INFO: Created: latency-svc-89md4
Nov 23 11:29:37.926: INFO: Got endpoints: latency-svc-cgpdg [755.112298ms]
Nov 23 11:29:37.952: INFO: Created: latency-svc-ppdrh
Nov 23 11:29:37.971: INFO: Got endpoints: latency-svc-hp6vz [754.116362ms]
Nov 23 11:29:37.981: INFO: Created: latency-svc-vf55m
Nov 23 11:29:38.024: INFO: Got endpoints: latency-svc-2c2kn [750.888228ms]
Nov 23 11:29:38.035: INFO: Created: latency-svc-2cnwj
Nov 23 11:29:38.072: INFO: Got endpoints: latency-svc-txv9l [753.555608ms]
Nov 23 11:29:38.087: INFO: Created: latency-svc-5vl7v
Nov 23 11:29:38.121: INFO: Got endpoints: latency-svc-nswkm [751.649999ms]
Nov 23 11:29:38.139: INFO: Created: latency-svc-5zlgf
Nov 23 11:29:38.171: INFO: Got endpoints: latency-svc-s6c4q [752.080679ms]
Nov 23 11:29:38.180: INFO: Created: latency-svc-jwdqd
Nov 23 11:29:38.219: INFO: Got endpoints: latency-svc-fbqmt [747.03693ms]
Nov 23 11:29:38.236: INFO: Created: latency-svc-ww8rl
Nov 23 11:29:38.271: INFO: Got endpoints: latency-svc-qzxqk [750.403189ms]
Nov 23 11:29:38.295: INFO: Created: latency-svc-8vtfh
Nov 23 11:29:38.323: INFO: Got endpoints: latency-svc-h7s5g [746.416579ms]
Nov 23 11:29:38.359: INFO: Created: latency-svc-jn4q9
Nov 23 11:29:38.367: INFO: Got endpoints: latency-svc-c9g8d [749.704841ms]
Nov 23 11:29:38.381: INFO: Created: latency-svc-9jgz7
Nov 23 11:29:38.419: INFO: Got endpoints: latency-svc-xmbdv [750.233889ms]
Nov 23 11:29:38.440: INFO: Created: latency-svc-csjqd
Nov 23 11:29:38.469: INFO: Got endpoints: latency-svc-qgrfn [751.381743ms]
Nov 23 11:29:38.485: INFO: Created: latency-svc-kjrmk
Nov 23 11:29:38.520: INFO: Got endpoints: latency-svc-rfxhz [748.558268ms]
Nov 23 11:29:38.531: INFO: Created: latency-svc-99p4k
Nov 23 11:29:38.567: INFO: Got endpoints: latency-svc-mxtjv [749.677869ms]
Nov 23 11:29:38.577: INFO: Created: latency-svc-gnnkr
Nov 23 11:29:38.619: INFO: Got endpoints: latency-svc-89md4 [750.540184ms]
Nov 23 11:29:38.633: INFO: Created: latency-svc-7q7xs
Nov 23 11:29:38.671: INFO: Got endpoints: latency-svc-ppdrh [744.675364ms]
Nov 23 11:29:38.684: INFO: Created: latency-svc-kprws
Nov 23 11:29:38.719: INFO: Got endpoints: latency-svc-vf55m [747.869912ms]
Nov 23 11:29:38.732: INFO: Created: latency-svc-xmd69
Nov 23 11:29:38.768: INFO: Got endpoints: latency-svc-2cnwj [743.724341ms]
Nov 23 11:29:38.781: INFO: Created: latency-svc-fn4wz
Nov 23 11:29:38.819: INFO: Got endpoints: latency-svc-5vl7v [746.91646ms]
Nov 23 11:29:38.839: INFO: Created: latency-svc-kjcmg
Nov 23 11:29:38.870: INFO: Got endpoints: latency-svc-5zlgf [749.054226ms]
Nov 23 11:29:38.885: INFO: Created: latency-svc-s2b5d
Nov 23 11:29:38.919: INFO: Got endpoints: latency-svc-jwdqd [748.27188ms]
Nov 23 11:29:38.933: INFO: Created: latency-svc-bj6fg
Nov 23 11:29:38.986: INFO: Got endpoints: latency-svc-ww8rl [766.616754ms]
Nov 23 11:29:39.002: INFO: Created: latency-svc-pzp8c
Nov 23 11:29:39.020: INFO: Got endpoints: latency-svc-8vtfh [748.260828ms]
Nov 23 11:29:39.036: INFO: Created: latency-svc-wsgn9
Nov 23 11:29:39.074: INFO: Got endpoints: latency-svc-jn4q9 [750.633639ms]
Nov 23 11:29:39.087: INFO: Created: latency-svc-2gxwb
Nov 23 11:29:39.123: INFO: Got endpoints: latency-svc-9jgz7 [755.099288ms]
Nov 23 11:29:39.140: INFO: Created: latency-svc-5b44n
Nov 23 11:29:39.172: INFO: Got endpoints: latency-svc-csjqd [753.560378ms]
Nov 23 11:29:39.188: INFO: Created: latency-svc-5lftd
Nov 23 11:29:39.221: INFO: Got endpoints: latency-svc-kjrmk [752.309976ms]
Nov 23 11:29:39.240: INFO: Created: latency-svc-dlds9
Nov 23 11:29:39.273: INFO: Got endpoints: latency-svc-99p4k [753.076614ms]
Nov 23 11:29:39.296: INFO: Created: latency-svc-b62xk
Nov 23 11:29:39.319: INFO: Got endpoints: latency-svc-gnnkr [751.971138ms]
Nov 23 11:29:39.352: INFO: Created: latency-svc-tz64m
Nov 23 11:29:39.371: INFO: Got endpoints: latency-svc-7q7xs [752.514261ms]
Nov 23 11:29:39.421: INFO: Created: latency-svc-czkfv
Nov 23 11:29:39.429: INFO: Got endpoints: latency-svc-kprws [758.805447ms]
Nov 23 11:29:39.505: INFO: Got endpoints: latency-svc-xmd69 [785.184397ms]
Nov 23 11:29:39.506: INFO: Created: latency-svc-pwvx5
Nov 23 11:29:39.527: INFO: Got endpoints: latency-svc-fn4wz [758.776629ms]
Nov 23 11:29:39.528: INFO: Created: latency-svc-xrk5n
Nov 23 11:29:39.602: INFO: Created: latency-svc-m85hr
Nov 23 11:29:39.614: INFO: Got endpoints: latency-svc-kjcmg [795.415771ms]
Nov 23 11:29:39.624: INFO: Got endpoints: latency-svc-s2b5d [754.605055ms]
Nov 23 11:29:39.652: INFO: Created: latency-svc-s8bxk
Nov 23 11:29:39.704: INFO: Got endpoints: latency-svc-bj6fg [785.436359ms]
Nov 23 11:29:39.717: INFO: Created: latency-svc-hdcgg
Nov 23 11:29:39.727: INFO: Got endpoints: latency-svc-pzp8c [741.270893ms]
Nov 23 11:29:39.736: INFO: Created: latency-svc-8p84r
Nov 23 11:29:39.761: INFO: Created: latency-svc-dxgft
Nov 23 11:29:39.803: INFO: Got endpoints: latency-svc-wsgn9 [783.40535ms]
Nov 23 11:29:39.832: INFO: Got endpoints: latency-svc-2gxwb [758.478629ms]
Nov 23 11:29:39.836: INFO: Created: latency-svc-6zg6c
Nov 23 11:29:39.849: INFO: Created: latency-svc-n2r8w
Nov 23 11:29:39.872: INFO: Got endpoints: latency-svc-5b44n [749.277406ms]
Nov 23 11:29:39.911: INFO: Created: latency-svc-79xx5
Nov 23 11:29:39.920: INFO: Got endpoints: latency-svc-5lftd [747.642143ms]
Nov 23 11:29:39.931: INFO: Created: latency-svc-rfdhv
Nov 23 11:29:39.969: INFO: Got endpoints: latency-svc-dlds9 [747.642417ms]
Nov 23 11:29:39.986: INFO: Created: latency-svc-vsmx4
Nov 23 11:29:40.021: INFO: Got endpoints: latency-svc-b62xk [748.36309ms]
Nov 23 11:29:40.037: INFO: Created: latency-svc-5bm9w
Nov 23 11:29:40.071: INFO: Got endpoints: latency-svc-tz64m [751.604519ms]
Nov 23 11:29:40.087: INFO: Created: latency-svc-gflzj
Nov 23 11:29:40.122: INFO: Got endpoints: latency-svc-czkfv [751.184814ms]
Nov 23 11:29:40.135: INFO: Created: latency-svc-65zrl
Nov 23 11:29:40.169: INFO: Got endpoints: latency-svc-pwvx5 [739.693955ms]
Nov 23 11:29:40.179: INFO: Created: latency-svc-2jpxf
Nov 23 11:29:40.225: INFO: Got endpoints: latency-svc-xrk5n [720.197752ms]
Nov 23 11:29:40.267: INFO: Got endpoints: latency-svc-m85hr [740.834571ms]
Nov 23 11:29:40.321: INFO: Got endpoints: latency-svc-s8bxk [706.309481ms]
Nov 23 11:29:40.369: INFO: Got endpoints: latency-svc-hdcgg [744.846703ms]
Nov 23 11:29:40.422: INFO: Got endpoints: latency-svc-8p84r [717.944072ms]
Nov 23 11:29:40.468: INFO: Got endpoints: latency-svc-dxgft [740.860113ms]
Nov 23 11:29:40.518: INFO: Got endpoints: latency-svc-6zg6c [714.889088ms]
Nov 23 11:29:40.568: INFO: Got endpoints: latency-svc-n2r8w [735.91573ms]
Nov 23 11:29:40.619: INFO: Got endpoints: latency-svc-79xx5 [746.909135ms]
Nov 23 11:29:40.668: INFO: Got endpoints: latency-svc-rfdhv [748.051698ms]
Nov 23 11:29:40.719: INFO: Got endpoints: latency-svc-vsmx4 [750.154016ms]
Nov 23 11:29:40.767: INFO: Got endpoints: latency-svc-5bm9w [745.755239ms]
Nov 23 11:29:40.819: INFO: Got endpoints: latency-svc-gflzj [747.745558ms]
Nov 23 11:29:40.870: INFO: Got endpoints: latency-svc-65zrl [747.10877ms]
Nov 23 11:29:40.918: INFO: Got endpoints: latency-svc-2jpxf [748.856409ms]
Nov 23 11:29:40.918: INFO: Latencies: [123.097397ms 147.947493ms 196.229654ms 222.96799ms 233.663493ms 241.279362ms 245.847573ms 246.951728ms 247.100412ms 247.708067ms 249.761028ms 263.5217ms 264.945801ms 265.433054ms 271.291969ms 284.410109ms 289.519081ms 291.388329ms 291.694408ms 295.085034ms 301.807115ms 303.190259ms 304.449249ms 314.49735ms 316.683269ms 320.285091ms 320.814006ms 323.629221ms 323.926915ms 324.256737ms 333.575185ms 338.3579ms 352.383478ms 357.132347ms 357.728404ms 364.423438ms 370.949189ms 371.772764ms 371.978065ms 381.321636ms 386.510971ms 388.589227ms 390.678711ms 391.68729ms 401.536343ms 404.476451ms 409.222766ms 436.027326ms 450.44598ms 452.010325ms 463.186208ms 467.57378ms 470.971039ms 481.981315ms 484.112289ms 493.776197ms 500.491113ms 501.662528ms 507.579311ms 517.146381ms 519.242971ms 543.139399ms 553.798255ms 554.26059ms 568.600655ms 580.584125ms 600.51389ms 602.042435ms 632.2771ms 647.880557ms 664.682217ms 706.309481ms 706.737623ms 714.889088ms 717.944072ms 718.059539ms 720.197752ms 735.91573ms 736.059514ms 737.695802ms 737.703477ms 739.693955ms 740.834571ms 740.860113ms 741.270893ms 742.451258ms 742.642243ms 742.887873ms 743.052855ms 743.724341ms 744.604334ms 744.675364ms 744.846703ms 745.079053ms 745.218722ms 745.755239ms 746.114267ms 746.196831ms 746.258262ms 746.401584ms 746.402895ms 746.416579ms 746.55041ms 746.578026ms 746.65669ms 746.665869ms 746.731813ms 746.800028ms 746.907528ms 746.909135ms 746.91646ms 747.03693ms 747.069354ms 747.10877ms 747.198598ms 747.396128ms 747.538682ms 747.642143ms 747.642417ms 747.686349ms 747.745558ms 747.788448ms 747.822027ms 747.867682ms 747.869912ms 747.936687ms 748.045892ms 748.051698ms 748.260828ms 748.27188ms 748.333495ms 748.36309ms 748.558268ms 748.856409ms 749.054226ms 749.277406ms 749.2784ms 749.532787ms 749.677869ms 749.704841ms 749.805507ms 750.118634ms 750.154016ms 750.233889ms 750.305431ms 750.348706ms 750.403189ms 750.540184ms 750.633639ms 750.888228ms 750.99725ms 751.009657ms 751.184814ms 751.351219ms 751.381743ms 751.604519ms 751.649999ms 751.650117ms 751.821393ms 751.909186ms 751.971138ms 752.037523ms 752.062125ms 752.080679ms 752.309976ms 752.514261ms 752.768854ms 752.84744ms 753.041226ms 753.058337ms 753.076614ms 753.555608ms 753.560378ms 754.116362ms 754.133176ms 754.605055ms 754.640525ms 755.099288ms 755.112298ms 755.160478ms 755.412986ms 756.194231ms 756.756617ms 757.29924ms 758.478629ms 758.776629ms 758.805447ms 758.844671ms 759.365761ms 760.064014ms 763.236261ms 763.666491ms 765.437182ms 766.616754ms 783.40535ms 785.184397ms 785.436359ms 794.208471ms 795.415771ms 811.173825ms]
Nov 23 11:29:40.918: INFO: 50 %ile: 746.402895ms
Nov 23 11:29:40.918: INFO: 90 %ile: 755.412986ms
Nov 23 11:29:40.918: INFO: 99 %ile: 795.415771ms
Nov 23 11:29:40.918: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:40.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1282" for this suite.

• [SLOW TEST:9.987 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":20,"skipped":323,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:40.934: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Nov 23 11:29:41.096: INFO: Waiting up to 5m0s for pod "var-expansion-91718d72-b561-43f0-b4ca-4952512762cf" in namespace "var-expansion-7103" to be "Succeeded or Failed"
Nov 23 11:29:41.106: INFO: Pod "var-expansion-91718d72-b561-43f0-b4ca-4952512762cf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.071635ms
Nov 23 11:29:43.110: INFO: Pod "var-expansion-91718d72-b561-43f0-b4ca-4952512762cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014324696s
STEP: Saw pod success
Nov 23 11:29:43.110: INFO: Pod "var-expansion-91718d72-b561-43f0-b4ca-4952512762cf" satisfied condition "Succeeded or Failed"
Nov 23 11:29:43.114: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod var-expansion-91718d72-b561-43f0-b4ca-4952512762cf container dapi-container: <nil>
STEP: delete the pod
Nov 23 11:29:43.145: INFO: Waiting for pod var-expansion-91718d72-b561-43f0-b4ca-4952512762cf to disappear
Nov 23 11:29:43.147: INFO: Pod var-expansion-91718d72-b561-43f0-b4ca-4952512762cf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:43.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7103" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:43.166: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 23 11:29:43.321: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:46.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5941" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":22,"skipped":358,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:46.406: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-800ce85e-d702-4a7c-b915-993844586296
STEP: Creating a pod to test consume secrets
Nov 23 11:29:46.595: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8" in namespace "projected-1816" to be "Succeeded or Failed"
Nov 23 11:29:46.610: INFO: Pod "pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.584161ms
Nov 23 11:29:48.616: INFO: Pod "pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020484593s
Nov 23 11:29:50.620: INFO: Pod "pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024731653s
STEP: Saw pod success
Nov 23 11:29:50.620: INFO: Pod "pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8" satisfied condition "Succeeded or Failed"
Nov 23 11:29:50.623: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 23 11:29:50.670: INFO: Waiting for pod pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8 to disappear
Nov 23 11:29:50.679: INFO: Pod pod-projected-secrets-ec8b7cb4-67db-40fe-99ba-4ba905b06da8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1816" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":23,"skipped":359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:50.718: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 23 11:29:50.933: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:50.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1123" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":24,"skipped":391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:50.998: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9371.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9371.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9371.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9371.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9371.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9371.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 11:29:53.300: INFO: DNS probes using dns-9371/dns-test-ce74097b-8059-45f7-a065-532d4fc1a5bf succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:29:53.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9371" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":25,"skipped":445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:29:53.358: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:04.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1413" for this suite.

• [SLOW TEST:11.260 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":26,"skipped":577,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:04.618: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Nov 23 11:30:04.777: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-710016297 proxy --unix-socket=/tmp/kubectl-proxy-unix329762420/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:04.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6831" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":27,"skipped":597,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:04.902: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 23 11:30:05.072: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 23 11:30:05.084: INFO: Waiting for terminating namespaces to be deleted...
Nov 23 11:30:05.090: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-0 before test
Nov 23 11:30:05.101: INFO: canal-92zmd from kube-system started at 2020-11-23 09:08:12 +0000 UTC (3 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 11:30:05.101: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 11:30:05.101: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 11:30:05.101: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 11:30:05.101: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 11:30:05.101: INFO: kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container main ready: true, restart count 0
Nov 23 11:30:05.101: INFO: metrics-server-v0.3.6-7d5754b8df-v4g9x from kube-system started at 2020-11-23 09:08:58 +0000 UTC (2 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container metrics-server ready: true, restart count 0
Nov 23 11:30:05.101: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 23 11:30:05.101: INFO: node-local-dns-vnlcb from kube-system started at 2020-11-23 09:08:12 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 11:30:05.101: INFO: sonobuoy from sonobuoy started at 2020-11-23 11:25:21 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 23 11:30:05.101: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:30:05.101: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:30:05.101: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 23 11:30:05.101: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-1 before test
Nov 23 11:30:05.110: INFO: canal-pgdpd from kube-system started at 2020-11-23 09:08:13 +0000 UTC (3 container statuses recorded)
Nov 23 11:30:05.110: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 11:30:05.110: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 11:30:05.110: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 11:30:05.110: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.110: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 11:30:05.110: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.110: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 11:30:05.110: INFO: kublr-node-name-reporter-c33b8d393ebe42a50f83769b2f12339f5d1f5d10dd75810de5733c313118246a-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.110: INFO: 	Container main ready: true, restart count 0
Nov 23 11:30:05.110: INFO: kublr-system-shell-564c49bf57-ksqmb from kube-system started at 2020-11-23 09:09:38 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.110: INFO: 	Container shell ready: true, restart count 0
Nov 23 11:30:05.110: INFO: node-local-dns-74tqs from kube-system started at 2020-11-23 09:08:13 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.110: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 11:30:05.110: INFO: kublr-operator-69c9677745-zk5lz from kublr started at 2020-11-23 11:19:03 +0000 UTC (1 container statuses recorded)
Nov 23 11:30:05.111: INFO: 	Container kublr-operator ready: true, restart count 0
Nov 23 11:30:05.111: INFO: sonobuoy-e2e-job-7d6aa2c1eacf4413 from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:30:05.111: INFO: 	Container e2e ready: true, restart count 0
Nov 23 11:30:05.111: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:30:05.111: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-sj5vn from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:30:05.111: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:30:05.111: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.164a1fa3b870d9fd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:06.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6852" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":28,"skipped":623,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:06.155: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:09.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1175" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":29,"skipped":637,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:09.364: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-f7863f08-a5e6-4cb3-b5df-642bce5d504b
STEP: Creating a pod to test consume secrets
Nov 23 11:30:09.520: INFO: Waiting up to 5m0s for pod "pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355" in namespace "secrets-426" to be "Succeeded or Failed"
Nov 23 11:30:09.523: INFO: Pod "pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355": Phase="Pending", Reason="", readiness=false. Elapsed: 3.533294ms
Nov 23 11:30:11.544: INFO: Pod "pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024402048s
STEP: Saw pod success
Nov 23 11:30:11.544: INFO: Pod "pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355" satisfied condition "Succeeded or Failed"
Nov 23 11:30:11.550: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355 container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 11:30:11.613: INFO: Waiting for pod pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355 to disappear
Nov 23 11:30:11.617: INFO: Pod pod-secrets-90e65c8b-aad0-4ea7-8d8d-8b92d2215355 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:11.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-426" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:11.634: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 23 11:30:11.826: INFO: Waiting up to 5m0s for pod "pod-93d66b0b-2035-483d-9d72-e2242185276c" in namespace "emptydir-2250" to be "Succeeded or Failed"
Nov 23 11:30:11.837: INFO: Pod "pod-93d66b0b-2035-483d-9d72-e2242185276c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.651642ms
Nov 23 11:30:13.842: INFO: Pod "pod-93d66b0b-2035-483d-9d72-e2242185276c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015392186s
STEP: Saw pod success
Nov 23 11:30:13.842: INFO: Pod "pod-93d66b0b-2035-483d-9d72-e2242185276c" satisfied condition "Succeeded or Failed"
Nov 23 11:30:13.844: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-93d66b0b-2035-483d-9d72-e2242185276c container test-container: <nil>
STEP: delete the pod
Nov 23 11:30:13.872: INFO: Waiting for pod pod-93d66b0b-2035-483d-9d72-e2242185276c to disappear
Nov 23 11:30:13.876: INFO: Pod pod-93d66b0b-2035-483d-9d72-e2242185276c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:13.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2250" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:13.884: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:21.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1808" for this suite.

• [SLOW TEST:7.181 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":32,"skipped":726,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:21.065: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 23 11:30:23.767: INFO: Successfully updated pod "adopt-release-hmn9n"
STEP: Checking that the Job readopts the Pod
Nov 23 11:30:23.767: INFO: Waiting up to 15m0s for pod "adopt-release-hmn9n" in namespace "job-810" to be "adopted"
Nov 23 11:30:23.771: INFO: Pod "adopt-release-hmn9n": Phase="Running", Reason="", readiness=true. Elapsed: 4.353816ms
Nov 23 11:30:25.776: INFO: Pod "adopt-release-hmn9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.009494841s
Nov 23 11:30:25.776: INFO: Pod "adopt-release-hmn9n" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 23 11:30:26.287: INFO: Successfully updated pod "adopt-release-hmn9n"
STEP: Checking that the Job releases the Pod
Nov 23 11:30:26.287: INFO: Waiting up to 15m0s for pod "adopt-release-hmn9n" in namespace "job-810" to be "released"
Nov 23 11:30:26.298: INFO: Pod "adopt-release-hmn9n": Phase="Running", Reason="", readiness=true. Elapsed: 11.087612ms
Nov 23 11:30:28.302: INFO: Pod "adopt-release-hmn9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.015091814s
Nov 23 11:30:28.303: INFO: Pod "adopt-release-hmn9n" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-810" for this suite.

• [SLOW TEST:7.246 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":33,"skipped":730,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:28.311: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:30:28.555: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4975c68d-be90-4e07-adad-23c61b2f1d59", Controller:(*bool)(0xc003b6d29e), BlockOwnerDeletion:(*bool)(0xc003b6d29f)}}
Nov 23 11:30:28.583: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"33b280d6-c62d-4d03-856e-8face08d17ac", Controller:(*bool)(0xc0054fe38e), BlockOwnerDeletion:(*bool)(0xc0054fe38f)}}
Nov 23 11:30:28.594: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79607048-c64b-4544-9bed-34bb7772d4bb", Controller:(*bool)(0xc003b9a15e), BlockOwnerDeletion:(*bool)(0xc003b9a15f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:33.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3531" for this suite.

• [SLOW TEST:5.319 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":34,"skipped":732,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:33.630: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Nov 23 11:30:33.790: INFO: Waiting up to 5m0s for pod "client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835" in namespace "containers-7761" to be "Succeeded or Failed"
Nov 23 11:30:33.796: INFO: Pod "client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78222ms
Nov 23 11:30:35.799: INFO: Pod "client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009114736s
STEP: Saw pod success
Nov 23 11:30:35.799: INFO: Pod "client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835" satisfied condition "Succeeded or Failed"
Nov 23 11:30:35.803: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835 container test-container: <nil>
STEP: delete the pod
Nov 23 11:30:35.830: INFO: Waiting for pod client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835 to disappear
Nov 23 11:30:35.835: INFO: Pod client-containers-cd3ff4a7-f7c8-43e8-a282-cec6b04c9835 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:35.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7761" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:35.845: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-l7bj
STEP: Creating a pod to test atomic-volume-subpath
Nov 23 11:30:36.021: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-l7bj" in namespace "subpath-6158" to be "Succeeded or Failed"
Nov 23 11:30:36.024: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.745886ms
Nov 23 11:30:38.030: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009105243s
Nov 23 11:30:40.036: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 4.014950171s
Nov 23 11:30:42.042: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 6.021459986s
Nov 23 11:30:44.045: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 8.024296337s
Nov 23 11:30:46.048: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027771603s
Nov 23 11:30:48.053: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 12.032225568s
Nov 23 11:30:50.057: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 14.036397986s
Nov 23 11:30:52.062: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 16.041433219s
Nov 23 11:30:54.065: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 18.044753255s
Nov 23 11:30:56.070: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Running", Reason="", readiness=true. Elapsed: 20.049403175s
Nov 23 11:30:58.074: INFO: Pod "pod-subpath-test-secret-l7bj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053008959s
STEP: Saw pod success
Nov 23 11:30:58.074: INFO: Pod "pod-subpath-test-secret-l7bj" satisfied condition "Succeeded or Failed"
Nov 23 11:30:58.077: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-subpath-test-secret-l7bj container test-container-subpath-secret-l7bj: <nil>
STEP: delete the pod
Nov 23 11:30:58.101: INFO: Waiting for pod pod-subpath-test-secret-l7bj to disappear
Nov 23 11:30:58.107: INFO: Pod pod-subpath-test-secret-l7bj no longer exists
STEP: Deleting pod pod-subpath-test-secret-l7bj
Nov 23 11:30:58.108: INFO: Deleting pod "pod-subpath-test-secret-l7bj" in namespace "subpath-6158"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:30:58.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6158" for this suite.

• [SLOW TEST:22.280 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":36,"skipped":780,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:30:58.125: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-6566
STEP: creating replication controller nodeport-test in namespace services-6566
I1123 11:30:58.334378      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6566, replica count: 2
Nov 23 11:31:01.384: INFO: Creating new exec pod
I1123 11:31:01.384808      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 11:31:04.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 23 11:31:06.736: INFO: rc: 1
Nov 23 11:31:06.736: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 11:31:07.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 23 11:31:09.941: INFO: rc: 1
Nov 23 11:31:09.941: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 11:31:10.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Nov 23 11:31:10.976: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 23 11:31:10.976: INFO: stdout: ""
Nov 23 11:31:10.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 100.69.190.196 80'
Nov 23 11:31:11.253: INFO: stderr: "+ nc -zv -t -w 2 100.69.190.196 80\nConnection to 100.69.190.196 80 port [tcp/http] succeeded!\n"
Nov 23 11:31:11.253: INFO: stdout: ""
Nov 23 11:31:11.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 30971'
Nov 23 11:31:11.549: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 30971\nConnection to 192.168.8.54 30971 port [tcp/30971] succeeded!\n"
Nov 23 11:31:11.549: INFO: stdout: ""
Nov 23 11:31:11.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 30971'
Nov 23 11:31:12.061: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 30971\nConnection to 192.168.8.34 30971 port [tcp/30971] succeeded!\n"
Nov 23 11:31:12.061: INFO: stdout: ""
Nov 23 11:31:12.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 30971'
Nov 23 11:31:12.372: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 30971\nConnection to 192.168.8.54 30971 port [tcp/30971] succeeded!\n"
Nov 23 11:31:12.372: INFO: stdout: ""
Nov 23 11:31:12.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6566 execpodppfzw -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 30971'
Nov 23 11:31:12.608: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 30971\nConnection to 192.168.8.34 30971 port [tcp/30971] succeeded!\n"
Nov 23 11:31:12.608: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:31:12.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6566" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.498 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":37,"skipped":782,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:31:12.624: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 23 11:31:12.777: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:31:36.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2" for this suite.

• [SLOW TEST:23.698 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":38,"skipped":792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:31:36.323: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:31:36.478: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 23 11:31:36.489: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 23 11:31:41.494: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 23 11:31:41.494: INFO: Creating deployment "test-rolling-update-deployment"
Nov 23 11:31:41.505: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 23 11:31:41.523: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 23 11:31:43.532: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 23 11:31:43.535: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 23 11:31:43.543: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2180 /apis/apps/v1/namespaces/deployment-2180/deployments/test-rolling-update-deployment 4fa6e078-19a5-446d-af56-d79b10793950 45984 1 2020-11-23 11:31:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-11-23 11:31:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 11:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005916ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-23 11:31:41 +0000 UTC,LastTransitionTime:2020-11-23 11:31:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-11-23 11:31:42 +0000 UTC,LastTransitionTime:2020-11-23 11:31:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 23 11:31:43.546: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-2180 /apis/apps/v1/namespaces/deployment-2180/replicasets/test-rolling-update-deployment-c4cb8d6d9 fee9ae7e-1a43-4081-a943-0e9a232bc671 45973 1 2020-11-23 11:31:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4fa6e078-19a5-446d-af56-d79b10793950 0xc003464100 0xc003464101}] []  [{kube-controller-manager Update apps/v1 2020-11-23 11:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4fa6e078-19a5-446d-af56-d79b10793950\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003464188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 23 11:31:43.546: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 23 11:31:43.546: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2180 /apis/apps/v1/namespaces/deployment-2180/replicasets/test-rolling-update-controller 02f18b9f-269c-42fa-826b-d4eff2a58158 45982 2 2020-11-23 11:31:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4fa6e078-19a5-446d-af56-d79b10793950 0xc000055eb7 0xc000055eb8}] []  [{e2e.test Update apps/v1 2020-11-23 11:31:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 11:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4fa6e078-19a5-446d-af56-d79b10793950\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003464088 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 11:31:43.550: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-2vxgw" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-2vxgw test-rolling-update-deployment-c4cb8d6d9- deployment-2180 /api/v1/namespaces/deployment-2180/pods/test-rolling-update-deployment-c4cb8d6d9-2vxgw 12f673ff-5379-49a9-af5b-521ebcb0c4cf 45972 0 2020-11-23 11:31:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:100.96.2.197/32 cni.projectcalico.org/podIPs:100.96.2.197/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 fee9ae7e-1a43-4081-a943-0e9a232bc671 0xc0034647f0 0xc0034647f1}] []  [{kube-controller-manager Update v1 2020-11-23 11:31:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fee9ae7e-1a43-4081-a943-0e9a232bc671\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 11:31:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 11:31:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cmr8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cmr8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cmr8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:31:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:31:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:31:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 11:31:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.197,StartTime:2020-11-23 11:31:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 11:31:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://35792673bb59fb0170b7b4e339f0a95c5a2fdb6f837f40092ca36aa226dd082c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:31:43.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2180" for this suite.

• [SLOW TEST:7.237 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":39,"skipped":846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:31:43.560: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:31:43.739: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-6ff2c098-d282-43ed-9395-2740e4b33abc" in namespace "security-context-test-4027" to be "Succeeded or Failed"
Nov 23 11:31:43.743: INFO: Pod "busybox-readonly-false-6ff2c098-d282-43ed-9395-2740e4b33abc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618199ms
Nov 23 11:31:45.748: INFO: Pod "busybox-readonly-false-6ff2c098-d282-43ed-9395-2740e4b33abc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008311692s
Nov 23 11:31:47.752: INFO: Pod "busybox-readonly-false-6ff2c098-d282-43ed-9395-2740e4b33abc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012613023s
Nov 23 11:31:47.752: INFO: Pod "busybox-readonly-false-6ff2c098-d282-43ed-9395-2740e4b33abc" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:31:47.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4027" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":874,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:31:47.762: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-4e537dcc-5515-4e27-89d2-611b90c15d5a
STEP: Creating a pod to test consume configMaps
Nov 23 11:31:47.923: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4" in namespace "projected-6722" to be "Succeeded or Failed"
Nov 23 11:31:47.933: INFO: Pod "pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.229318ms
Nov 23 11:31:49.940: INFO: Pod "pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016099497s
STEP: Saw pod success
Nov 23 11:31:49.940: INFO: Pod "pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4" satisfied condition "Succeeded or Failed"
Nov 23 11:31:49.942: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 11:31:49.970: INFO: Waiting for pod pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4 to disappear
Nov 23 11:31:49.975: INFO: Pod pod-projected-configmaps-f1bb864a-9797-4d66-9027-545b32811fe4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:31:49.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6722" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":877,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:31:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7053
STEP: creating service affinity-nodeport-transition in namespace services-7053
STEP: creating replication controller affinity-nodeport-transition in namespace services-7053
I1123 11:31:50.162238      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7053, replica count: 3
I1123 11:31:53.212841      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 11:31:53.222: INFO: Creating new exec pod
Nov 23 11:31:56.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov 23 11:31:58.486: INFO: rc: 1
Nov 23 11:31:58.486: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 11:31:59.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov 23 11:32:01.716: INFO: rc: 1
Nov 23 11:32:01.716: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 11:32:02.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Nov 23 11:32:02.727: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 23 11:32:02.727: INFO: stdout: ""
Nov 23 11:32:02.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 100.68.191.68 80'
Nov 23 11:32:02.990: INFO: stderr: "+ nc -zv -t -w 2 100.68.191.68 80\nConnection to 100.68.191.68 80 port [tcp/http] succeeded!\n"
Nov 23 11:32:02.990: INFO: stdout: ""
Nov 23 11:32:02.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 30887'
Nov 23 11:32:03.236: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 30887\nConnection to 192.168.8.54 30887 port [tcp/30887] succeeded!\n"
Nov 23 11:32:03.236: INFO: stdout: ""
Nov 23 11:32:03.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 30887'
Nov 23 11:32:03.480: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 30887\nConnection to 192.168.8.34 30887 port [tcp/30887] succeeded!\n"
Nov 23 11:32:03.480: INFO: stdout: ""
Nov 23 11:32:03.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 30887'
Nov 23 11:32:03.704: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 30887\nConnection to 192.168.8.54 30887 port [tcp/30887] succeeded!\n"
Nov 23 11:32:03.704: INFO: stdout: ""
Nov 23 11:32:03.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 30887'
Nov 23 11:32:03.896: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 30887\nConnection to 192.168.8.34 30887 port [tcp/30887] succeeded!\n"
Nov 23 11:32:03.897: INFO: stdout: ""
Nov 23 11:32:03.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.54:30887/ ; done'
Nov 23 11:32:04.220: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n"
Nov 23 11:32:04.220: INFO: stdout: "\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq"
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:04.220: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.54:30887/ ; done'
Nov 23 11:32:34.552: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n"
Nov 23 11:32:34.552: INFO: stdout: "\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-zkbk4\naffinity-nodeport-transition-zkbk4\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-zkbk4\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-zkbk4\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-sdr9k\naffinity-nodeport-transition-sdr9k"
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-zkbk4
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-zkbk4
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-zkbk4
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-zkbk4
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.552: INFO: Received response from host: affinity-nodeport-transition-sdr9k
Nov 23 11:32:34.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-7053 execpod-affinityq9c9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.54:30887/ ; done'
Nov 23 11:32:34.923: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:30887/\n"
Nov 23 11:32:34.924: INFO: stdout: "\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq\naffinity-nodeport-transition-v78mq"
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Received response from host: affinity-nodeport-transition-v78mq
Nov 23 11:32:34.924: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7053, will wait for the garbage collector to delete the pods
Nov 23 11:32:35.025: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.730245ms
Nov 23 11:32:35.625: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.296181ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:32:50.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7053" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:60.999 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":42,"skipped":882,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:32:50.988: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-e8274952-03fe-46ad-b313-91497b4cbc67
STEP: Creating a pod to test consume configMaps
Nov 23 11:32:51.159: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f" in namespace "configmap-2999" to be "Succeeded or Failed"
Nov 23 11:32:51.163: INFO: Pod "pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.808817ms
Nov 23 11:32:53.166: INFO: Pod "pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007505821s
STEP: Saw pod success
Nov 23 11:32:53.167: INFO: Pod "pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f" satisfied condition "Succeeded or Failed"
Nov 23 11:32:53.171: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 11:32:53.192: INFO: Waiting for pod pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f to disappear
Nov 23 11:32:53.196: INFO: Pod pod-configmaps-f4cc6069-c43c-4a7c-80fc-37dd68df4c8f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:32:53.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2999" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":882,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:32:53.206: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:32:53.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8529" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":44,"skipped":890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:32:53.391: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 23 11:32:56.104: INFO: Successfully updated pod "annotationupdatec90d5de2-9a93-4adf-b196-47e1b7546ffd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:33:00.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7204" for this suite.

• [SLOW TEST:6.774 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":45,"skipped":927,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:33:00.166: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:33:00.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04" in namespace "projected-4750" to be "Succeeded or Failed"
Nov 23 11:33:00.335: INFO: Pod "downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13426ms
Nov 23 11:33:02.341: INFO: Pod "downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010052282s
STEP: Saw pod success
Nov 23 11:33:02.341: INFO: Pod "downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04" satisfied condition "Succeeded or Failed"
Nov 23 11:33:02.346: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04 container client-container: <nil>
STEP: delete the pod
Nov 23 11:33:02.410: INFO: Waiting for pod downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04 to disappear
Nov 23 11:33:02.420: INFO: Pod downwardapi-volume-ee7cff04-0be8-4f02-8ca1-6305c668df04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:33:02.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4750" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":933,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:33:02.437: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:33:07.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7544" for this suite.

• [SLOW TEST:5.408 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":47,"skipped":938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:33:07.845: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 23 11:33:10.547: INFO: Successfully updated pod "pod-update-8c69b8d2-fe93-4f2e-903e-1cadc956902c"
STEP: verifying the updated pod is in kubernetes
Nov 23 11:33:10.561: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:33:10.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2589" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":966,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:33:10.579: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-bab4432d-1d9c-4735-834f-4052fb09c6a1 in namespace container-probe-4412
Nov 23 11:33:12.759: INFO: Started pod test-webserver-bab4432d-1d9c-4735-834f-4052fb09c6a1 in namespace container-probe-4412
STEP: checking the pod's current state and verifying that restartCount is present
Nov 23 11:33:12.763: INFO: Initial restart count of pod test-webserver-bab4432d-1d9c-4735-834f-4052fb09c6a1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:13.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4412" for this suite.

• [SLOW TEST:242.843 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":968,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:13.423: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 11:37:14.182: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 11:37:17.198: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:37:17.202: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9230-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:18.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3305" for this suite.
STEP: Destroying namespace "webhook-3305-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.229 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":50,"skipped":987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:18.652: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 23 11:37:18.850: INFO: Waiting up to 5m0s for pod "pod-01d8e776-c88c-4667-97a5-2d105c45d625" in namespace "emptydir-9521" to be "Succeeded or Failed"
Nov 23 11:37:18.868: INFO: Pod "pod-01d8e776-c88c-4667-97a5-2d105c45d625": Phase="Pending", Reason="", readiness=false. Elapsed: 17.155503ms
Nov 23 11:37:20.872: INFO: Pod "pod-01d8e776-c88c-4667-97a5-2d105c45d625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021136449s
STEP: Saw pod success
Nov 23 11:37:20.872: INFO: Pod "pod-01d8e776-c88c-4667-97a5-2d105c45d625" satisfied condition "Succeeded or Failed"
Nov 23 11:37:20.875: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-01d8e776-c88c-4667-97a5-2d105c45d625 container test-container: <nil>
STEP: delete the pod
Nov 23 11:37:20.905: INFO: Waiting for pod pod-01d8e776-c88c-4667-97a5-2d105c45d625 to disappear
Nov 23 11:37:20.909: INFO: Pod pod-01d8e776-c88c-4667-97a5-2d105c45d625 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:20.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9521" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":1013,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:20.920: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 23 11:37:21.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-1566'
Nov 23 11:37:21.532: INFO: stderr: ""
Nov 23 11:37:21.532: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 23 11:37:21.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1566'
Nov 23 11:37:21.813: INFO: stderr: ""
Nov 23 11:37:21.813: INFO: stdout: "update-demo-nautilus-wjcpw update-demo-nautilus-z4svf "
Nov 23 11:37:21.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-wjcpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:21.927: INFO: stderr: ""
Nov 23 11:37:21.927: INFO: stdout: ""
Nov 23 11:37:21.927: INFO: update-demo-nautilus-wjcpw is created but not running
Nov 23 11:37:26.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1566'
Nov 23 11:37:27.044: INFO: stderr: ""
Nov 23 11:37:27.044: INFO: stdout: "update-demo-nautilus-wjcpw update-demo-nautilus-z4svf "
Nov 23 11:37:27.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-wjcpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:27.151: INFO: stderr: ""
Nov 23 11:37:27.151: INFO: stdout: "true"
Nov 23 11:37:27.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-wjcpw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:27.269: INFO: stderr: ""
Nov 23 11:37:27.269: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:37:27.269: INFO: validating pod update-demo-nautilus-wjcpw
Nov 23 11:37:27.275: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:37:27.275: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:37:27.275: INFO: update-demo-nautilus-wjcpw is verified up and running
Nov 23 11:37:27.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-z4svf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:27.381: INFO: stderr: ""
Nov 23 11:37:27.381: INFO: stdout: "true"
Nov 23 11:37:27.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-z4svf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:27.512: INFO: stderr: ""
Nov 23 11:37:27.512: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:37:27.512: INFO: validating pod update-demo-nautilus-z4svf
Nov 23 11:37:27.520: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:37:27.520: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:37:27.520: INFO: update-demo-nautilus-z4svf is verified up and running
STEP: scaling down the replication controller
Nov 23 11:37:27.523: INFO: scanned /root for discovery docs: <nil>
Nov 23 11:37:27.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1566'
Nov 23 11:37:28.671: INFO: stderr: ""
Nov 23 11:37:28.671: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 23 11:37:28.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1566'
Nov 23 11:37:28.778: INFO: stderr: ""
Nov 23 11:37:28.778: INFO: stdout: "update-demo-nautilus-wjcpw update-demo-nautilus-z4svf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 23 11:37:33.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1566'
Nov 23 11:37:33.910: INFO: stderr: ""
Nov 23 11:37:33.910: INFO: stdout: "update-demo-nautilus-z4svf "
Nov 23 11:37:33.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-z4svf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:34.043: INFO: stderr: ""
Nov 23 11:37:34.043: INFO: stdout: "true"
Nov 23 11:37:34.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-z4svf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:34.189: INFO: stderr: ""
Nov 23 11:37:34.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:37:34.189: INFO: validating pod update-demo-nautilus-z4svf
Nov 23 11:37:34.198: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:37:34.198: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:37:34.198: INFO: update-demo-nautilus-z4svf is verified up and running
STEP: scaling up the replication controller
Nov 23 11:37:34.203: INFO: scanned /root for discovery docs: <nil>
Nov 23 11:37:34.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1566'
Nov 23 11:37:34.387: INFO: stderr: ""
Nov 23 11:37:34.387: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 23 11:37:34.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1566'
Nov 23 11:37:34.541: INFO: stderr: ""
Nov 23 11:37:34.541: INFO: stdout: "update-demo-nautilus-fvdsz update-demo-nautilus-z4svf "
Nov 23 11:37:34.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-fvdsz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:34.670: INFO: stderr: ""
Nov 23 11:37:34.670: INFO: stdout: ""
Nov 23 11:37:34.670: INFO: update-demo-nautilus-fvdsz is created but not running
Nov 23 11:37:39.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1566'
Nov 23 11:37:39.785: INFO: stderr: ""
Nov 23 11:37:39.785: INFO: stdout: "update-demo-nautilus-fvdsz update-demo-nautilus-z4svf "
Nov 23 11:37:39.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-fvdsz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:39.904: INFO: stderr: ""
Nov 23 11:37:39.904: INFO: stdout: "true"
Nov 23 11:37:39.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-fvdsz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:40.012: INFO: stderr: ""
Nov 23 11:37:40.012: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:37:40.012: INFO: validating pod update-demo-nautilus-fvdsz
Nov 23 11:37:40.017: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:37:40.017: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:37:40.017: INFO: update-demo-nautilus-fvdsz is verified up and running
Nov 23 11:37:40.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-z4svf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:40.141: INFO: stderr: ""
Nov 23 11:37:40.141: INFO: stdout: "true"
Nov 23 11:37:40.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-z4svf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1566'
Nov 23 11:37:40.262: INFO: stderr: ""
Nov 23 11:37:40.262: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:37:40.262: INFO: validating pod update-demo-nautilus-z4svf
Nov 23 11:37:40.267: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:37:40.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:37:40.267: INFO: update-demo-nautilus-z4svf is verified up and running
STEP: using delete to clean up resources
Nov 23 11:37:40.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-1566'
Nov 23 11:37:40.402: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 11:37:40.402: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 23 11:37:40.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1566'
Nov 23 11:37:40.523: INFO: stderr: "No resources found in kubectl-1566 namespace.\n"
Nov 23 11:37:40.523: INFO: stdout: ""
Nov 23 11:37:40.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -l name=update-demo --namespace=kubectl-1566 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 23 11:37:40.635: INFO: stderr: ""
Nov 23 11:37:40.635: INFO: stdout: "update-demo-nautilus-fvdsz\nupdate-demo-nautilus-z4svf\n"
Nov 23 11:37:41.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1566'
Nov 23 11:37:41.290: INFO: stderr: "No resources found in kubectl-1566 namespace.\n"
Nov 23 11:37:41.290: INFO: stdout: ""
Nov 23 11:37:41.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -l name=update-demo --namespace=kubectl-1566 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 23 11:37:41.416: INFO: stderr: ""
Nov 23 11:37:41.416: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:41.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1566" for this suite.

• [SLOW TEST:20.507 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":52,"skipped":1020,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:41.427: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 23 11:37:41.589: INFO: Waiting up to 5m0s for pod "downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029" in namespace "downward-api-5084" to be "Succeeded or Failed"
Nov 23 11:37:41.600: INFO: Pod "downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029": Phase="Pending", Reason="", readiness=false. Elapsed: 10.251478ms
Nov 23 11:37:43.604: INFO: Pod "downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01492986s
STEP: Saw pod success
Nov 23 11:37:43.604: INFO: Pod "downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029" satisfied condition "Succeeded or Failed"
Nov 23 11:37:43.609: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029 container dapi-container: <nil>
STEP: delete the pod
Nov 23 11:37:43.633: INFO: Waiting for pod downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029 to disappear
Nov 23 11:37:43.639: INFO: Pod downward-api-4c9a4020-0199-4658-bb2f-866c96ce7029 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:43.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5084" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":53,"skipped":1023,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:43.651: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-2404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 23 11:37:43.832: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 23 11:37:43.836: INFO: starting watch
STEP: patching
STEP: updating
Nov 23 11:37:43.847: INFO: waiting for watch events with expected annotations
Nov 23 11:37:43.847: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2404" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":54,"skipped":1027,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:43.887: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 23 11:37:46.060: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:46.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4112" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":1031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:46.086: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 11:37:46.703: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 23 11:37:48.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728266, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728266, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728266, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728266, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 11:37:51.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 23 11:37:53.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 attach --namespace=webhook-799 to-be-attached-pod -i -c=container1'
Nov 23 11:37:53.954: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:53.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-799" for this suite.
STEP: Destroying namespace "webhook-799-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.956 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":56,"skipped":1081,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:54.042: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:37:54.243: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e" in namespace "downward-api-154" to be "Succeeded or Failed"
Nov 23 11:37:54.246: INFO: Pod "downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855363ms
Nov 23 11:37:56.250: INFO: Pod "downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006874162s
STEP: Saw pod success
Nov 23 11:37:56.250: INFO: Pod "downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e" satisfied condition "Succeeded or Failed"
Nov 23 11:37:56.253: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e container client-container: <nil>
STEP: delete the pod
Nov 23 11:37:56.278: INFO: Waiting for pod downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e to disappear
Nov 23 11:37:56.282: INFO: Pod downwardapi-volume-df8d3fee-3748-4c12-aec4-f6a36547193e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:37:56.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-154" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":1094,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:37:56.297: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:37:56.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8" in namespace "projected-4003" to be "Succeeded or Failed"
Nov 23 11:37:56.501: INFO: Pod "downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.975767ms
Nov 23 11:37:58.505: INFO: Pod "downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006748815s
Nov 23 11:38:00.510: INFO: Pod "downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011413786s
STEP: Saw pod success
Nov 23 11:38:00.510: INFO: Pod "downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8" satisfied condition "Succeeded or Failed"
Nov 23 11:38:00.515: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8 container client-container: <nil>
STEP: delete the pod
Nov 23 11:38:00.541: INFO: Waiting for pod downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8 to disappear
Nov 23 11:38:00.547: INFO: Pod downwardapi-volume-b45a4e33-15cc-4f7e-b2f7-dd1365c442a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:38:00.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4003" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":58,"skipped":1106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:38:00.561: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9501
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 23 11:38:00.703: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 11:38:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:38:23.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9501" for this suite.

• [SLOW TEST:22.949 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":59,"skipped":1156,"failed":0}
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:38:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:40:23.691: INFO: Deleting pod "var-expansion-a283f386-2198-40d3-8ce5-15bee33ee7ce" in namespace "var-expansion-8505"
Nov 23 11:40:23.698: INFO: Wait up to 5m0s for pod "var-expansion-a283f386-2198-40d3-8ce5-15bee33ee7ce" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:25.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8505" for this suite.

• [SLOW TEST:122.206 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":60,"skipped":1156,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:25.717: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 23 11:40:26.402: INFO: starting watch
STEP: patching
STEP: updating
Nov 23 11:40:26.415: INFO: waiting for watch events with expected annotations
Nov 23 11:40:26.415: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:26.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9126" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":61,"skipped":1171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:26.485: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 23 11:40:26.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7886'
Nov 23 11:40:26.797: INFO: stderr: ""
Nov 23 11:40:26.797: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Nov 23 11:40:26.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete pods e2e-test-httpd-pod --namespace=kubectl-7886'
Nov 23 11:40:38.952: INFO: stderr: ""
Nov 23 11:40:38.952: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:38.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7886" for this suite.

• [SLOW TEST:12.479 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":62,"skipped":1198,"failed":0}
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:38.964: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 23 11:40:39.150: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1056 /api/v1/namespaces/watch-1056/configmaps/e2e-watch-test-resource-version 65897c0d-c222-48d3-a59f-5a396bcd54b4 48469 0 2020-11-23 11:40:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-23 11:40:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 11:40:39.150: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1056 /api/v1/namespaces/watch-1056/configmaps/e2e-watch-test-resource-version 65897c0d-c222-48d3-a59f-5a396bcd54b4 48470 0 2020-11-23 11:40:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-11-23 11:40:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:39.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1056" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":63,"skipped":1198,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:39.163: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 23 11:40:39.345: INFO: Waiting up to 5m0s for pod "pod-30fc864e-2c17-4835-b372-fa141b7dd7b2" in namespace "emptydir-7467" to be "Succeeded or Failed"
Nov 23 11:40:39.351: INFO: Pod "pod-30fc864e-2c17-4835-b372-fa141b7dd7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.478052ms
Nov 23 11:40:41.360: INFO: Pod "pod-30fc864e-2c17-4835-b372-fa141b7dd7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014300699s
STEP: Saw pod success
Nov 23 11:40:41.360: INFO: Pod "pod-30fc864e-2c17-4835-b372-fa141b7dd7b2" satisfied condition "Succeeded or Failed"
Nov 23 11:40:41.363: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-30fc864e-2c17-4835-b372-fa141b7dd7b2 container test-container: <nil>
STEP: delete the pod
Nov 23 11:40:41.408: INFO: Waiting for pod pod-30fc864e-2c17-4835-b372-fa141b7dd7b2 to disappear
Nov 23 11:40:41.413: INFO: Pod pod-30fc864e-2c17-4835-b372-fa141b7dd7b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:41.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7467" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:41.424: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:40:41.613: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e" in namespace "projected-802" to be "Succeeded or Failed"
Nov 23 11:40:41.619: INFO: Pod "downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.524721ms
Nov 23 11:40:43.625: INFO: Pod "downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01198122s
STEP: Saw pod success
Nov 23 11:40:43.625: INFO: Pod "downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e" satisfied condition "Succeeded or Failed"
Nov 23 11:40:43.634: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e container client-container: <nil>
STEP: delete the pod
Nov 23 11:40:43.680: INFO: Waiting for pod downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e to disappear
Nov 23 11:40:43.685: INFO: Pod downwardapi-volume-fa599bc2-06fd-48be-b4d8-d48836211c3e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-802" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":1264,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Nov 23 11:40:43.854: INFO: Waiting up to 5m0s for pod "var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd" in namespace "var-expansion-280" to be "Succeeded or Failed"
Nov 23 11:40:43.858: INFO: Pod "var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.380413ms
Nov 23 11:40:45.864: INFO: Pod "var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009519787s
STEP: Saw pod success
Nov 23 11:40:45.864: INFO: Pod "var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd" satisfied condition "Succeeded or Failed"
Nov 23 11:40:45.867: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd container dapi-container: <nil>
STEP: delete the pod
Nov 23 11:40:45.888: INFO: Waiting for pod var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd to disappear
Nov 23 11:40:45.892: INFO: Pod var-expansion-d206df6d-bc92-4a3c-80ee-27f9de7531dd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:45.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-280" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:45.901: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Nov 23 11:40:46.061: INFO: Waiting up to 5m0s for pod "var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb" in namespace "var-expansion-3261" to be "Succeeded or Failed"
Nov 23 11:40:46.064: INFO: Pod "var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.906738ms
Nov 23 11:40:48.068: INFO: Pod "var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007121506s
Nov 23 11:40:50.074: INFO: Pod "var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013369356s
STEP: Saw pod success
Nov 23 11:40:50.074: INFO: Pod "var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb" satisfied condition "Succeeded or Failed"
Nov 23 11:40:50.079: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb container dapi-container: <nil>
STEP: delete the pod
Nov 23 11:40:50.100: INFO: Waiting for pod var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb to disappear
Nov 23 11:40:50.103: INFO: Pod var-expansion-4453ef5f-d912-4f0a-b51e-8778766fafbb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:40:50.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3261" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":67,"skipped":1316,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:40:50.113: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 23 11:40:50.253: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Nov 23 11:40:50.981: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 23 11:41:04.674: INFO: Waited 11.616529266s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:41:05.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-758" for this suite.

• [SLOW TEST:15.419 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":68,"skipped":1325,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:41:05.532: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-1512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 23 11:41:05.812: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 23 11:42:05.882: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:42:05.886: INFO: Starting informer...
STEP: Starting pod...
Nov 23 11:42:06.109: INFO: Pod is running on dvi-conformance-1606121581-vsp1-group1-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 23 11:42:06.130: INFO: Pod wasn't evicted. Proceeding
Nov 23 11:42:06.130: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 23 11:43:21.194: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:21.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1512" for this suite.

• [SLOW TEST:135.673 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":69,"skipped":1332,"failed":0}
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:21.206: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-4588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 23 11:43:21.361: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 23 11:43:21.367: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 23 11:43:21.367: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 23 11:43:21.381: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 23 11:43:21.381: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 23 11:43:21.399: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 23 11:43:21.399: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 23 11:43:28.464: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:28.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4588" for this suite.

• [SLOW TEST:7.294 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":70,"skipped":1332,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:28.501: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:43:28.663: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5" in namespace "downward-api-4077" to be "Succeeded or Failed"
Nov 23 11:43:28.667: INFO: Pod "downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.429108ms
Nov 23 11:43:30.670: INFO: Pod "downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007193969s
STEP: Saw pod success
Nov 23 11:43:30.670: INFO: Pod "downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5" satisfied condition "Succeeded or Failed"
Nov 23 11:43:30.673: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5 container client-container: <nil>
STEP: delete the pod
Nov 23 11:43:30.782: INFO: Waiting for pod downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5 to disappear
Nov 23 11:43:30.794: INFO: Pod downwardapi-volume-a3226b41-2ba2-4686-a9b5-0fa82267d3c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:30.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4077" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:30.825: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8459.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8459.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8459.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8459.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8459.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8459.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 11:43:33.221: INFO: DNS probes using dns-8459/dns-test-a72d5b9b-b4eb-41ea-ab3b-c450a3c203f9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:33.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8459" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":72,"skipped":1361,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:33.331: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:35.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3666" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":73,"skipped":1364,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:35.574: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Nov 23 11:43:35.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-7328'
Nov 23 11:43:36.089: INFO: stderr: ""
Nov 23 11:43:36.089: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 23 11:43:36.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7328'
Nov 23 11:43:36.246: INFO: stderr: ""
Nov 23 11:43:36.246: INFO: stdout: "update-demo-nautilus-7sgq2 update-demo-nautilus-m28zh "
Nov 23 11:43:36.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-7sgq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7328'
Nov 23 11:43:36.369: INFO: stderr: ""
Nov 23 11:43:36.369: INFO: stdout: ""
Nov 23 11:43:36.369: INFO: update-demo-nautilus-7sgq2 is created but not running
Nov 23 11:43:41.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7328'
Nov 23 11:43:41.494: INFO: stderr: ""
Nov 23 11:43:41.494: INFO: stdout: "update-demo-nautilus-7sgq2 update-demo-nautilus-m28zh "
Nov 23 11:43:41.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-7sgq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7328'
Nov 23 11:43:41.619: INFO: stderr: ""
Nov 23 11:43:41.619: INFO: stdout: "true"
Nov 23 11:43:41.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-7sgq2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7328'
Nov 23 11:43:41.737: INFO: stderr: ""
Nov 23 11:43:41.737: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:43:41.737: INFO: validating pod update-demo-nautilus-7sgq2
Nov 23 11:43:41.745: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:43:41.745: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:43:41.745: INFO: update-demo-nautilus-7sgq2 is verified up and running
Nov 23 11:43:41.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-m28zh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7328'
Nov 23 11:43:41.870: INFO: stderr: ""
Nov 23 11:43:41.870: INFO: stdout: "true"
Nov 23 11:43:41.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods update-demo-nautilus-m28zh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7328'
Nov 23 11:43:41.993: INFO: stderr: ""
Nov 23 11:43:41.993: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 23 11:43:41.993: INFO: validating pod update-demo-nautilus-m28zh
Nov 23 11:43:42.002: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 23 11:43:42.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 23 11:43:42.002: INFO: update-demo-nautilus-m28zh is verified up and running
STEP: using delete to clean up resources
Nov 23 11:43:42.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-7328'
Nov 23 11:43:42.161: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 11:43:42.161: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 23 11:43:42.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7328'
Nov 23 11:43:42.327: INFO: stderr: "No resources found in kubectl-7328 namespace.\n"
Nov 23 11:43:42.327: INFO: stdout: ""
Nov 23 11:43:42.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -l name=update-demo --namespace=kubectl-7328 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 23 11:43:42.477: INFO: stderr: ""
Nov 23 11:43:42.477: INFO: stdout: "update-demo-nautilus-7sgq2\nupdate-demo-nautilus-m28zh\n"
Nov 23 11:43:42.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7328'
Nov 23 11:43:43.123: INFO: stderr: "No resources found in kubectl-7328 namespace.\n"
Nov 23 11:43:43.124: INFO: stdout: ""
Nov 23 11:43:43.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -l name=update-demo --namespace=kubectl-7328 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 23 11:43:43.293: INFO: stderr: ""
Nov 23 11:43:43.293: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7328" for this suite.

• [SLOW TEST:7.745 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":74,"skipped":1368,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:43.319: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5109.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5109.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 11:43:45.553: INFO: DNS probes using dns-5109/dns-test-db83429c-7952-42d7-bf70-1b2add8529ca succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:45.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5109" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":75,"skipped":1375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:45.604: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:43:45.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-2858'
Nov 23 11:43:46.048: INFO: stderr: ""
Nov 23 11:43:46.048: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 23 11:43:46.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-2858'
Nov 23 11:43:46.357: INFO: stderr: ""
Nov 23 11:43:46.357: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 23 11:43:47.362: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 11:43:47.362: INFO: Found 1 / 1
Nov 23 11:43:47.362: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 23 11:43:47.372: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 11:43:47.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 23 11:43:47.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 describe pod agnhost-primary-lgswd --namespace=kubectl-2858'
Nov 23 11:43:47.527: INFO: stderr: ""
Nov 23 11:43:47.527: INFO: stdout: "Name:         agnhost-primary-lgswd\nNamespace:    kubectl-2858\nPriority:     0\nNode:         dvi-conformance-1606121581-vsp1-group1-0/192.168.8.54\nStart Time:   Mon, 23 Nov 2020 11:43:46 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 100.96.2.229/32\n              cni.projectcalico.org/podIPs: 100.96.2.229/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.96.2.229\nIPs:\n  IP:           100.96.2.229\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://8acf86325bb9fd18939240cdf3c6c17104cc0fc88c6ea729a6ab5b26ac88266b\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 23 Nov 2020 11:43:47 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ftrfs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ftrfs:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ftrfs\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2858/agnhost-primary-lgswd to dvi-conformance-1606121581-vsp1-group1-0\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Nov 23 11:43:47.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 describe rc agnhost-primary --namespace=kubectl-2858'
Nov 23 11:43:47.686: INFO: stderr: ""
Nov 23 11:43:47.686: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2858\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-lgswd\n"
Nov 23 11:43:47.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 describe service agnhost-primary --namespace=kubectl-2858'
Nov 23 11:43:47.814: INFO: stderr: ""
Nov 23 11:43:47.814: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2858\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                100.69.35.49\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.2.229:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 23 11:43:47.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 describe node dvi-conformance-1606121581-vsp1-group1-0'
Nov 23 11:43:47.995: INFO: stderr: ""
Nov 23 11:43:47.995: INFO: stdout: "Name:               dvi-conformance-1606121581-vsp1-group1-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=dvi-conformance-1606121581-vsp1-group1-0\n                    kubernetes.io/os=linux\n                    kublr.io/location=vsp1\n                    kublr.io/node-group=group1\n                    kublr.io/node-identifier=f1b679a0-f39b-41be-a3e8-0cc4365c3743\n                    kublr.io/node-ordinal=0\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"16:5d:cc:48:20:7f\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.8.54\n                    kublr.io/agent-status:\n                      {\"clusterName\":\"dvi-conformance-1606121581\",\"location\":\"vsp1\",\"nodeGroup\":\"group1\",\"nodeOrdinal\":0,\"nodeIdentifier\":\"f1b679a0-f39b-41be-a3...\n                    kublr.io/location: vsp1\n                    kublr.io/node-group: group1\n                    kublr.io/node-identifier: f1b679a0-f39b-41be-a3e8-0cc4365c3743\n                    kublr.io/node-ordinal: 0\n                    kublr.io/seeder-status:\n                      {\"clusterName\":\"dvi-conformance-1606121581\",\"location\":\"vsp1\",\"nodeGroup\":\"group1\",\"nodeOrdinal\":0,\"nodeIdentifier\":\"f1b679a0-f39b-41be-a3...\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.96.2.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 23 Nov 2020 09:08:11 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  dvi-conformance-1606121581-vsp1-group1-0\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 23 Nov 2020 11:43:41 +0000\nConditions:\n  Type                             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                             ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable               False   Mon, 23 Nov 2020 09:08:58 +0000   Mon, 23 Nov 2020 09:08:58 +0000   FlannelIsUp                  Flannel is running on this node\n  KublrAgentContainerEngineReady   True    Mon, 23 Nov 2020 11:43:39 +0000   Mon, 23 Nov 2020 09:07:31 +0000   DockerRunning                Docker is running\n  KublrAgentInstanceReady          True    Mon, 23 Nov 2020 11:43:39 +0000   Mon, 23 Nov 2020 09:07:18 +0000   AgentRunning                 Agent is running\n  KublrAgentKubeletReady           True    Mon, 23 Nov 2020 11:43:39 +0000   Mon, 23 Nov 2020 09:07:31 +0000   KubeletRunning               Kubelet is running\n  KublrSeederAgentReady            True    Mon, 23 Nov 2020 11:43:26 +0000   Mon, 23 Nov 2020 09:07:47 +0000   KublrAgentRunning            Kublr agent is running\n  KublrSeederInstanceReady         True    Mon, 23 Nov 2020 11:43:26 +0000   Mon, 23 Nov 2020 09:01:38 +0000   SeederRunning                Seeder is running\n  KublrSeederSecretStoreReady      True    Mon, 23 Nov 2020 11:43:26 +0000   Mon, 23 Nov 2020 09:02:09 +0000   SecretStoreOk                Secret store is running and accessible\n  MemoryPressure                   False   Mon, 23 Nov 2020 11:40:20 +0000   Mon, 23 Nov 2020 09:08:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure                     False   Mon, 23 Nov 2020 11:40:20 +0000   Mon, 23 Nov 2020 09:08:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure                      False   Mon, 23 Nov 2020 11:40:20 +0000   Mon, 23 Nov 2020 09:08:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                            True    Mon, 23 Nov 2020 11:40:20 +0000   Mon, 23 Nov 2020 09:08:31 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  ExternalIP:  192.168.8.54\n  InternalIP:  192.168.8.54\n  Hostname:    dvi-conformance-1606121581-vsp1-group1-0\nCapacity:\n  cpu:                4\n  ephemeral-storage:  8178Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8009040Ki\n  pods:               110\nAllocatable:\n  cpu:                3900m\n  ephemeral-storage:  5570245415\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6858064Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 71cba81124a343e080fd7361e49da267\n  System UUID:                EC290C42-E9B6-B3A4-5DBB-79B4D51CD507\n  Boot ID:                    52124215-7835-4b43-b5c8-91be0e56bd3e\n  Kernel Version:             3.10.0-1062.9.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://1.13.1\n  Kubelet Version:            v1.19.3\n  Kube-Proxy Version:         v1.19.3\nPodCIDR:                      100.96.2.0/24\nPodCIDRs:                     100.96.2.0/24\nProviderID:                   vsphere://420c29ec-b6e9-a4b3-5dbb-79b4d51cd507\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                                                                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                                                                                  ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-92zmd                                                                                                                           40m (1%)      0 (0%)      82Mi (1%)        793Mi (11%)    155m\n  kube-system                 k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0             1m (0%)       0 (0%)      20Mi (0%)        20Mi (0%)      154m\n  kube-system                 kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0                  5m (0%)       250m (6%)   24Mi (0%)        512Mi (7%)     154m\n  kube-system                 kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0    0 (0%)        0 (0%)      32Mi (0%)        32Mi (0%)      154m\n  kube-system                 node-local-dns-vnlcb                                                                                                                  25m (0%)      0 (0%)      5Mi (0%)         30Mi (0%)      155m\n  kubectl-2858                agnhost-primary-lgswd                                                                                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\n  kubelet-test-3666           busybox-host-aliases09837ff0-d84c-4cb7-a5c7-bf2649b4bab0                                                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         14s\n  sonobuoy                    sonobuoy                                                                                                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd                                                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                71m (1%)    250m (6%)\n  memory             163Mi (2%)  1387Mi (20%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Nov 23 11:43:47.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 describe namespace kubectl-2858'
Nov 23 11:43:48.131: INFO: stderr: ""
Nov 23 11:43:48.131: INFO: stdout: "Name:         kubectl-2858\nLabels:       e2e-framework=kubectl\n              e2e-run=46cb2407-ea3a-471f-9b5e-c3c1adc3b604\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:48.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2858" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":76,"skipped":1403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:48.148: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 23 11:43:51.351: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:43:51.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1427" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":77,"skipped":1429,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:43:51.390: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1958
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:43:51.560: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:44:01.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1958" for this suite.

• [SLOW TEST:9.823 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":78,"skipped":1429,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:44:01.214: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8603
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 11:44:02.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728642, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728642, loc:(*time.Location)(0x770e880)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-cbccbf6bb\""}}, CollisionCount:(*int32)(nil)}
Nov 23 11:44:04.752: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728642, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728642, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728642, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741728642, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 11:44:07.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:44:07.754: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9820-crds.webhook.example.com via the AdmissionRegistration API
Nov 23 11:44:18.605: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:44:19.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8603" for this suite.
STEP: Destroying namespace "webhook-8603-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.210 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":79,"skipped":1432,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:44:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 23 11:44:29.679: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1123 11:44:29.679399      23 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1123 11:44:29.679429      23 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1123 11:44:29.679437      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 23 11:44:29.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ph7b" in namespace "gc-423"
Nov 23 11:44:29.698: INFO: Deleting pod "simpletest-rc-to-be-deleted-86dws" in namespace "gc-423"
Nov 23 11:44:29.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkfss" in namespace "gc-423"
Nov 23 11:44:29.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-mx8bc" in namespace "gc-423"
Nov 23 11:44:29.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-nvnvr" in namespace "gc-423"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:44:29.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-423" for this suite.

• [SLOW TEST:10.415 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":80,"skipped":1442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:44:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 23 11:44:30.048: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 23 11:45:30.123: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 23 11:45:30.155: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 23 11:45:30.179: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:46:02.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-218" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:92.526 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":81,"skipped":1464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:46:02.370: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1066
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1066
STEP: creating replication controller externalsvc in namespace services-1066
I1123 11:46:02.583623      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1066, replica count: 2
I1123 11:46:05.635164      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 23 11:46:05.663: INFO: Creating new exec pod
Nov 23 11:46:07.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1066 execpodt2sqr -- /bin/sh -x -c nslookup nodeport-service.services-1066.svc.cluster.local'
Nov 23 11:46:07.926: INFO: stderr: "+ nslookup nodeport-service.services-1066.svc.cluster.local\n"
Nov 23 11:46:07.926: INFO: stdout: ";; Got recursion not available from 169.254.20.10, trying next server\n;; Got recursion not available from 169.254.20.10, trying next server\n;; Got recursion not available from 169.254.20.10, trying next server\n;; Got recursion not available from 169.254.20.10, trying next server\nServer:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-1066.svc.cluster.local\tcanonical name = externalsvc.services-1066.svc.cluster.local.\nName:\texternalsvc.services-1066.svc.cluster.local\nAddress: 100.69.87.249\n;; Got recursion not available from 169.254.20.10, trying next server\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1066, will wait for the garbage collector to delete the pods
Nov 23 11:46:07.988: INFO: Deleting ReplicationController externalsvc took: 6.465143ms
Nov 23 11:46:08.588: INFO: Terminating ReplicationController externalsvc pods took: 600.249798ms
Nov 23 11:46:19.014: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:46:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1066" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.679 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":82,"skipped":1488,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:46:19.050: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6515
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 23 11:46:19.211: INFO: Found 0 stateful pods, waiting for 3
Nov 23 11:46:29.218: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:46:29.218: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:46:29.218: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 23 11:46:29.256: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 23 11:46:39.300: INFO: Updating stateful set ss2
Nov 23 11:46:39.308: INFO: Waiting for Pod statefulset-6515/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Nov 23 11:46:49.385: INFO: Found 1 stateful pods, waiting for 3
Nov 23 11:46:59.390: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:46:59.390: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:46:59.390: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 23 11:46:59.424: INFO: Updating stateful set ss2
Nov 23 11:46:59.448: INFO: Waiting for Pod statefulset-6515/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 23 11:47:09.478: INFO: Updating stateful set ss2
Nov 23 11:47:09.492: INFO: Waiting for StatefulSet statefulset-6515/ss2 to complete update
Nov 23 11:47:09.492: INFO: Waiting for Pod statefulset-6515/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 23 11:47:19.499: INFO: Deleting all statefulset in ns statefulset-6515
Nov 23 11:47:19.502: INFO: Scaling statefulset ss2 to 0
Nov 23 11:47:39.521: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 11:47:39.526: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:47:39.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6515" for this suite.

• [SLOW TEST:80.521 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":83,"skipped":1509,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:47:39.571: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3608
Nov 23 11:47:41.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3608 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 23 11:47:41.991: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 23 11:47:41.991: INFO: stdout: "iptables"
Nov 23 11:47:41.991: INFO: proxyMode: iptables
Nov 23 11:47:41.999: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:47:42.003: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:47:44.003: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:47:44.009: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:47:46.003: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:47:46.011: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:47:48.003: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:47:48.008: INFO: Pod kube-proxy-mode-detector still exists
Nov 23 11:47:50.003: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 23 11:47:50.007: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3608
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3608
I1123 11:47:50.024973      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3608, replica count: 3
I1123 11:47:53.075638      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 11:47:53.081: INFO: Creating new exec pod
Nov 23 11:47:56.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3608 execpod-affinityc8sr7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Nov 23 11:47:56.336: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 23 11:47:56.336: INFO: stdout: ""
Nov 23 11:47:56.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3608 execpod-affinityc8sr7 -- /bin/sh -x -c nc -zv -t -w 2 100.66.6.63 80'
Nov 23 11:47:56.577: INFO: stderr: "+ nc -zv -t -w 2 100.66.6.63 80\nConnection to 100.66.6.63 80 port [tcp/http] succeeded!\n"
Nov 23 11:47:56.577: INFO: stdout: ""
Nov 23 11:47:56.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3608 execpod-affinityc8sr7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.66.6.63:80/ ; done'
Nov 23 11:47:56.932: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n"
Nov 23 11:47:56.932: INFO: stdout: "\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h\naffinity-clusterip-timeout-2ml2h"
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Received response from host: affinity-clusterip-timeout-2ml2h
Nov 23 11:47:56.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3608 execpod-affinityc8sr7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.66.6.63:80/'
Nov 23 11:47:57.168: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n"
Nov 23 11:47:57.168: INFO: stdout: "affinity-clusterip-timeout-2ml2h"
Nov 23 11:48:12.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3608 execpod-affinityc8sr7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.66.6.63:80/'
Nov 23 11:48:12.421: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.66.6.63:80/\n"
Nov 23 11:48:12.421: INFO: stdout: "affinity-clusterip-timeout-t6rqm"
Nov 23 11:48:12.421: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3608, will wait for the garbage collector to delete the pods
Nov 23 11:48:12.520: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 22.803345ms
Nov 23 11:48:13.120: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.206728ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:48:17.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3608" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:37.992 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":84,"skipped":1512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:48:17.564: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2174
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-2846fd3b-a961-4ee6-bfc1-edeaf9625762
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2846fd3b-a961-4ee6-bfc1-edeaf9625762
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:48:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2174" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1543,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:48:21.806: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 11:48:22.393: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 11:48:25.409: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
Nov 23 11:48:34.703: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:48:34.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6628" for this suite.
STEP: Destroying namespace "webhook-6628-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.262 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":86,"skipped":1550,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:48:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-59d7581d-71c7-47bf-aff6-b2902b979982
STEP: Creating a pod to test consume secrets
Nov 23 11:48:35.246: INFO: Waiting up to 5m0s for pod "pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4" in namespace "secrets-6983" to be "Succeeded or Failed"
Nov 23 11:48:35.251: INFO: Pod "pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818324ms
Nov 23 11:48:37.256: INFO: Pod "pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009898558s
STEP: Saw pod success
Nov 23 11:48:37.256: INFO: Pod "pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4" satisfied condition "Succeeded or Failed"
Nov 23 11:48:37.261: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4 container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 11:48:37.291: INFO: Waiting for pod pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4 to disappear
Nov 23 11:48:37.296: INFO: Pod pod-secrets-2f7586fd-2e6d-4597-8d5a-88a0fef664a4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:48:37.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6983" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1561,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:48:37.307: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-42eaa333-33cf-4543-b68f-7dc789003c2b
STEP: Creating a pod to test consume secrets
Nov 23 11:48:37.463: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569" in namespace "projected-8399" to be "Succeeded or Failed"
Nov 23 11:48:37.471: INFO: Pod "pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569": Phase="Pending", Reason="", readiness=false. Elapsed: 7.997955ms
Nov 23 11:48:39.474: INFO: Pod "pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011050043s
STEP: Saw pod success
Nov 23 11:48:39.474: INFO: Pod "pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569" satisfied condition "Succeeded or Failed"
Nov 23 11:48:39.476: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 23 11:48:39.494: INFO: Waiting for pod pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569 to disappear
Nov 23 11:48:39.500: INFO: Pod pod-projected-secrets-fbca80c0-a429-4b5a-935a-f27884a7b569 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:48:39.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8399" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1567,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:48:39.508: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4891
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Nov 23 11:48:39.674: INFO: Found 0 stateful pods, waiting for 3
Nov 23 11:48:49.679: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:48:49.679: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:48:49.679: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:48:49.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-4891 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 11:48:49.967: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 11:48:49.967: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 11:48:49.967: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Nov 23 11:49:00.004: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 23 11:49:10.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-4891 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 11:49:10.268: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 11:49:10.268: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 11:49:10.268: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 11:49:20.287: INFO: Waiting for StatefulSet statefulset-4891/ss2 to complete update
Nov 23 11:49:20.287: INFO: Waiting for Pod statefulset-4891/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Nov 23 11:49:20.287: INFO: Waiting for Pod statefulset-4891/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Nov 23 11:49:30.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-4891 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 11:49:30.526: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 11:49:30.526: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 11:49:30.526: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 11:49:40.556: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 23 11:49:50.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-4891 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 11:49:50.840: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 11:49:50.840: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 11:49:50.840: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 11:50:00.861: INFO: Waiting for StatefulSet statefulset-4891/ss2 to complete update
Nov 23 11:50:00.861: INFO: Waiting for Pod statefulset-4891/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 23 11:50:10.870: INFO: Deleting all statefulset in ns statefulset-4891
Nov 23 11:50:10.872: INFO: Scaling statefulset ss2 to 0
Nov 23 11:50:30.891: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 11:50:30.895: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:50:30.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4891" for this suite.

• [SLOW TEST:111.439 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":89,"skipped":1568,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:50:30.947: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-145 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-145;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-145 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-145;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-145.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-145.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-145.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-145.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-145.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 44.246.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.246.44_udp@PTR;check="$$(dig +tcp +noall +answer +search 44.246.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.246.44_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-145 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-145;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-145 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-145;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-145.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-145.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-145.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-145.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-145.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-145.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-145.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-145.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 44.246.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.246.44_udp@PTR;check="$$(dig +tcp +noall +answer +search 44.246.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.246.44_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 11:50:33.203: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.208: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.212: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.216: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.219: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.223: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.266: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.270: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.273: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.281: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.286: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:33.330: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:50:38.335: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.339: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.343: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.351: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.354: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.397: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.401: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.405: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.409: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.414: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.417: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:38.449: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:50:43.335: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.340: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.352: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.357: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.366: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.397: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.400: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.404: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.407: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.411: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.415: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:43.454: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:50:48.339: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.349: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.353: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.359: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.366: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.370: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.429: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.440: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.447: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.459: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.468: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.479: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:48.528: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:50:53.339: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.344: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.352: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.358: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.366: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.372: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.418: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.427: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.434: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.439: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.444: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.450: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:53.506: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:50:58.334: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.340: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.361: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.367: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.411: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.415: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.419: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.422: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.425: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:50:58.459: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:51:03.336: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.340: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.350: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.354: INFO: Unable to read wheezy_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.358: INFO: Unable to read wheezy_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.385: INFO: Unable to read jessie_udp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.388: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.391: INFO: Unable to read jessie_udp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.393: INFO: Unable to read jessie_tcp@dns-test-service.dns-145 from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.397: INFO: Unable to read jessie_udp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.401: INFO: Unable to read jessie_tcp@dns-test-service.dns-145.svc from pod dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e: the server could not find the requested resource (get pods dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e)
Nov 23 11:51:03.423: INFO: Lookups using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-145 wheezy_tcp@dns-test-service.dns-145 wheezy_udp@dns-test-service.dns-145.svc wheezy_tcp@dns-test-service.dns-145.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-145 jessie_tcp@dns-test-service.dns-145 jessie_udp@dns-test-service.dns-145.svc jessie_tcp@dns-test-service.dns-145.svc]

Nov 23 11:51:08.466: INFO: DNS probes using dns-145/dns-test-989caaa7-bbc2-44b0-8ce2-ca67677c1e0e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:51:08.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-145" for this suite.

• [SLOW TEST:37.684 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":90,"skipped":1569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:51:08.632: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-3306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:51:09.716: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 23 11:51:09.717: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 23 11:51:09.717: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.717: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 23 11:51:09.717: INFO: Checking APIGroup: extensions
Nov 23 11:51:09.718: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Nov 23 11:51:09.718: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Nov 23 11:51:09.718: INFO: extensions/v1beta1 matches extensions/v1beta1
Nov 23 11:51:09.718: INFO: Checking APIGroup: apps
Nov 23 11:51:09.720: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 23 11:51:09.720: INFO: Versions found [{apps/v1 v1}]
Nov 23 11:51:09.720: INFO: apps/v1 matches apps/v1
Nov 23 11:51:09.720: INFO: Checking APIGroup: events.k8s.io
Nov 23 11:51:09.720: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 23 11:51:09.720: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.721: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 23 11:51:09.721: INFO: Checking APIGroup: authentication.k8s.io
Nov 23 11:51:09.722: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 23 11:51:09.722: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.722: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 23 11:51:09.722: INFO: Checking APIGroup: authorization.k8s.io
Nov 23 11:51:09.723: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 23 11:51:09.723: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.723: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 23 11:51:09.723: INFO: Checking APIGroup: autoscaling
Nov 23 11:51:09.724: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov 23 11:51:09.724: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 23 11:51:09.724: INFO: autoscaling/v1 matches autoscaling/v1
Nov 23 11:51:09.724: INFO: Checking APIGroup: batch
Nov 23 11:51:09.724: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 23 11:51:09.724: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 23 11:51:09.724: INFO: batch/v1 matches batch/v1
Nov 23 11:51:09.724: INFO: Checking APIGroup: certificates.k8s.io
Nov 23 11:51:09.725: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 23 11:51:09.725: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.725: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 23 11:51:09.725: INFO: Checking APIGroup: networking.k8s.io
Nov 23 11:51:09.726: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 23 11:51:09.726: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.726: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 23 11:51:09.726: INFO: Checking APIGroup: policy
Nov 23 11:51:09.727: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Nov 23 11:51:09.727: INFO: Versions found [{policy/v1beta1 v1beta1}]
Nov 23 11:51:09.727: INFO: policy/v1beta1 matches policy/v1beta1
Nov 23 11:51:09.727: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 23 11:51:09.728: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 23 11:51:09.728: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.728: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 23 11:51:09.728: INFO: Checking APIGroup: storage.k8s.io
Nov 23 11:51:09.728: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 23 11:51:09.728: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.728: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 23 11:51:09.728: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 23 11:51:09.729: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 23 11:51:09.729: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.729: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 23 11:51:09.729: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 23 11:51:09.730: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 23 11:51:09.730: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.730: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 23 11:51:09.730: INFO: Checking APIGroup: scheduling.k8s.io
Nov 23 11:51:09.730: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 23 11:51:09.730: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.730: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 23 11:51:09.730: INFO: Checking APIGroup: coordination.k8s.io
Nov 23 11:51:09.731: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 23 11:51:09.731: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.731: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 23 11:51:09.731: INFO: Checking APIGroup: node.k8s.io
Nov 23 11:51:09.732: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Nov 23 11:51:09.732: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.732: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Nov 23 11:51:09.732: INFO: Checking APIGroup: discovery.k8s.io
Nov 23 11:51:09.733: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Nov 23 11:51:09.733: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.733: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Nov 23 11:51:09.733: INFO: Checking APIGroup: core.kublr.io
Nov 23 11:51:09.734: INFO: PreferredVersion.GroupVersion: core.kublr.io/v1
Nov 23 11:51:09.734: INFO: Versions found [{core.kublr.io/v1 v1}]
Nov 23 11:51:09.734: INFO: core.kublr.io/v1 matches core.kublr.io/v1
Nov 23 11:51:09.734: INFO: Checking APIGroup: crd.projectcalico.org
Nov 23 11:51:09.734: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 23 11:51:09.734: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 23 11:51:09.734: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 23 11:51:09.734: INFO: Checking APIGroup: feature.crd.kublr.com
Nov 23 11:51:09.735: INFO: PreferredVersion.GroupVersion: feature.crd.kublr.com/v1
Nov 23 11:51:09.735: INFO: Versions found [{feature.crd.kublr.com/v1 v1}]
Nov 23 11:51:09.735: INFO: feature.crd.kublr.com/v1 matches feature.crd.kublr.com/v1
Nov 23 11:51:09.735: INFO: Checking APIGroup: metrics.k8s.io
Nov 23 11:51:09.736: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 23 11:51:09.736: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 23 11:51:09.736: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:51:09.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3306" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":91,"skipped":1593,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:51:09.747: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 23 11:51:09.913: INFO: Waiting up to 5m0s for pod "downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f" in namespace "downward-api-1828" to be "Succeeded or Failed"
Nov 23 11:51:09.917: INFO: Pod "downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107151ms
Nov 23 11:51:11.921: INFO: Pod "downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008387607s
Nov 23 11:51:13.924: INFO: Pod "downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011334338s
STEP: Saw pod success
Nov 23 11:51:13.924: INFO: Pod "downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f" satisfied condition "Succeeded or Failed"
Nov 23 11:51:13.927: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f container dapi-container: <nil>
STEP: delete the pod
Nov 23 11:51:13.967: INFO: Waiting for pod downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f to disappear
Nov 23 11:51:13.973: INFO: Pod downward-api-05f02cc9-09b4-4542-a50b-c337be0b481f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:51:13.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1828" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":92,"skipped":1596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:51:13.986: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:51:14.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6079" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":93,"skipped":1642,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:51:14.202: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-f33b3f08-3de6-493b-9695-d4359175fcae
STEP: Creating a pod to test consume configMaps
Nov 23 11:51:14.353: INFO: Waiting up to 5m0s for pod "pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339" in namespace "configmap-9922" to be "Succeeded or Failed"
Nov 23 11:51:14.356: INFO: Pod "pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877246ms
Nov 23 11:51:16.361: INFO: Pod "pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007490091s
STEP: Saw pod success
Nov 23 11:51:16.361: INFO: Pod "pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339" satisfied condition "Succeeded or Failed"
Nov 23 11:51:16.364: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 11:51:16.393: INFO: Waiting for pod pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339 to disappear
Nov 23 11:51:16.404: INFO: Pod pod-configmaps-bab5092b-b481-4678-b104-7c5c6dafe339 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:51:16.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9922" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1642,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:51:16.414: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 23 11:51:16.847: INFO: Pod name wrapped-volume-race-c597ec96-97a9-489e-997a-00abe5c9f96a: Found 1 pods out of 5
Nov 23 11:51:21.857: INFO: Pod name wrapped-volume-race-c597ec96-97a9-489e-997a-00abe5c9f96a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c597ec96-97a9-489e-997a-00abe5c9f96a in namespace emptydir-wrapper-3275, will wait for the garbage collector to delete the pods
Nov 23 11:51:31.959: INFO: Deleting ReplicationController wrapped-volume-race-c597ec96-97a9-489e-997a-00abe5c9f96a took: 10.764537ms
Nov 23 11:51:32.159: INFO: Terminating ReplicationController wrapped-volume-race-c597ec96-97a9-489e-997a-00abe5c9f96a pods took: 200.205264ms
STEP: Creating RC which spawns configmap-volume pods
Nov 23 11:51:41.085: INFO: Pod name wrapped-volume-race-3359a9fa-cb6a-49eb-a567-c25632387bd4: Found 0 pods out of 5
Nov 23 11:51:46.095: INFO: Pod name wrapped-volume-race-3359a9fa-cb6a-49eb-a567-c25632387bd4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3359a9fa-cb6a-49eb-a567-c25632387bd4 in namespace emptydir-wrapper-3275, will wait for the garbage collector to delete the pods
Nov 23 11:51:56.176: INFO: Deleting ReplicationController wrapped-volume-race-3359a9fa-cb6a-49eb-a567-c25632387bd4 took: 8.240822ms
Nov 23 11:51:56.876: INFO: Terminating ReplicationController wrapped-volume-race-3359a9fa-cb6a-49eb-a567-c25632387bd4 pods took: 700.279412ms
STEP: Creating RC which spawns configmap-volume pods
Nov 23 11:52:11.023: INFO: Pod name wrapped-volume-race-79e37285-512e-44bd-a990-7d40b7c86be7: Found 0 pods out of 5
Nov 23 11:52:16.031: INFO: Pod name wrapped-volume-race-79e37285-512e-44bd-a990-7d40b7c86be7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-79e37285-512e-44bd-a990-7d40b7c86be7 in namespace emptydir-wrapper-3275, will wait for the garbage collector to delete the pods
Nov 23 11:52:26.114: INFO: Deleting ReplicationController wrapped-volume-race-79e37285-512e-44bd-a990-7d40b7c86be7 took: 7.386199ms
Nov 23 11:52:26.814: INFO: Terminating ReplicationController wrapped-volume-race-79e37285-512e-44bd-a990-7d40b7c86be7 pods took: 700.255637ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:30.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3275" for this suite.

• [SLOW TEST:74.470 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":95,"skipped":1648,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:30.884: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:52:31.046: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c" in namespace "downward-api-6922" to be "Succeeded or Failed"
Nov 23 11:52:31.053: INFO: Pod "downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.581274ms
Nov 23 11:52:33.059: INFO: Pod "downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013147794s
STEP: Saw pod success
Nov 23 11:52:33.059: INFO: Pod "downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c" satisfied condition "Succeeded or Failed"
Nov 23 11:52:33.063: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c container client-container: <nil>
STEP: delete the pod
Nov 23 11:52:33.131: INFO: Waiting for pod downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c to disappear
Nov 23 11:52:33.136: INFO: Pod downwardapi-volume-d99f86ce-4967-4da9-a9ed-b751a2a2723c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:33.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6922" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1649,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:33.151: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:52:33.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d" in namespace "projected-7618" to be "Succeeded or Failed"
Nov 23 11:52:33.332: INFO: Pod "downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.821789ms
Nov 23 11:52:35.338: INFO: Pod "downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016449349s
STEP: Saw pod success
Nov 23 11:52:35.338: INFO: Pod "downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d" satisfied condition "Succeeded or Failed"
Nov 23 11:52:35.341: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d container client-container: <nil>
STEP: delete the pod
Nov 23 11:52:35.366: INFO: Waiting for pod downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d to disappear
Nov 23 11:52:35.369: INFO: Pod downwardapi-volume-08e95b5f-d06c-41dd-b504-39bf3e61441d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:35.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7618" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":97,"skipped":1659,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:35.379: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9824
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 23 11:52:35.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-9824'
Nov 23 11:52:35.929: INFO: stderr: ""
Nov 23 11:52:35.929: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 23 11:52:36.933: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 11:52:36.933: INFO: Found 0 / 1
Nov 23 11:52:37.935: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 11:52:37.935: INFO: Found 1 / 1
Nov 23 11:52:37.935: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 23 11:52:37.938: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 11:52:37.938: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 23 11:52:37.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 patch pod agnhost-primary-qsfhz --namespace=kubectl-9824 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 23 11:52:38.056: INFO: stderr: ""
Nov 23 11:52:38.056: INFO: stdout: "pod/agnhost-primary-qsfhz patched\n"
STEP: checking annotations
Nov 23 11:52:38.060: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 11:52:38.060: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:38.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9824" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":98,"skipped":1661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-7e3c8c94-ccc5-47cb-a2c8-f21706154934
STEP: Creating a pod to test consume configMaps
Nov 23 11:52:38.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b" in namespace "configmap-409" to be "Succeeded or Failed"
Nov 23 11:52:38.240: INFO: Pod "pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.34243ms
Nov 23 11:52:40.245: INFO: Pod "pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011753578s
STEP: Saw pod success
Nov 23 11:52:40.245: INFO: Pod "pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b" satisfied condition "Succeeded or Failed"
Nov 23 11:52:40.248: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 11:52:40.271: INFO: Waiting for pod pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b to disappear
Nov 23 11:52:40.280: INFO: Pod pod-configmaps-74d6d368-4ce7-4cd3-82b1-81523d538f2b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:40.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-409" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":99,"skipped":1723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:40.312: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 11:52:40.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80" in namespace "downward-api-988" to be "Succeeded or Failed"
Nov 23 11:52:40.490: INFO: Pod "downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178624ms
Nov 23 11:52:42.494: INFO: Pod "downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010344029s
STEP: Saw pod success
Nov 23 11:52:42.494: INFO: Pod "downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80" satisfied condition "Succeeded or Failed"
Nov 23 11:52:42.498: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80 container client-container: <nil>
STEP: delete the pod
Nov 23 11:52:42.521: INFO: Waiting for pod downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80 to disappear
Nov 23 11:52:42.526: INFO: Pod downwardapi-volume-a3db33e8-36f1-4950-a0ac-3c3f14c4eb80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-988" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1745,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 23 11:52:42.697: INFO: Waiting up to 5m0s for pod "pod-ecd69176-17b0-4d10-8f06-fba253d92ed1" in namespace "emptydir-4975" to be "Succeeded or Failed"
Nov 23 11:52:42.701: INFO: Pod "pod-ecd69176-17b0-4d10-8f06-fba253d92ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50151ms
Nov 23 11:52:44.705: INFO: Pod "pod-ecd69176-17b0-4d10-8f06-fba253d92ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008117417s
STEP: Saw pod success
Nov 23 11:52:44.705: INFO: Pod "pod-ecd69176-17b0-4d10-8f06-fba253d92ed1" satisfied condition "Succeeded or Failed"
Nov 23 11:52:44.712: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-ecd69176-17b0-4d10-8f06-fba253d92ed1 container test-container: <nil>
STEP: delete the pod
Nov 23 11:52:44.752: INFO: Waiting for pod pod-ecd69176-17b0-4d10-8f06-fba253d92ed1 to disappear
Nov 23 11:52:44.759: INFO: Pod pod-ecd69176-17b0-4d10-8f06-fba253d92ed1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:44.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4975" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1759,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:44.773: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3074
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 23 11:52:44.936: INFO: Waiting up to 5m0s for pod "pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c" in namespace "emptydir-3074" to be "Succeeded or Failed"
Nov 23 11:52:44.940: INFO: Pod "pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844315ms
Nov 23 11:52:46.945: INFO: Pod "pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008528001s
STEP: Saw pod success
Nov 23 11:52:46.945: INFO: Pod "pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c" satisfied condition "Succeeded or Failed"
Nov 23 11:52:46.948: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c container test-container: <nil>
STEP: delete the pod
Nov 23 11:52:46.976: INFO: Waiting for pod pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c to disappear
Nov 23 11:52:46.980: INFO: Pod pod-5e7d8aec-9d80-4da3-9fba-c1125af6068c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:52:46.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3074" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:52:46.990: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 23 11:52:47.132: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 23 11:52:47.138: INFO: Waiting for terminating namespaces to be deleted...
Nov 23 11:52:47.140: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-0 before test
Nov 23 11:52:47.147: INFO: canal-92zmd from kube-system started at 2020-11-23 09:08:12 +0000 UTC (3 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 11:52:47.147: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 11:52:47.147: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 11:52:47.147: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 11:52:47.147: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 11:52:47.147: INFO: kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container main ready: true, restart count 0
Nov 23 11:52:47.147: INFO: node-local-dns-vnlcb from kube-system started at 2020-11-23 09:08:12 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 11:52:47.147: INFO: sonobuoy from sonobuoy started at 2020-11-23 11:25:21 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 23 11:52:47.147: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:52:47.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:52:47.147: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 23 11:52:47.147: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-1 before test
Nov 23 11:52:47.156: INFO: canal-pgdpd from kube-system started at 2020-11-23 09:08:13 +0000 UTC (3 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 11:52:47.156: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 11:52:47.156: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 11:52:47.156: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 11:52:47.156: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 11:52:47.156: INFO: kublr-node-name-reporter-c33b8d393ebe42a50f83769b2f12339f5d1f5d10dd75810de5733c313118246a-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container main ready: true, restart count 0
Nov 23 11:52:47.156: INFO: kublr-system-shell-564c49bf57-ksqmb from kube-system started at 2020-11-23 09:09:38 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container shell ready: true, restart count 0
Nov 23 11:52:47.156: INFO: metrics-server-v0.3.6-7d5754b8df-2hd8f from kube-system started at 2020-11-23 11:42:06 +0000 UTC (2 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container metrics-server ready: true, restart count 0
Nov 23 11:52:47.156: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 23 11:52:47.156: INFO: node-local-dns-74tqs from kube-system started at 2020-11-23 09:08:13 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 11:52:47.156: INFO: kublr-operator-69c9677745-zk5lz from kublr started at 2020-11-23 11:19:03 +0000 UTC (1 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container kublr-operator ready: true, restart count 0
Nov 23 11:52:47.156: INFO: sonobuoy-e2e-job-7d6aa2c1eacf4413 from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container e2e ready: true, restart count 0
Nov 23 11:52:47.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:52:47.156: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-sj5vn from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:52:47.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:52:47.156: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e3af77df-0577-42ee-b05e-2b2772723180 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e3af77df-0577-42ee-b05e-2b2772723180 off the node dvi-conformance-1606121581-vsp1-group1-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3af77df-0577-42ee-b05e-2b2772723180
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:57:51.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1488" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.322 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":103,"skipped":1805,"failed":0}
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:57:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-1ea9ccd7-c15f-448a-a150-a90ee48d970c
STEP: Creating a pod to test consume secrets
Nov 23 11:57:51.485: INFO: Waiting up to 5m0s for pod "pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c" in namespace "secrets-5135" to be "Succeeded or Failed"
Nov 23 11:57:51.488: INFO: Pod "pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929453ms
Nov 23 11:57:53.492: INFO: Pod "pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006929244s
STEP: Saw pod success
Nov 23 11:57:53.492: INFO: Pod "pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c" satisfied condition "Succeeded or Failed"
Nov 23 11:57:53.495: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 11:57:53.533: INFO: Waiting for pod pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c to disappear
Nov 23 11:57:53.540: INFO: Pod pod-secrets-aa0ea23d-8e91-411d-af12-fd509afbb59c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:57:53.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5135" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1805,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:57:53.552: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 23 11:57:53.699: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 23 11:57:53.708: INFO: Waiting for terminating namespaces to be deleted...
Nov 23 11:57:53.711: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-0 before test
Nov 23 11:57:53.720: INFO: canal-92zmd from kube-system started at 2020-11-23 09:08:12 +0000 UTC (3 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 11:57:53.720: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 11:57:53.720: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 11:57:53.720: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 11:57:53.720: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 11:57:53.720: INFO: kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container main ready: true, restart count 0
Nov 23 11:57:53.720: INFO: node-local-dns-vnlcb from kube-system started at 2020-11-23 09:08:12 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 11:57:53.720: INFO: pod4 from sched-pred-1488 started at 2020-11-23 11:52:49 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container pod4 ready: true, restart count 0
Nov 23 11:57:53.720: INFO: sonobuoy from sonobuoy started at 2020-11-23 11:25:21 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 23 11:57:53.720: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:57:53.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:57:53.720: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 23 11:57:53.720: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-1 before test
Nov 23 11:57:53.729: INFO: canal-pgdpd from kube-system started at 2020-11-23 09:08:13 +0000 UTC (3 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 11:57:53.729: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 11:57:53.729: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 11:57:53.729: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 11:57:53.729: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 11:57:53.729: INFO: kublr-node-name-reporter-c33b8d393ebe42a50f83769b2f12339f5d1f5d10dd75810de5733c313118246a-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container main ready: true, restart count 0
Nov 23 11:57:53.729: INFO: kublr-system-shell-564c49bf57-ksqmb from kube-system started at 2020-11-23 09:09:38 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container shell ready: true, restart count 0
Nov 23 11:57:53.729: INFO: metrics-server-v0.3.6-7d5754b8df-2hd8f from kube-system started at 2020-11-23 11:42:06 +0000 UTC (2 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container metrics-server ready: true, restart count 0
Nov 23 11:57:53.729: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 23 11:57:53.729: INFO: node-local-dns-74tqs from kube-system started at 2020-11-23 09:08:13 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 11:57:53.729: INFO: kublr-operator-69c9677745-zk5lz from kublr started at 2020-11-23 11:19:03 +0000 UTC (1 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container kublr-operator ready: true, restart count 0
Nov 23 11:57:53.729: INFO: sonobuoy-e2e-job-7d6aa2c1eacf4413 from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container e2e ready: true, restart count 0
Nov 23 11:57:53.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:57:53.729: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-sj5vn from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 11:57:53.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 11:57:53.729: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ce4b5f5d-e020-49f8-94f8-38237be86b7b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ce4b5f5d-e020-49f8-94f8-38237be86b7b off the node dvi-conformance-1606121581-vsp1-group1-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ce4b5f5d-e020-49f8-94f8-38237be86b7b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:58:01.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9246" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.379 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":105,"skipped":1813,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:58:01.931: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-786
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-786
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-786
Nov 23 11:58:02.128: INFO: Found 0 stateful pods, waiting for 1
Nov 23 11:58:12.132: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 23 11:58:12.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 11:58:12.394: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 11:58:12.394: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 11:58:12.394: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 11:58:12.397: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 23 11:58:22.404: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 11:58:22.404: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 11:58:22.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999954s
Nov 23 11:58:23.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997160811s
Nov 23 11:58:24.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992524343s
Nov 23 11:58:25.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987586183s
Nov 23 11:58:26.438: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98349149s
Nov 23 11:58:27.441: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977722065s
Nov 23 11:58:28.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.974174567s
Nov 23 11:58:29.454: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966974854s
Nov 23 11:58:30.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.961136332s
Nov 23 11:58:31.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.449078ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-786
Nov 23 11:58:32.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 11:58:32.731: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 11:58:32.732: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 11:58:32.732: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 11:58:32.738: INFO: Found 1 stateful pods, waiting for 3
Nov 23 11:58:42.745: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:58:42.745: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 11:58:42.745: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 23 11:58:42.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 11:58:43.008: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 11:58:43.008: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 11:58:43.008: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 11:58:43.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 11:58:43.295: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 11:58:43.295: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 11:58:43.295: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 11:58:43.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 11:58:43.551: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 11:58:43.551: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 11:58:43.551: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 11:58:43.551: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 11:58:43.554: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 23 11:58:53.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 11:58:53.562: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 11:58:53.562: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 11:58:53.575: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999543s
Nov 23 11:58:54.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992718545s
Nov 23 11:58:55.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987728017s
Nov 23 11:58:56.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979587063s
Nov 23 11:58:57.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973186609s
Nov 23 11:58:58.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96989671s
Nov 23 11:58:59.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96599106s
Nov 23 11:59:00.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961144956s
Nov 23 11:59:01.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957058549s
Nov 23 11:59:02.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.975177ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-786
Nov 23 11:59:03.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 11:59:03.844: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 11:59:03.844: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 11:59:03.844: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 11:59:03.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 11:59:04.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 11:59:04.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 11:59:04.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 11:59:04.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-786 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 11:59:04.358: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 11:59:04.358: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 11:59:04.358: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 11:59:04.358: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 23 11:59:24.376: INFO: Deleting all statefulset in ns statefulset-786
Nov 23 11:59:24.380: INFO: Scaling statefulset ss to 0
Nov 23 11:59:24.401: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 11:59:24.404: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:59:24.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-786" for this suite.

• [SLOW TEST:82.543 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":106,"skipped":1815,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:59:24.475: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:59:49.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1467" for this suite.

• [SLOW TEST:25.478 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1833,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:59:49.954: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 11:59:51.387: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 23 11:59:53.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729591, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729591, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729591, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729591, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 11:59:56.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 11:59:56.426: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2115-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:59:57.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2890" for this suite.
STEP: Destroying namespace "webhook-2890-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":108,"skipped":1844,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:59:58.026: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 11:59:58.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2408" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":109,"skipped":1860,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 11:59:58.255: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-2f29fc78-f53d-49c9-945a-2d67640ddae2
STEP: Creating a pod to test consume configMaps
Nov 23 11:59:58.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae" in namespace "projected-2917" to be "Succeeded or Failed"
Nov 23 11:59:58.514: INFO: Pod "pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae": Phase="Pending", Reason="", readiness=false. Elapsed: 18.834538ms
Nov 23 12:00:00.519: INFO: Pod "pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023161495s
STEP: Saw pod success
Nov 23 12:00:00.519: INFO: Pod "pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae" satisfied condition "Succeeded or Failed"
Nov 23 12:00:00.539: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:00:00.632: INFO: Waiting for pod pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae to disappear
Nov 23 12:00:00.716: INFO: Pod pod-projected-configmaps-0add33ca-e66a-4b6f-8b85-dfe6c0a45cae no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:00.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2917" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1875,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:00.732: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:01.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":111,"skipped":1891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:01.164: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:00:01.344: INFO: Creating ReplicaSet my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d
Nov 23 12:00:01.355: INFO: Pod name my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d: Found 0 pods out of 1
Nov 23 12:00:06.361: INFO: Pod name my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d: Found 1 pods out of 1
Nov 23 12:00:06.361: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d" is running
Nov 23 12:00:06.368: INFO: Pod "my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d-tmhsg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:00:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:00:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:00:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:00:01 +0000 UTC Reason: Message:}])
Nov 23 12:00:06.368: INFO: Trying to dial the pod
Nov 23 12:00:11.381: INFO: Controller my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d: Got expected result from replica 1 [my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d-tmhsg]: "my-hostname-basic-ad528166-dce3-43fd-91c9-3cacda77294d-tmhsg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:11.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1542" for this suite.

• [SLOW TEST:10.232 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":112,"skipped":1945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:11.396: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 23 12:00:11.548: INFO: Waiting up to 5m0s for pod "downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730" in namespace "downward-api-3959" to be "Succeeded or Failed"
Nov 23 12:00:11.551: INFO: Pod "downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730": Phase="Pending", Reason="", readiness=false. Elapsed: 3.181987ms
Nov 23 12:00:13.556: INFO: Pod "downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007327133s
STEP: Saw pod success
Nov 23 12:00:13.556: INFO: Pod "downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730" satisfied condition "Succeeded or Failed"
Nov 23 12:00:13.559: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730 container dapi-container: <nil>
STEP: delete the pod
Nov 23 12:00:13.580: INFO: Waiting for pod downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730 to disappear
Nov 23 12:00:13.587: INFO: Pod downward-api-cd3295e6-a57a-4a41-ae33-2bb36b8aa730 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:13.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3959" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":113,"skipped":1979,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:13.599: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:00:14.186: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:00:17.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
Nov 23 12:00:18.237: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:18.347: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:19.453: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:20.572: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:21.660: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:22.781: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:23.869: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:24.963: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:00:26.077: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 23 12:00:27.169: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:27.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2928" for this suite.
STEP: Destroying namespace "webhook-2928-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.675 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":114,"skipped":1986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:27.276: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:00:27.426: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 23 12:00:28.480: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:29.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4609" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":115,"skipped":2017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:29.498: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:29.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9447" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":116,"skipped":2045,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:29.672: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-jx2j
STEP: Creating a pod to test atomic-volume-subpath
Nov 23 12:00:29.840: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jx2j" in namespace "subpath-780" to be "Succeeded or Failed"
Nov 23 12:00:29.846: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Pending", Reason="", readiness=false. Elapsed: 5.533533ms
Nov 23 12:00:31.851: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 2.011179344s
Nov 23 12:00:33.855: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 4.014735692s
Nov 23 12:00:35.860: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 6.01950615s
Nov 23 12:00:37.863: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 8.022961605s
Nov 23 12:00:39.868: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 10.027870743s
Nov 23 12:00:41.872: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 12.03193769s
Nov 23 12:00:43.876: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 14.03571961s
Nov 23 12:00:45.880: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 16.040206221s
Nov 23 12:00:47.886: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 18.046199293s
Nov 23 12:00:49.890: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Running", Reason="", readiness=true. Elapsed: 20.050391338s
Nov 23 12:00:51.927: INFO: Pod "pod-subpath-test-downwardapi-jx2j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.086740395s
STEP: Saw pod success
Nov 23 12:00:51.927: INFO: Pod "pod-subpath-test-downwardapi-jx2j" satisfied condition "Succeeded or Failed"
Nov 23 12:00:51.936: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-subpath-test-downwardapi-jx2j container test-container-subpath-downwardapi-jx2j: <nil>
STEP: delete the pod
Nov 23 12:00:52.092: INFO: Waiting for pod pod-subpath-test-downwardapi-jx2j to disappear
Nov 23 12:00:52.113: INFO: Pod pod-subpath-test-downwardapi-jx2j no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jx2j
Nov 23 12:00:52.113: INFO: Deleting pod "pod-subpath-test-downwardapi-jx2j" in namespace "subpath-780"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:52.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-780" for this suite.

• [SLOW TEST:22.472 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":117,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:52.145: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:00:54.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5253" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":2088,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:00:54.337: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-bmhm
STEP: Creating a pod to test atomic-volume-subpath
Nov 23 12:00:54.494: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bmhm" in namespace "subpath-8917" to be "Succeeded or Failed"
Nov 23 12:00:54.499: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.621419ms
Nov 23 12:00:56.503: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008971098s
Nov 23 12:00:58.506: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 4.012287607s
Nov 23 12:01:00.510: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 6.015427897s
Nov 23 12:01:02.513: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 8.019135656s
Nov 23 12:01:04.517: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 10.023171688s
Nov 23 12:01:06.522: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 12.027961862s
Nov 23 12:01:08.527: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 14.033037602s
Nov 23 12:01:10.531: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 16.036568364s
Nov 23 12:01:12.535: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 18.041312653s
Nov 23 12:01:14.541: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Running", Reason="", readiness=true. Elapsed: 20.046391106s
Nov 23 12:01:16.544: INFO: Pod "pod-subpath-test-projected-bmhm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049817735s
STEP: Saw pod success
Nov 23 12:01:16.544: INFO: Pod "pod-subpath-test-projected-bmhm" satisfied condition "Succeeded or Failed"
Nov 23 12:01:16.548: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-subpath-test-projected-bmhm container test-container-subpath-projected-bmhm: <nil>
STEP: delete the pod
Nov 23 12:01:16.574: INFO: Waiting for pod pod-subpath-test-projected-bmhm to disappear
Nov 23 12:01:16.578: INFO: Pod pod-subpath-test-projected-bmhm no longer exists
STEP: Deleting pod pod-subpath-test-projected-bmhm
Nov 23 12:01:16.578: INFO: Deleting pod "pod-subpath-test-projected-bmhm" in namespace "subpath-8917"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:01:16.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8917" for this suite.

• [SLOW TEST:22.255 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":119,"skipped":2091,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:01:16.592: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:01:16.745: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 23 12:01:21.749: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 23 12:01:21.749: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 23 12:01:23.757: INFO: Creating deployment "test-rollover-deployment"
Nov 23 12:01:23.783: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 23 12:01:25.795: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 23 12:01:25.805: INFO: Ensure that both replica sets have 1 created replica
Nov 23 12:01:25.812: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 23 12:01:25.827: INFO: Updating deployment test-rollover-deployment
Nov 23 12:01:25.827: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 23 12:01:27.839: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 23 12:01:27.846: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 23 12:01:27.853: INFO: all replica sets need to contain the pod-template-hash label
Nov 23 12:01:27.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729687, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 23 12:01:29.861: INFO: all replica sets need to contain the pod-template-hash label
Nov 23 12:01:29.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729687, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 23 12:01:31.864: INFO: all replica sets need to contain the pod-template-hash label
Nov 23 12:01:31.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729687, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 23 12:01:33.861: INFO: all replica sets need to contain the pod-template-hash label
Nov 23 12:01:33.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729687, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 23 12:01:35.863: INFO: all replica sets need to contain the pod-template-hash label
Nov 23 12:01:35.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729687, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741729683, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 23 12:01:37.866: INFO: 
Nov 23 12:01:37.866: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 23 12:01:37.878: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6076 /apis/apps/v1/namespaces/deployment-6076/deployments/test-rollover-deployment 7e48a6d7-3838-4a86-b4ba-9366fbc4f446 56392 2 2020-11-23 12:01:23 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-23 12:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 12:01:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003497da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-11-23 12:01:23 +0000 UTC,LastTransitionTime:2020-11-23 12:01:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-11-23 12:01:37 +0000 UTC,LastTransitionTime:2020-11-23 12:01:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 23 12:01:37.885: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-6076 /apis/apps/v1/namespaces/deployment-6076/replicasets/test-rollover-deployment-5797c7764 f45a226a-80f7-4bdf-b7bf-c614f41a9faa 56381 2 2020-11-23 12:01:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7e48a6d7-3838-4a86-b4ba-9366fbc4f446 0xc004dea430 0xc004dea431}] []  [{kube-controller-manager Update apps/v1 2020-11-23 12:01:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e48a6d7-3838-4a86-b4ba-9366fbc4f446\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004dea4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:01:37.885: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 23 12:01:37.886: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6076 /apis/apps/v1/namespaces/deployment-6076/replicasets/test-rollover-controller a6122a9b-1671-44ec-a990-fcb3f294613d 56391 2 2020-11-23 12:01:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7e48a6d7-3838-4a86-b4ba-9366fbc4f446 0xc004dea327 0xc004dea328}] []  [{e2e.test Update apps/v1 2020-11-23 12:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 12:01:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e48a6d7-3838-4a86-b4ba-9366fbc4f446\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004dea3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:01:37.886: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6076 /apis/apps/v1/namespaces/deployment-6076/replicasets/test-rollover-deployment-78bc8b888c 0e0a0db0-460c-43bf-8c76-0ea86cff0ab5 56337 2 2020-11-23 12:01:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7e48a6d7-3838-4a86-b4ba-9366fbc4f446 0xc004dea517 0xc004dea518}] []  [{kube-controller-manager Update apps/v1 2020-11-23 12:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e48a6d7-3838-4a86-b4ba-9366fbc4f446\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004dea5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:01:37.889: INFO: Pod "test-rollover-deployment-5797c7764-zn7l8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-zn7l8 test-rollover-deployment-5797c7764- deployment-6076 /api/v1/namespaces/deployment-6076/pods/test-rollover-deployment-5797c7764-zn7l8 d3bae016-ed7e-4f4b-ab5d-dc83db9e24c7 56352 0 2020-11-23 12:01:25 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:100.96.2.40/32 cni.projectcalico.org/podIPs:100.96.2.40/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 f45a226a-80f7-4bdf-b7bf-c614f41a9faa 0xc004deab70 0xc004deab71}] []  [{kube-controller-manager Update v1 2020-11-23 12:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f45a226a-80f7-4bdf-b7bf-c614f41a9faa\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4t2wp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4t2wp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4t2wp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:01:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:01:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.40,StartTime:2020-11-23 12:01:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:01:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://0f830919bedb2b54796b901a8a770d5c4efb67ae03189774d1c3a2e08eed0d7d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:01:37.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6076" for this suite.

• [SLOW TEST:21.304 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":120,"skipped":2112,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:01:37.897: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-d05a889a-9cd9-49eb-9a83-770507a2f564
STEP: Creating secret with name secret-projected-all-test-volume-376c833f-d466-4c41-9da6-0381aeab82c1
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 23 12:01:38.058: INFO: Waiting up to 5m0s for pod "projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9" in namespace "projected-5887" to be "Succeeded or Failed"
Nov 23 12:01:38.068: INFO: Pod "projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.121628ms
Nov 23 12:01:40.073: INFO: Pod "projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014597937s
STEP: Saw pod success
Nov 23 12:01:40.073: INFO: Pod "projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9" satisfied condition "Succeeded or Failed"
Nov 23 12:01:40.075: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 23 12:01:40.099: INFO: Waiting for pod projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9 to disappear
Nov 23 12:01:40.104: INFO: Pod projected-volume-0f3570ca-b310-4582-88f4-7da1f40e80a9 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:01:40.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5887" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":121,"skipped":2121,"failed":0}

------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:01:40.123: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Nov 23 12:01:40.275: INFO: created test-podtemplate-1
Nov 23 12:01:40.280: INFO: created test-podtemplate-2
Nov 23 12:01:40.284: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 23 12:01:40.294: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 23 12:01:40.317: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:01:40.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-868" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":122,"skipped":2121,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:01:40.331: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-51a09d57-970a-4080-840e-a3aac9e0271b
STEP: Creating a pod to test consume secrets
Nov 23 12:01:40.485: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948" in namespace "projected-8440" to be "Succeeded or Failed"
Nov 23 12:01:40.487: INFO: Pod "pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948": Phase="Pending", Reason="", readiness=false. Elapsed: 2.579896ms
Nov 23 12:01:42.493: INFO: Pod "pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008235368s
STEP: Saw pod success
Nov 23 12:01:42.493: INFO: Pod "pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948" satisfied condition "Succeeded or Failed"
Nov 23 12:01:42.496: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948 container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:01:42.524: INFO: Waiting for pod pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948 to disappear
Nov 23 12:01:42.528: INFO: Pod pod-projected-secrets-45316062-6625-458e-a53c-f6c301024948 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:01:42.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8440" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":2134,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:01:42.540: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 23 12:01:42.705: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 23 12:02:42.761: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:02:42.764: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 23 12:02:44.953: INFO: found a healthy node: dvi-conformance-1606121581-vsp1-group1-0
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:02:57.106: INFO: pods created so far: [1 1 1]
Nov 23 12:02:57.106: INFO: length of pods created so far: 3
Nov 23 12:03:45.119: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:03:52.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4515" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:03:52.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7285" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:129.724 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":124,"skipped":2137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:03:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 23 12:03:52.448: INFO: Waiting up to 5m0s for pod "pod-8232e6d6-171f-4818-8e31-7012fdb0e526" in namespace "emptydir-5228" to be "Succeeded or Failed"
Nov 23 12:03:52.453: INFO: Pod "pod-8232e6d6-171f-4818-8e31-7012fdb0e526": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171512ms
Nov 23 12:03:54.458: INFO: Pod "pod-8232e6d6-171f-4818-8e31-7012fdb0e526": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009663099s
STEP: Saw pod success
Nov 23 12:03:54.458: INFO: Pod "pod-8232e6d6-171f-4818-8e31-7012fdb0e526" satisfied condition "Succeeded or Failed"
Nov 23 12:03:54.462: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-8232e6d6-171f-4818-8e31-7012fdb0e526 container test-container: <nil>
STEP: delete the pod
Nov 23 12:03:54.507: INFO: Waiting for pod pod-8232e6d6-171f-4818-8e31-7012fdb0e526 to disappear
Nov 23 12:03:54.511: INFO: Pod pod-8232e6d6-171f-4818-8e31-7012fdb0e526 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:03:54.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5228" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2160,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:03:54.521: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 23 12:03:54.684: INFO: Waiting up to 5m0s for pod "pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3" in namespace "emptydir-7358" to be "Succeeded or Failed"
Nov 23 12:03:54.694: INFO: Pod "pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.653339ms
Nov 23 12:03:56.700: INFO: Pod "pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015592928s
STEP: Saw pod success
Nov 23 12:03:56.700: INFO: Pod "pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3" satisfied condition "Succeeded or Failed"
Nov 23 12:03:56.706: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3 container test-container: <nil>
STEP: delete the pod
Nov 23 12:03:56.734: INFO: Waiting for pod pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3 to disappear
Nov 23 12:03:56.737: INFO: Pod pod-41854ab5-7ab1-43ad-bbc8-f13a141aafa3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:03:56.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7358" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2179,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:03:56.761: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-72111a5b-0571-4716-89f4-b699866fb9fb in namespace container-probe-8097
Nov 23 12:03:58.979: INFO: Started pod busybox-72111a5b-0571-4716-89f4-b699866fb9fb in namespace container-probe-8097
STEP: checking the pod's current state and verifying that restartCount is present
Nov 23 12:03:58.982: INFO: Initial restart count of pod busybox-72111a5b-0571-4716-89f4-b699866fb9fb is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:07:59.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8097" for this suite.

• [SLOW TEST:242.753 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:07:59.515: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:08:01.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7401" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":128,"skipped":2223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:08:01.779: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-467, will wait for the garbage collector to delete the pods
Nov 23 12:08:04.028: INFO: Deleting Job.batch foo took: 17.073416ms
Nov 23 12:08:04.628: INFO: Terminating Job.batch foo pods took: 600.211327ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:08:37.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-467" for this suite.

• [SLOW TEST:35.671 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":129,"skipped":2251,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:08:37.451: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 23 12:08:37.600: INFO: Waiting up to 5m0s for pod "pod-84468a6b-4c99-42de-8e91-617a766f2cc4" in namespace "emptydir-143" to be "Succeeded or Failed"
Nov 23 12:08:37.603: INFO: Pod "pod-84468a6b-4c99-42de-8e91-617a766f2cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.102388ms
Nov 23 12:08:39.609: INFO: Pod "pod-84468a6b-4c99-42de-8e91-617a766f2cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008870321s
STEP: Saw pod success
Nov 23 12:08:39.609: INFO: Pod "pod-84468a6b-4c99-42de-8e91-617a766f2cc4" satisfied condition "Succeeded or Failed"
Nov 23 12:08:39.611: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-84468a6b-4c99-42de-8e91-617a766f2cc4 container test-container: <nil>
STEP: delete the pod
Nov 23 12:08:39.636: INFO: Waiting for pod pod-84468a6b-4c99-42de-8e91-617a766f2cc4 to disappear
Nov 23 12:08:39.638: INFO: Pod pod-84468a6b-4c99-42de-8e91-617a766f2cc4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:08:39.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-143" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":130,"skipped":2255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:08:39.647: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4534
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 23 12:08:39.789: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 23 12:08:39.831: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:08:41.836: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:08:43.836: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:08:45.893: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:08:47.835: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:08:49.838: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:08:51.836: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:08:53.836: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 23 12:08:53.844: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 23 12:08:55.867: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.56:8080/dial?request=hostname&protocol=http&host=100.96.2.55&port=8080&tries=1'] Namespace:pod-network-test-4534 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:08:55.867: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:08:55.971: INFO: Waiting for responses: map[]
Nov 23 12:08:55.979: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.56:8080/dial?request=hostname&protocol=http&host=100.96.1.75&port=8080&tries=1'] Namespace:pod-network-test-4534 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:08:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:08:56.099: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:08:56.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4534" for this suite.

• [SLOW TEST:16.465 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:08:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:08:56.308: INFO: Create a RollingUpdate DaemonSet
Nov 23 12:08:56.316: INFO: Check that daemon pods launch on every node of the cluster
Nov 23 12:08:56.323: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:08:56.330: INFO: Number of nodes with available pods: 0
Nov 23 12:08:56.330: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:08:57.337: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:08:57.348: INFO: Number of nodes with available pods: 0
Nov 23 12:08:57.348: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:08:58.335: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:08:58.339: INFO: Number of nodes with available pods: 2
Nov 23 12:08:58.340: INFO: Number of running nodes: 2, number of available pods: 2
Nov 23 12:08:58.340: INFO: Update the DaemonSet to trigger a rollout
Nov 23 12:08:58.348: INFO: Updating DaemonSet daemon-set
Nov 23 12:09:11.373: INFO: Roll back the DaemonSet before rollout is complete
Nov 23 12:09:11.387: INFO: Updating DaemonSet daemon-set
Nov 23 12:09:11.387: INFO: Make sure DaemonSet rollback is complete
Nov 23 12:09:11.391: INFO: Wrong image for pod: daemon-set-kthhz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 23 12:09:11.391: INFO: Pod daemon-set-kthhz is not available
Nov 23 12:09:11.398: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:09:12.402: INFO: Wrong image for pod: daemon-set-kthhz. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Nov 23 12:09:12.402: INFO: Pod daemon-set-kthhz is not available
Nov 23 12:09:12.407: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:09:13.403: INFO: Pod daemon-set-x65w9 is not available
Nov 23 12:09:13.410: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6858, will wait for the garbage collector to delete the pods
Nov 23 12:09:13.479: INFO: Deleting DaemonSet.extensions daemon-set took: 9.333677ms
Nov 23 12:09:14.081: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.850339ms
Nov 23 12:10:30.885: INFO: Number of nodes with available pods: 0
Nov 23 12:10:30.885: INFO: Number of running nodes: 0, number of available pods: 0
Nov 23 12:10:30.888: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6858/daemonsets","resourceVersion":"58503"},"items":null}

Nov 23 12:10:30.890: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6858/pods","resourceVersion":"58503"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:10:30.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6858" for this suite.

• [SLOW TEST:94.796 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":132,"skipped":2327,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:10:30.910: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 23 12:10:35.126: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:35.131: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:37.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:37.135: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:39.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:39.135: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:41.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:41.135: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:43.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:43.135: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:45.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:45.134: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:47.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:47.136: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 23 12:10:49.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 23 12:10:49.139: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:10:49.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2577" for this suite.

• [SLOW TEST:18.241 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2340,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:10:49.152: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:10:49.966: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:10:52.997: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:10:53.000: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:10:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9332" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.499 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":134,"skipped":2360,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:10:54.651: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9966
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:10:54.864: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 23 12:11:01.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 create -f -'
Nov 23 12:11:02.138: INFO: stderr: ""
Nov 23 12:11:02.139: INFO: stdout: "e2e-test-crd-publish-openapi-6406-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 23 12:11:02.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 delete e2e-test-crd-publish-openapi-6406-crds test-foo'
Nov 23 12:11:02.273: INFO: stderr: ""
Nov 23 12:11:02.273: INFO: stdout: "e2e-test-crd-publish-openapi-6406-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 23 12:11:02.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 apply -f -'
Nov 23 12:11:02.565: INFO: stderr: ""
Nov 23 12:11:02.565: INFO: stdout: "e2e-test-crd-publish-openapi-6406-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 23 12:11:02.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 delete e2e-test-crd-publish-openapi-6406-crds test-foo'
Nov 23 12:11:02.689: INFO: stderr: ""
Nov 23 12:11:02.689: INFO: stdout: "e2e-test-crd-publish-openapi-6406-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 23 12:11:02.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 create -f -'
Nov 23 12:11:03.049: INFO: rc: 1
Nov 23 12:11:03.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 apply -f -'
Nov 23 12:11:03.325: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 23 12:11:03.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 create -f -'
Nov 23 12:11:03.608: INFO: rc: 1
Nov 23 12:11:03.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-9966 apply -f -'
Nov 23 12:11:03.847: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 23 12:11:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-6406-crds'
Nov 23 12:11:04.101: INFO: stderr: ""
Nov 23 12:11:04.101: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6406-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 23 12:11:04.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-6406-crds.metadata'
Nov 23 12:11:04.448: INFO: stderr: ""
Nov 23 12:11:04.448: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6406-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 23 12:11:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-6406-crds.spec'
Nov 23 12:11:04.718: INFO: stderr: ""
Nov 23 12:11:04.718: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6406-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 23 12:11:04.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-6406-crds.spec.bars'
Nov 23 12:11:04.994: INFO: stderr: ""
Nov 23 12:11:04.994: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6406-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 23 12:11:04.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-6406-crds.spec.bars2'
Nov 23 12:11:05.298: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:09.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9966" for this suite.

• [SLOW TEST:15.191 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":135,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:09.843: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:11:10.013: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a2fa78fe-b8cf-4600-b44d-0c35657e74dc" in namespace "security-context-test-1000" to be "Succeeded or Failed"
Nov 23 12:11:10.024: INFO: Pod "busybox-user-65534-a2fa78fe-b8cf-4600-b44d-0c35657e74dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.135883ms
Nov 23 12:11:12.028: INFO: Pod "busybox-user-65534-a2fa78fe-b8cf-4600-b44d-0c35657e74dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015164793s
Nov 23 12:11:12.028: INFO: Pod "busybox-user-65534-a2fa78fe-b8cf-4600-b44d-0c35657e74dc" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:12.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1000" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2390,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:12.038: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:11:12.205: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e6cb7ec9-9c70-467d-88dc-3eb2370350a5" in namespace "security-context-test-9946" to be "Succeeded or Failed"
Nov 23 12:11:12.210: INFO: Pod "busybox-privileged-false-e6cb7ec9-9c70-467d-88dc-3eb2370350a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.839005ms
Nov 23 12:11:14.215: INFO: Pod "busybox-privileged-false-e6cb7ec9-9c70-467d-88dc-3eb2370350a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010236222s
Nov 23 12:11:16.219: INFO: Pod "busybox-privileged-false-e6cb7ec9-9c70-467d-88dc-3eb2370350a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014418912s
Nov 23 12:11:16.219: INFO: Pod "busybox-privileged-false-e6cb7ec9-9c70-467d-88dc-3eb2370350a5" satisfied condition "Succeeded or Failed"
Nov 23 12:11:16.228: INFO: Got logs for pod "busybox-privileged-false-e6cb7ec9-9c70-467d-88dc-3eb2370350a5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:16.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9946" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":137,"skipped":2394,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:16.239: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6037
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5178
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:29.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3834" for this suite.
STEP: Destroying namespace "nsdeletetest-6037" for this suite.
Nov 23 12:11:29.701: INFO: Namespace nsdeletetest-6037 was already deleted
STEP: Destroying namespace "nsdeletetest-5178" for this suite.

• [SLOW TEST:13.469 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":138,"skipped":2409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:29.709: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:11:29.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f" in namespace "downward-api-5479" to be "Succeeded or Failed"
Nov 23 12:11:29.873: INFO: Pod "downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.878839ms
Nov 23 12:11:31.876: INFO: Pod "downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00887781s
STEP: Saw pod success
Nov 23 12:11:31.876: INFO: Pod "downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f" satisfied condition "Succeeded or Failed"
Nov 23 12:11:31.878: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f container client-container: <nil>
STEP: delete the pod
Nov 23 12:11:31.902: INFO: Waiting for pod downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f to disappear
Nov 23 12:11:31.905: INFO: Pod downwardapi-volume-7f9db3db-e2f9-49d2-9adb-e6b3f1492e9f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:31.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5479" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:31.920: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 23 12:11:32.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6359'
Nov 23 12:11:32.201: INFO: stderr: ""
Nov 23 12:11:32.201: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 23 12:11:37.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pod e2e-test-httpd-pod --namespace=kubectl-6359 -o json'
Nov 23 12:11:37.355: INFO: stderr: ""
Nov 23 12:11:37.355: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.2.65/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.96.2.65/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-11-23T12:11:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-23T12:11:32Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-23T12:11:32Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"100.96.2.65\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-23T12:11:33Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6359\",\n        \"resourceVersion\": \"58983\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6359/pods/e2e-test-httpd-pod\",\n        \"uid\": \"96d8c4ca-0a8d-4475-9535-55ec02816ed6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qghmd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"dvi-conformance-1606121581-vsp1-group1-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qghmd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qghmd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:11:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:11:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:11:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:11:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ef0d4c4838afd01a7fe6e691e66bb3649d9aaa899693ff0b38165a56d39f5535\",\n                \"image\": \"docker.io/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-11-23T12:11:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.8.54\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.2.65\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.2.65\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-23T12:11:32Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 23 12:11:37.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 replace -f - --namespace=kubectl-6359'
Nov 23 12:11:37.728: INFO: stderr: ""
Nov 23 12:11:37.728: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Nov 23 12:11:37.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete pods e2e-test-httpd-pod --namespace=kubectl-6359'
Nov 23 12:11:39.592: INFO: stderr: ""
Nov 23 12:11:39.592: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:39.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6359" for this suite.

• [SLOW TEST:7.684 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":140,"skipped":2482,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:39.604: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a79b7bc2-a58a-4b27-aacc-76f9cfedc7d8
STEP: Creating a pod to test consume configMaps
Nov 23 12:11:39.784: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3" in namespace "projected-505" to be "Succeeded or Failed"
Nov 23 12:11:39.792: INFO: Pod "pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.829747ms
Nov 23 12:11:41.799: INFO: Pod "pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014466349s
STEP: Saw pod success
Nov 23 12:11:41.799: INFO: Pod "pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3" satisfied condition "Succeeded or Failed"
Nov 23 12:11:41.803: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:11:41.822: INFO: Waiting for pod pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3 to disappear
Nov 23 12:11:41.830: INFO: Pod pod-projected-configmaps-633692df-455c-4c54-8559-774ea0206ce3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:11:41.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-505" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2482,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:11:41.841: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8669
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-8669
Nov 23 12:11:42.021: INFO: Found 0 stateful pods, waiting for 1
Nov 23 12:11:52.028: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 23 12:11:52.062: INFO: Deleting all statefulset in ns statefulset-8669
Nov 23 12:11:52.068: INFO: Scaling statefulset ss to 0
Nov 23 12:12:12.109: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 12:12:12.112: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:12.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8669" for this suite.

• [SLOW TEST:30.300 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":142,"skipped":2493,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Nov 23 12:12:12.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 cluster-info'
Nov 23 12:12:12.417: INFO: stderr: ""
Nov 23 12:12:12.417: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mmetrics-server\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:12.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6257" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":143,"skipped":2495,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:12:12.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27" in namespace "projected-2748" to be "Succeeded or Failed"
Nov 23 12:12:12.601: INFO: Pod "downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.129952ms
Nov 23 12:12:14.615: INFO: Pod "downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024451195s
STEP: Saw pod success
Nov 23 12:12:14.615: INFO: Pod "downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27" satisfied condition "Succeeded or Failed"
Nov 23 12:12:14.629: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27 container client-container: <nil>
STEP: delete the pod
Nov 23 12:12:14.682: INFO: Waiting for pod downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27 to disappear
Nov 23 12:12:14.692: INFO: Pod downwardapi-volume-b96d3283-7c4b-4d9c-a161-ebeae0495b27 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:14.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2748" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 23 12:12:14.905: INFO: Waiting up to 5m0s for pod "pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb" in namespace "emptydir-6784" to be "Succeeded or Failed"
Nov 23 12:12:14.913: INFO: Pod "pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.537806ms
Nov 23 12:12:16.918: INFO: Pod "pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012608849s
STEP: Saw pod success
Nov 23 12:12:16.918: INFO: Pod "pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb" satisfied condition "Succeeded or Failed"
Nov 23 12:12:16.922: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb container test-container: <nil>
STEP: delete the pod
Nov 23 12:12:16.948: INFO: Waiting for pod pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb to disappear
Nov 23 12:12:16.953: INFO: Pod pod-0f739e8d-2ee2-488f-b8cc-749d96285fcb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6784" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2526,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:16.969: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b
Nov 23 12:12:17.136: INFO: Pod name my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b: Found 0 pods out of 1
Nov 23 12:12:22.139: INFO: Pod name my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b: Found 1 pods out of 1
Nov 23 12:12:22.139: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b" are running
Nov 23 12:12:22.143: INFO: Pod "my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b-l57gx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:12:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:12:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:12:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-11-23 12:12:17 +0000 UTC Reason: Message:}])
Nov 23 12:12:22.143: INFO: Trying to dial the pod
Nov 23 12:12:27.154: INFO: Controller my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b: Got expected result from replica 1 [my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b-l57gx]: "my-hostname-basic-ae3b42ab-4c07-4431-95c6-cddc0c674e0b-l57gx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:27.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5994" for this suite.

• [SLOW TEST:10.194 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":146,"skipped":2539,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:27.163: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b8425879-d17a-4b5b-a403-9487f0139405
STEP: Creating a pod to test consume secrets
Nov 23 12:12:27.330: INFO: Waiting up to 5m0s for pod "pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76" in namespace "secrets-5258" to be "Succeeded or Failed"
Nov 23 12:12:27.333: INFO: Pod "pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157669ms
Nov 23 12:12:29.339: INFO: Pod "pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008745968s
STEP: Saw pod success
Nov 23 12:12:29.339: INFO: Pod "pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76" satisfied condition "Succeeded or Failed"
Nov 23 12:12:29.342: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76 container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:12:29.380: INFO: Waiting for pod pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76 to disappear
Nov 23 12:12:29.387: INFO: Pod pod-secrets-293f85e7-051a-4136-b1be-7b147d36cb76 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:29.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5258" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2539,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:29.401: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:12:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6334" for this suite.

• [SLOW TEST:17.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":148,"skipped":2541,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:12:46.613: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5880.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5880.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 12:12:48.794: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.797: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.799: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.801: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.809: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.812: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.814: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.816: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:48.822: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:12:53.827: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.833: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.837: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.841: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.851: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.856: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.860: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.863: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:53.871: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:12:58.828: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.831: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.833: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.836: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.845: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.848: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.851: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.853: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:12:58.859: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:13:03.827: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.832: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.838: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.845: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.861: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.866: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.876: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.881: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:03.890: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:13:08.827: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.830: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.833: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.837: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.845: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.848: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.851: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.854: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:08.860: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:13:13.826: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.831: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.835: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.839: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.847: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.850: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.853: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.856: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:13.861: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:13:18.826: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.829: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.839: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.844: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.852: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.854: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.857: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.859: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local from pod dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9: the server could not find the requested resource (get pods dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9)
Nov 23 12:13:18.866: INFO: Lookups using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5880.svc.cluster.local jessie_udp@dns-test-service-2.dns-5880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5880.svc.cluster.local]

Nov 23 12:13:23.866: INFO: DNS probes using dns-5880/dns-test-daea8aff-0ee7-48a2-9dd0-e2fdd0bd1ef9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:23.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5880" for this suite.

• [SLOW TEST:37.368 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":149,"skipped":2550,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:23.981: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5542
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:13:24.133: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 23 12:13:28.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-5542 create -f -'
Nov 23 12:13:29.010: INFO: stderr: ""
Nov 23 12:13:29.010: INFO: stdout: "e2e-test-crd-publish-openapi-2551-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 23 12:13:29.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-5542 delete e2e-test-crd-publish-openapi-2551-crds test-cr'
Nov 23 12:13:29.135: INFO: stderr: ""
Nov 23 12:13:29.135: INFO: stdout: "e2e-test-crd-publish-openapi-2551-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 23 12:13:29.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-5542 apply -f -'
Nov 23 12:13:29.462: INFO: stderr: ""
Nov 23 12:13:29.462: INFO: stdout: "e2e-test-crd-publish-openapi-2551-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 23 12:13:29.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-5542 delete e2e-test-crd-publish-openapi-2551-crds test-cr'
Nov 23 12:13:29.628: INFO: stderr: ""
Nov 23 12:13:29.628: INFO: stdout: "e2e-test-crd-publish-openapi-2551-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 23 12:13:29.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-2551-crds'
Nov 23 12:13:29.870: INFO: stderr: ""
Nov 23 12:13:29.870: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2551-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:34.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5542" for this suite.

• [SLOW TEST:10.474 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":150,"skipped":2570,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:34.456: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Nov 23 12:13:36.634: INFO: Pod pod-hostip-aa07bbf5-37c8-4829-941f-a3a75680062d has hostIP: 192.168.8.54
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:36.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5245" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2574,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:36.644: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:36.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6831" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":152,"skipped":2577,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:36.805: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 23 12:13:42.996: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 23 12:13:43.004: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 23 12:13:45.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 23 12:13:45.009: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 23 12:13:47.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 23 12:13:47.009: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 23 12:13:49.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 23 12:13:49.008: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:49.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5310" for this suite.

• [SLOW TEST:12.232 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2586,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:49.037: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:49.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-466" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":154,"skipped":2604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:49.185: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:13:49.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2842" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":155,"skipped":2640,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:13:49.360: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:13:49.991: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 23 12:13:52.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741730429, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741730429, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741730430, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741730429, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:13:55.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4599" for this suite.
STEP: Destroying namespace "webhook-4599-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":156,"skipped":2640,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:05.207: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:14:05.370: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9" in namespace "security-context-test-7333" to be "Succeeded or Failed"
Nov 23 12:14:05.375: INFO: Pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72337ms
Nov 23 12:14:07.378: INFO: Pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00791722s
Nov 23 12:14:09.389: INFO: Pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019236845s
Nov 23 12:14:11.393: INFO: Pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022848916s
Nov 23 12:14:13.397: INFO: Pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027287712s
Nov 23 12:14:13.397: INFO: Pod "alpine-nnp-false-fc1dd5b8-bfff-4758-a66b-98888b20cce9" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:13.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7333" for this suite.

• [SLOW TEST:8.203 seconds]
[k8s.io] Security Context
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2660,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:13.411: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 23 12:14:16.086: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6005cb6c-0d40-4068-af2c-10a3958c0d18"
Nov 23 12:14:16.086: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6005cb6c-0d40-4068-af2c-10a3958c0d18" in namespace "pods-7504" to be "terminated due to deadline exceeded"
Nov 23 12:14:16.090: INFO: Pod "pod-update-activedeadlineseconds-6005cb6c-0d40-4068-af2c-10a3958c0d18": Phase="Running", Reason="", readiness=true. Elapsed: 3.247762ms
Nov 23 12:14:18.097: INFO: Pod "pod-update-activedeadlineseconds-6005cb6c-0d40-4068-af2c-10a3958c0d18": Phase="Running", Reason="", readiness=true. Elapsed: 2.010696846s
Nov 23 12:14:20.102: INFO: Pod "pod-update-activedeadlineseconds-6005cb6c-0d40-4068-af2c-10a3958c0d18": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01596327s
Nov 23 12:14:20.102: INFO: Pod "pod-update-activedeadlineseconds-6005cb6c-0d40-4068-af2c-10a3958c0d18" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:20.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7504" for this suite.

• [SLOW TEST:6.702 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2668,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 23 12:14:20.266: INFO: Waiting up to 5m0s for pod "downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb" in namespace "downward-api-496" to be "Succeeded or Failed"
Nov 23 12:14:20.272: INFO: Pod "downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.804571ms
Nov 23 12:14:22.276: INFO: Pod "downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010450337s
STEP: Saw pod success
Nov 23 12:14:22.276: INFO: Pod "downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb" satisfied condition "Succeeded or Failed"
Nov 23 12:14:22.279: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb container dapi-container: <nil>
STEP: delete the pod
Nov 23 12:14:22.323: INFO: Waiting for pod downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb to disappear
Nov 23 12:14:22.331: INFO: Pod downward-api-c19d8827-59aa-464e-aa71-84c2ebe307cb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:22.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-496" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2677,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-cbf10649-e2a2-4a37-aa38-9bb05eb0fb17
STEP: Creating a pod to test consume configMaps
Nov 23 12:14:22.540: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5" in namespace "projected-317" to be "Succeeded or Failed"
Nov 23 12:14:22.543: INFO: Pod "pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78752ms
Nov 23 12:14:24.549: INFO: Pod "pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008703486s
Nov 23 12:14:26.554: INFO: Pod "pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014507649s
STEP: Saw pod success
Nov 23 12:14:26.554: INFO: Pod "pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5" satisfied condition "Succeeded or Failed"
Nov 23 12:14:26.558: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:14:26.584: INFO: Waiting for pod pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5 to disappear
Nov 23 12:14:26.589: INFO: Pod pod-projected-configmaps-0ee85bb5-9184-4e1f-a67d-f9b717bb71b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:26.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-317" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2677,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:26.599: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-1886
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1886
STEP: Deleting pre-stop pod
Nov 23 12:14:35.788: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:35.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1886" for this suite.

• [SLOW TEST:9.238 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":161,"skipped":2688,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:35.838: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 23 12:14:36.030: INFO: Waiting up to 5m0s for pod "pod-aa0be0cc-2723-40e2-b763-1607189b5560" in namespace "emptydir-6073" to be "Succeeded or Failed"
Nov 23 12:14:36.045: INFO: Pod "pod-aa0be0cc-2723-40e2-b763-1607189b5560": Phase="Pending", Reason="", readiness=false. Elapsed: 14.81657ms
Nov 23 12:14:38.048: INFO: Pod "pod-aa0be0cc-2723-40e2-b763-1607189b5560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018121246s
STEP: Saw pod success
Nov 23 12:14:38.048: INFO: Pod "pod-aa0be0cc-2723-40e2-b763-1607189b5560" satisfied condition "Succeeded or Failed"
Nov 23 12:14:38.051: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-aa0be0cc-2723-40e2-b763-1607189b5560 container test-container: <nil>
STEP: delete the pod
Nov 23 12:14:38.077: INFO: Waiting for pod pod-aa0be0cc-2723-40e2-b763-1607189b5560 to disappear
Nov 23 12:14:38.083: INFO: Pod pod-aa0be0cc-2723-40e2-b763-1607189b5560 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:38.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6073" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2692,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:38.097: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:14:38.254: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:40.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7562" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2694,"failed":0}
S
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:40.397: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9109
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9109" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":164,"skipped":2695,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:40.605: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:14:40.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455" in namespace "projected-7579" to be "Succeeded or Failed"
Nov 23 12:14:40.762: INFO: Pod "downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.731551ms
Nov 23 12:14:42.766: INFO: Pod "downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00859907s
STEP: Saw pod success
Nov 23 12:14:42.766: INFO: Pod "downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455" satisfied condition "Succeeded or Failed"
Nov 23 12:14:42.768: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455 container client-container: <nil>
STEP: delete the pod
Nov 23 12:14:42.800: INFO: Waiting for pod downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455 to disappear
Nov 23 12:14:42.805: INFO: Pod downwardapi-volume-6cc52598-9048-477d-8d13-eeb13fd60455 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:14:42.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7579" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":165,"skipped":2697,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:14:42.818: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3013
STEP: creating service affinity-nodeport in namespace services-3013
STEP: creating replication controller affinity-nodeport in namespace services-3013
I1123 12:14:43.001434      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-3013, replica count: 3
I1123 12:14:46.052307      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 12:14:46.063: INFO: Creating new exec pod
Nov 23 12:14:49.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov 23 12:14:51.298: INFO: rc: 1
Nov 23 12:14:51.298: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport 80
nc: connect to affinity-nodeport port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:14:52.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov 23 12:14:54.530: INFO: rc: 1
Nov 23 12:14:54.530: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-nodeport 80
nc: connect to affinity-nodeport port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:14:55.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Nov 23 12:14:55.546: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 23 12:14:55.546: INFO: stdout: ""
Nov 23 12:14:55.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 100.68.124.115 80'
Nov 23 12:14:55.789: INFO: stderr: "+ nc -zv -t -w 2 100.68.124.115 80\nConnection to 100.68.124.115 80 port [tcp/http] succeeded!\n"
Nov 23 12:14:55.789: INFO: stdout: ""
Nov 23 12:14:55.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 32606'
Nov 23 12:14:56.011: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 32606\nConnection to 192.168.8.54 32606 port [tcp/32606] succeeded!\n"
Nov 23 12:14:56.011: INFO: stdout: ""
Nov 23 12:14:56.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 32606'
Nov 23 12:14:56.218: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 32606\nConnection to 192.168.8.34 32606 port [tcp/32606] succeeded!\n"
Nov 23 12:14:56.218: INFO: stdout: ""
Nov 23 12:14:56.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 32606'
Nov 23 12:14:56.463: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 32606\nConnection to 192.168.8.54 32606 port [tcp/32606] succeeded!\n"
Nov 23 12:14:56.463: INFO: stdout: ""
Nov 23 12:14:56.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 32606'
Nov 23 12:14:56.727: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 32606\nConnection to 192.168.8.34 32606 port [tcp/32606] succeeded!\n"
Nov 23 12:14:56.727: INFO: stdout: ""
Nov 23 12:14:56.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-3013 execpod-affinityvch6v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.54:32606/ ; done'
Nov 23 12:14:57.071: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.54:32606/\n"
Nov 23 12:14:57.071: INFO: stdout: "\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg\naffinity-nodeport-2kjjg"
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Received response from host: affinity-nodeport-2kjjg
Nov 23 12:14:57.071: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3013, will wait for the garbage collector to delete the pods
Nov 23 12:14:57.147: INFO: Deleting ReplicationController affinity-nodeport took: 5.423165ms
Nov 23 12:14:57.747: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.249821ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:15:11.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3013" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:28.202 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":166,"skipped":2704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:15:11.022: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:15:13.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1764" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":167,"skipped":2742,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:15:13.284: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7075
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5d6af87c-1454-42aa-bb3f-309239e848c0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-5d6af87c-1454-42aa-bb3f-309239e848c0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:15:17.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7075" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":168,"skipped":2745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:15:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:15:18.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:15:21.241: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:15:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4909" for this suite.
STEP: Destroying namespace "webhook-4909-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":169,"skipped":2770,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:15:21.343: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 23 12:15:21.564: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 23 12:15:21.576: INFO: Waiting for terminating namespaces to be deleted...
Nov 23 12:15:21.592: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-0 before test
Nov 23 12:15:21.617: INFO: canal-92zmd from kube-system started at 2020-11-23 09:08:12 +0000 UTC (3 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 12:15:21.617: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 12:15:21.617: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 12:15:21.617: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 12:15:21.617: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 12:15:21.617: INFO: kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container main ready: true, restart count 0
Nov 23 12:15:21.617: INFO: node-local-dns-vnlcb from kube-system started at 2020-11-23 09:08:12 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 12:15:21.617: INFO: busybox-readonly-fs26427f30-7e5b-4cb5-8668-a6d0fd950b73 from kubelet-test-1764 started at 2020-11-23 12:15:11 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container busybox-readonly-fs26427f30-7e5b-4cb5-8668-a6d0fd950b73 ready: true, restart count 0
Nov 23 12:15:21.617: INFO: pod-projected-configmaps-e5dcdfc3-36aa-45f0-8e37-b19bc18f972c from projected-7075 started at 2020-11-23 12:15:13 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container projected-configmap-volume-test ready: true, restart count 0
Nov 23 12:15:21.617: INFO: sonobuoy from sonobuoy started at 2020-11-23 11:25:21 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 23 12:15:21.617: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 12:15:21.617: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 12:15:21.617: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 23 12:15:21.617: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-1 before test
Nov 23 12:15:21.628: INFO: canal-pgdpd from kube-system started at 2020-11-23 09:08:13 +0000 UTC (3 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 12:15:21.628: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 12:15:21.628: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 12:15:21.628: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 12:15:21.628: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 12:15:21.628: INFO: kublr-node-name-reporter-c33b8d393ebe42a50f83769b2f12339f5d1f5d10dd75810de5733c313118246a-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container main ready: true, restart count 0
Nov 23 12:15:21.628: INFO: kublr-system-shell-564c49bf57-ksqmb from kube-system started at 2020-11-23 09:09:38 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container shell ready: true, restart count 0
Nov 23 12:15:21.628: INFO: metrics-server-v0.3.6-7d5754b8df-2hd8f from kube-system started at 2020-11-23 11:42:06 +0000 UTC (2 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container metrics-server ready: true, restart count 0
Nov 23 12:15:21.628: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 23 12:15:21.628: INFO: node-local-dns-74tqs from kube-system started at 2020-11-23 09:08:13 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 12:15:21.628: INFO: kublr-operator-69c9677745-zk5lz from kublr started at 2020-11-23 11:19:03 +0000 UTC (1 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container kublr-operator ready: true, restart count 0
Nov 23 12:15:21.628: INFO: sonobuoy-e2e-job-7d6aa2c1eacf4413 from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container e2e ready: true, restart count 0
Nov 23 12:15:21.628: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 12:15:21.628: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-sj5vn from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 12:15:21.628: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 12:15:21.628: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1ec0fdbe-b53b-433f-bc19-332ca98422c6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-1ec0fdbe-b53b-433f-bc19-332ca98422c6 off the node dvi-conformance-1606121581-vsp1-group1-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1ec0fdbe-b53b-433f-bc19-332ca98422c6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:15:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-818" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":170,"skipped":2772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:15:25.821: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:15:27.026: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:15:30.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:15:30.061: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Nov 23 12:15:40.593: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:15:41.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8498" for this suite.
STEP: Destroying namespace "webhook-8498-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":171,"skipped":2813,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:15:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 in namespace container-probe-9381
Nov 23 12:15:43.733: INFO: Started pod liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 in namespace container-probe-9381
STEP: checking the pod's current state and verifying that restartCount is present
Nov 23 12:15:43.735: INFO: Initial restart count of pod liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 is 0
Nov 23 12:15:55.760: INFO: Restart count of pod container-probe-9381/liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 is now 1 (12.025188319s elapsed)
Nov 23 12:16:15.802: INFO: Restart count of pod container-probe-9381/liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 is now 2 (32.067019518s elapsed)
Nov 23 12:16:35.855: INFO: Restart count of pod container-probe-9381/liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 is now 3 (52.119985072s elapsed)
Nov 23 12:16:55.896: INFO: Restart count of pod container-probe-9381/liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 is now 4 (1m12.161203275s elapsed)
Nov 23 12:18:06.045: INFO: Restart count of pod container-probe-9381/liveness-fdea99f0-2b14-4ca5-a335-592b8660aa65 is now 5 (2m22.310334212s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:18:06.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9381" for this suite.

• [SLOW TEST:144.695 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:18:06.066: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:18:06.992: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:18:10.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:18:10.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4610" for this suite.
STEP: Destroying namespace "webhook-4610-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":173,"skipped":2842,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:18:10.171: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-c258ec24-fd3b-464c-be7f-f8958e45091a
STEP: Creating a pod to test consume configMaps
Nov 23 12:18:10.348: INFO: Waiting up to 5m0s for pod "pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984" in namespace "configmap-1959" to be "Succeeded or Failed"
Nov 23 12:18:10.364: INFO: Pod "pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984": Phase="Pending", Reason="", readiness=false. Elapsed: 15.538777ms
Nov 23 12:18:12.367: INFO: Pod "pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018969258s
STEP: Saw pod success
Nov 23 12:18:12.367: INFO: Pod "pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984" satisfied condition "Succeeded or Failed"
Nov 23 12:18:12.370: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:18:12.402: INFO: Waiting for pod pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984 to disappear
Nov 23 12:18:12.405: INFO: Pod pod-configmaps-c38976c8-3aca-4424-ac40-0ed49efba984 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:18:12.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1959" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":2843,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:18:12.414: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-b2ng
STEP: Creating a pod to test atomic-volume-subpath
Nov 23 12:18:12.589: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-b2ng" in namespace "subpath-6854" to be "Succeeded or Failed"
Nov 23 12:18:12.592: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Pending", Reason="", readiness=false. Elapsed: 2.418721ms
Nov 23 12:18:14.596: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 2.006226509s
Nov 23 12:18:16.606: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 4.01649067s
Nov 23 12:18:18.609: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 6.019590461s
Nov 23 12:18:20.612: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 8.022856736s
Nov 23 12:18:22.616: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 10.026475359s
Nov 23 12:18:24.620: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 12.030820481s
Nov 23 12:18:26.624: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 14.035050407s
Nov 23 12:18:28.629: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 16.03944983s
Nov 23 12:18:30.632: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 18.043138501s
Nov 23 12:18:32.636: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Running", Reason="", readiness=true. Elapsed: 20.046204328s
Nov 23 12:18:34.640: INFO: Pod "pod-subpath-test-configmap-b2ng": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050411902s
STEP: Saw pod success
Nov 23 12:18:34.640: INFO: Pod "pod-subpath-test-configmap-b2ng" satisfied condition "Succeeded or Failed"
Nov 23 12:18:34.643: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-subpath-test-configmap-b2ng container test-container-subpath-configmap-b2ng: <nil>
STEP: delete the pod
Nov 23 12:18:34.670: INFO: Waiting for pod pod-subpath-test-configmap-b2ng to disappear
Nov 23 12:18:34.675: INFO: Pod pod-subpath-test-configmap-b2ng no longer exists
STEP: Deleting pod pod-subpath-test-configmap-b2ng
Nov 23 12:18:34.675: INFO: Deleting pod "pod-subpath-test-configmap-b2ng" in namespace "subpath-6854"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:18:34.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6854" for this suite.

• [SLOW TEST:22.279 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":175,"skipped":2847,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:18:34.692: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:18:50.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9785" for this suite.

• [SLOW TEST:16.215 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":176,"skipped":2868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:18:50.908: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:18:51.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3657" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":177,"skipped":2899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:18:51.116: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4791
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4791
STEP: creating replication controller externalsvc in namespace services-4791
I1123 12:18:51.311997      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4791, replica count: 2
I1123 12:18:54.362599      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 23 12:18:54.381: INFO: Creating new exec pod
Nov 23 12:18:56.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-4791 execpodrr6xc -- /bin/sh -x -c nslookup clusterip-service.services-4791.svc.cluster.local'
Nov 23 12:18:56.664: INFO: stderr: "+ nslookup clusterip-service.services-4791.svc.cluster.local\n"
Nov 23 12:18:56.664: INFO: stdout: ";; Got recursion not available from 169.254.20.10, trying next server\n;; Got recursion not available from 169.254.20.10, trying next server\n;; Got recursion not available from 169.254.20.10, trying next server\n;; Got recursion not available from 169.254.20.10, trying next server\nServer:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-4791.svc.cluster.local\tcanonical name = externalsvc.services-4791.svc.cluster.local.\nName:\texternalsvc.services-4791.svc.cluster.local\nAddress: 100.69.9.97\n;; Got recursion not available from 169.254.20.10, trying next server\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4791, will wait for the garbage collector to delete the pods
Nov 23 12:18:56.726: INFO: Deleting ReplicationController externalsvc took: 7.53398ms
Nov 23 12:18:57.327: INFO: Terminating ReplicationController externalsvc pods took: 600.365089ms
Nov 23 12:19:09.087: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:19:09.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4791" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.999 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":178,"skipped":2930,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:19:09.116: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8108
STEP: creating service affinity-clusterip-transition in namespace services-8108
STEP: creating replication controller affinity-clusterip-transition in namespace services-8108
I1123 12:19:09.288526      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8108, replica count: 3
I1123 12:19:12.338905      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 12:19:12.343: INFO: Creating new exec pod
Nov 23 12:19:15.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-8108 execpod-affinityd4pch -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Nov 23 12:19:15.607: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 23 12:19:15.607: INFO: stdout: ""
Nov 23 12:19:15.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-8108 execpod-affinityd4pch -- /bin/sh -x -c nc -zv -t -w 2 100.70.111.164 80'
Nov 23 12:19:15.856: INFO: stderr: "+ nc -zv -t -w 2 100.70.111.164 80\nConnection to 100.70.111.164 80 port [tcp/http] succeeded!\n"
Nov 23 12:19:15.856: INFO: stdout: ""
Nov 23 12:19:15.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-8108 execpod-affinityd4pch -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.70.111.164:80/ ; done'
Nov 23 12:19:16.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n"
Nov 23 12:19:16.186: INFO: stdout: "\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd"
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:16.186: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-8108 execpod-affinityd4pch -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.70.111.164:80/ ; done'
Nov 23 12:19:46.493: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n"
Nov 23 12:19:46.493: INFO: stdout: "\naffinity-clusterip-transition-8586v\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-8586v\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-8586v\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-8586v\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-xgwcd\naffinity-clusterip-transition-8586v"
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-8586v
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-8586v
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-8586v
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-8586v
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-xgwcd
Nov 23 12:19:46.493: INFO: Received response from host: affinity-clusterip-transition-8586v
Nov 23 12:19:46.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-8108 execpod-affinityd4pch -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.70.111.164:80/ ; done'
Nov 23 12:19:46.840: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.70.111.164:80/\n"
Nov 23 12:19:46.841: INFO: stdout: "\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25\naffinity-clusterip-transition-2gp25"
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Received response from host: affinity-clusterip-transition-2gp25
Nov 23 12:19:46.841: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8108, will wait for the garbage collector to delete the pods
Nov 23 12:19:46.924: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.918749ms
Nov 23 12:19:47.527: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 602.625211ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:20:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8108" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:51.751 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":179,"skipped":2931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:20:00.866: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 23 12:20:01.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8842 /api/v1/namespaces/watch-8842/configmaps/e2e-watch-test-watch-closed b5ab483d-0342-47c8-9aaa-a08a3257077e 62063 0 2020-11-23 12:20:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-23 12:20:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 12:20:01.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8842 /api/v1/namespaces/watch-8842/configmaps/e2e-watch-test-watch-closed b5ab483d-0342-47c8-9aaa-a08a3257077e 62064 0 2020-11-23 12:20:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-23 12:20:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 23 12:20:01.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8842 /api/v1/namespaces/watch-8842/configmaps/e2e-watch-test-watch-closed b5ab483d-0342-47c8-9aaa-a08a3257077e 62065 0 2020-11-23 12:20:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-23 12:20:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 12:20:01.033: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8842 /api/v1/namespaces/watch-8842/configmaps/e2e-watch-test-watch-closed b5ab483d-0342-47c8-9aaa-a08a3257077e 62066 0 2020-11-23 12:20:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-11-23 12:20:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:20:01.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8842" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":180,"skipped":2960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:20:01.050: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 23 12:20:07.258: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.258: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:07.354: INFO: Exec stderr: ""
Nov 23 12:20:07.354: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:07.437: INFO: Exec stderr: ""
Nov 23 12:20:07.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.437: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:07.556: INFO: Exec stderr: ""
Nov 23 12:20:07.556: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.556: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:07.673: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 23 12:20:07.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.673: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:07.780: INFO: Exec stderr: ""
Nov 23 12:20:07.780: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.780: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:07.904: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 23 12:20:07.904: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:07.904: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:08.018: INFO: Exec stderr: ""
Nov 23 12:20:08.018: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:08.018: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:08.155: INFO: Exec stderr: ""
Nov 23 12:20:08.155: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:08.155: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:08.278: INFO: Exec stderr: ""
Nov 23 12:20:08.278: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:20:08.278: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:08.409: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:20:08.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6757" for this suite.

• [SLOW TEST:7.377 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":3000,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:20:08.427: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:20:08.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094" in namespace "projected-1230" to be "Succeeded or Failed"
Nov 23 12:20:08.606: INFO: Pod "downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990018ms
Nov 23 12:20:10.611: INFO: Pod "downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00802616s
STEP: Saw pod success
Nov 23 12:20:10.611: INFO: Pod "downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094" satisfied condition "Succeeded or Failed"
Nov 23 12:20:10.615: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094 container client-container: <nil>
STEP: delete the pod
Nov 23 12:20:10.650: INFO: Waiting for pod downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094 to disappear
Nov 23 12:20:10.655: INFO: Pod downwardapi-volume-f70b2041-54c2-402c-b111-dd9aca624094 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:20:10.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1230" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":3020,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:20:10.667: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:20:11.705: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:20:14.728: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:20:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:20:45.524: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-8234-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-3861.svc:9443/crdconvert?timeout=30s": context deadline exceeded
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:20:46.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3861" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:35.571 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":183,"skipped":3048,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:20:46.239: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:20:47.078: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:20:50.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
Nov 23 12:20:52.160: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:20:54.310: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:20:56.504: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:20:58.652: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:21:10.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5643" for this suite.
STEP: Destroying namespace "webhook-5643-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:24.617 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":184,"skipped":3078,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:21:10.857: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 23 12:23:11.560: INFO: Successfully updated pod "var-expansion-b109a37d-88f2-470a-962a-04331e9cdc9d"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 23 12:23:13.568: INFO: Deleting pod "var-expansion-b109a37d-88f2-470a-962a-04331e9cdc9d" in namespace "var-expansion-5327"
Nov 23 12:23:13.573: INFO: Wait up to 5m0s for pod "var-expansion-b109a37d-88f2-470a-962a-04331e9cdc9d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:23:45.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5327" for this suite.

• [SLOW TEST:154.736 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":185,"skipped":3099,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:23:45.593: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:23:46.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:23:49.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:23:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-140" for this suite.
STEP: Destroying namespace "webhook-140-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":186,"skipped":3106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:23:49.490: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:23:49.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8" in namespace "downward-api-2966" to be "Succeeded or Failed"
Nov 23 12:23:49.641: INFO: Pod "downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470063ms
Nov 23 12:23:51.644: INFO: Pod "downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005829266s
STEP: Saw pod success
Nov 23 12:23:51.644: INFO: Pod "downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8" satisfied condition "Succeeded or Failed"
Nov 23 12:23:51.652: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8 container client-container: <nil>
STEP: delete the pod
Nov 23 12:23:51.702: INFO: Waiting for pod downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8 to disappear
Nov 23 12:23:51.707: INFO: Pod downwardapi-volume-8d4c2361-05b8-48c7-9054-8db0706dadc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:23:51.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2966" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":3155,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:23:51.719: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 23 12:23:51.876: INFO: Created pod &Pod{ObjectMeta:{dns-9236  dns-9236 /api/v1/namespaces/dns-9236/pods/dns-9236 820b3cbf-0b45-4be6-b31b-0bc46ec6d465 63108 0 2020-11-23 12:23:51 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-11-23 12:23:51 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mt68b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mt68b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mt68b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:23:51.890: INFO: The status of Pod dns-9236 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:23:53.893: INFO: The status of Pod dns-9236 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 23 12:23:53.894: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9236 PodName:dns-9236 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:23:53.894: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Verifying customized DNS server is configured on pod...
Nov 23 12:23:54.000: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9236 PodName:dns-9236 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:23:54.000: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:23:54.112: INFO: Deleting pod dns-9236...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:23:54.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9236" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":188,"skipped":3157,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:23:54.138: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3647
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-3647
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3647
Nov 23 12:23:54.316: INFO: Found 0 stateful pods, waiting for 1
Nov 23 12:24:04.321: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 23 12:24:04.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 12:24:04.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 12:24:04.602: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 12:24:04.602: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 12:24:04.607: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 23 12:24:14.616: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 12:24:14.616: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 12:24:14.652: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:14.652: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:14.652: INFO: ss-1  dvi-conformance-1606121581-vsp1-group1-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:14.652: INFO: 
Nov 23 12:24:14.652: INFO: StatefulSet ss has not reached scale 3, at 2
Nov 23 12:24:15.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976070318s
Nov 23 12:24:16.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971988161s
Nov 23 12:24:17.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96255305s
Nov 23 12:24:18.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.955354731s
Nov 23 12:24:19.685: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95032571s
Nov 23 12:24:20.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943981774s
Nov 23 12:24:21.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938902829s
Nov 23 12:24:22.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933883079s
Nov 23 12:24:23.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.656072ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3647
Nov 23 12:24:24.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:24:24.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 23 12:24:24.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 12:24:24.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 12:24:24.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:24:25.227: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 23 12:24:25.227: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 12:24:25.227: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 12:24:25.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:24:25.469: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 23 12:24:25.469: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 23 12:24:25.469: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 23 12:24:25.474: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 23 12:24:35.479: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 12:24:35.479: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 23 12:24:35.479: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 23 12:24:35.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 12:24:35.727: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 12:24:35.727: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 12:24:35.727: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 12:24:35.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 12:24:35.976: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 12:24:35.976: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 12:24:35.976: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 12:24:35.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 23 12:24:36.202: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 23 12:24:36.202: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 23 12:24:36.202: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 23 12:24:36.202: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 12:24:36.205: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 23 12:24:46.212: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 12:24:46.212: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 12:24:46.212: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 23 12:24:46.225: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:46.226: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:46.226: INFO: ss-1  dvi-conformance-1606121581-vsp1-group1-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:46.226: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:46.226: INFO: 
Nov 23 12:24:46.226: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 23 12:24:47.230: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:47.230: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:47.230: INFO: ss-1  dvi-conformance-1606121581-vsp1-group1-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:47.230: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:47.230: INFO: 
Nov 23 12:24:47.230: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 23 12:24:48.236: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:48.236: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:48.236: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:48.236: INFO: 
Nov 23 12:24:48.236: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:49.240: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:49.240: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:49.240: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:49.240: INFO: 
Nov 23 12:24:49.240: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:50.244: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:50.244: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:50.244: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:50.244: INFO: 
Nov 23 12:24:50.244: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:51.248: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:51.248: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:51.248: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:51.248: INFO: 
Nov 23 12:24:51.248: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:52.253: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:52.253: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:52.253: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:52.253: INFO: 
Nov 23 12:24:52.253: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:53.258: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:53.258: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:53.258: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:53.258: INFO: 
Nov 23 12:24:53.258: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:54.262: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:54.262: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:54.262: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:54.262: INFO: 
Nov 23 12:24:54.262: INFO: StatefulSet ss has not reached scale 0, at 2
Nov 23 12:24:55.266: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Nov 23 12:24:55.266: INFO: ss-0  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:23:54 +0000 UTC  }]
Nov 23 12:24:55.266: INFO: ss-2  dvi-conformance-1606121581-vsp1-group1-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-11-23 12:24:14 +0000 UTC  }]
Nov 23 12:24:55.266: INFO: 
Nov 23 12:24:55.266: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3647
Nov 23 12:24:56.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:24:56.435: INFO: rc: 1
Nov 23 12:24:56.435: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Nov 23 12:25:06.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:25:06.547: INFO: rc: 1
Nov 23 12:25:06.547: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:25:16.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:25:16.665: INFO: rc: 1
Nov 23 12:25:16.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:25:26.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:25:26.786: INFO: rc: 1
Nov 23 12:25:26.786: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:25:36.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:25:36.916: INFO: rc: 1
Nov 23 12:25:36.917: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:25:46.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:25:47.032: INFO: rc: 1
Nov 23 12:25:47.032: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:25:57.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:25:57.161: INFO: rc: 1
Nov 23 12:25:57.161: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:26:07.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:26:07.265: INFO: rc: 1
Nov 23 12:26:07.265: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:26:17.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:26:17.377: INFO: rc: 1
Nov 23 12:26:17.377: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:26:27.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:26:27.506: INFO: rc: 1
Nov 23 12:26:27.506: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:26:37.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:26:37.628: INFO: rc: 1
Nov 23 12:26:37.629: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:26:47.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:26:47.753: INFO: rc: 1
Nov 23 12:26:47.753: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:26:57.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:26:57.868: INFO: rc: 1
Nov 23 12:26:57.868: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:27:07.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:27:07.986: INFO: rc: 1
Nov 23 12:27:07.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:27:17.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:27:18.101: INFO: rc: 1
Nov 23 12:27:18.101: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:27:28.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:27:28.213: INFO: rc: 1
Nov 23 12:27:28.213: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:27:38.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:27:38.324: INFO: rc: 1
Nov 23 12:27:38.324: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:27:48.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:27:48.441: INFO: rc: 1
Nov 23 12:27:48.441: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:27:58.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:27:58.554: INFO: rc: 1
Nov 23 12:27:58.554: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:28:08.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:28:08.676: INFO: rc: 1
Nov 23 12:28:08.676: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:28:18.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:28:18.782: INFO: rc: 1
Nov 23 12:28:18.782: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:28:28.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:28:28.899: INFO: rc: 1
Nov 23 12:28:28.899: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:28:38.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:28:39.023: INFO: rc: 1
Nov 23 12:28:39.023: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:28:49.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:28:49.145: INFO: rc: 1
Nov 23 12:28:49.145: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:28:59.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:28:59.265: INFO: rc: 1
Nov 23 12:28:59.265: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:29:09.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:29:09.382: INFO: rc: 1
Nov 23 12:29:09.382: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:29:19.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:29:19.494: INFO: rc: 1
Nov 23 12:29:19.494: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:29:29.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:29:29.602: INFO: rc: 1
Nov 23 12:29:29.602: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:29:39.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:29:39.715: INFO: rc: 1
Nov 23 12:29:39.715: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:29:49.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:29:49.821: INFO: rc: 1
Nov 23 12:29:49.822: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Nov 23 12:29:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=statefulset-3647 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 23 12:29:59.933: INFO: rc: 1
Nov 23 12:29:59.933: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Nov 23 12:29:59.933: INFO: Scaling statefulset ss to 0
Nov 23 12:29:59.943: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 23 12:29:59.947: INFO: Deleting all statefulset in ns statefulset-3647
Nov 23 12:29:59.953: INFO: Scaling statefulset ss to 0
Nov 23 12:29:59.961: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 12:29:59.964: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:29:59.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3647" for this suite.

• [SLOW TEST:365.856 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":189,"skipped":3170,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:29:59.994: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-c1f317f8-de18-4858-860b-b0a498c2326a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:00.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8724" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":190,"skipped":3180,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:00.172: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:30:00.348: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01" in namespace "projected-1929" to be "Succeeded or Failed"
Nov 23 12:30:00.357: INFO: Pod "downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01": Phase="Pending", Reason="", readiness=false. Elapsed: 9.432235ms
Nov 23 12:30:02.362: INFO: Pod "downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014308203s
STEP: Saw pod success
Nov 23 12:30:02.362: INFO: Pod "downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01" satisfied condition "Succeeded or Failed"
Nov 23 12:30:02.366: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01 container client-container: <nil>
STEP: delete the pod
Nov 23 12:30:02.407: INFO: Waiting for pod downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01 to disappear
Nov 23 12:30:02.412: INFO: Pod downwardapi-volume-bf8801bf-7f82-4a7e-9742-0b4d39be4f01 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:02.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1929" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":3185,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:02.424: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Nov 23 12:30:02.610: INFO: created test-pod-1
Nov 23 12:30:02.616: INFO: created test-pod-2
Nov 23 12:30:02.632: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:02.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9113" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":192,"skipped":3194,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:02.746: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-62030681-efc2-4def-8a0b-532ef936dde8
STEP: Creating a pod to test consume configMaps
Nov 23 12:30:02.925: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58" in namespace "projected-2280" to be "Succeeded or Failed"
Nov 23 12:30:02.943: INFO: Pod "pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58": Phase="Pending", Reason="", readiness=false. Elapsed: 18.393858ms
Nov 23 12:30:04.950: INFO: Pod "pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025220881s
STEP: Saw pod success
Nov 23 12:30:04.950: INFO: Pod "pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58" satisfied condition "Succeeded or Failed"
Nov 23 12:30:04.954: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:30:04.992: INFO: Waiting for pod pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58 to disappear
Nov 23 12:30:05.001: INFO: Pod pod-projected-configmaps-0ccbc448-997a-4bdb-85a6-272e444f2a58 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:05.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2280" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:05.024: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3654
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 23 12:30:07.830: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3654 pod-service-account-37f9c81d-0a63-4610-9836-7f319742a394 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 23 12:30:08.069: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3654 pod-service-account-37f9c81d-0a63-4610-9836-7f319742a394 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 23 12:30:08.327: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3654 pod-service-account-37f9c81d-0a63-4610-9836-7f319742a394 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3654" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":194,"skipped":3247,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:08.543: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 23 12:30:08.680: INFO: PodSpec: initContainers in spec.initContainers
Nov 23 12:30:52.856: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-192cbe15-c1aa-4a27-9704-1d717b246a3f", GenerateName:"", Namespace:"init-container-625", SelfLink:"/api/v1/namespaces/init-container-625/pods/pod-init-192cbe15-c1aa-4a27-9704-1d717b246a3f", UID:"a5650391-bd92-4094-ac3a-73b9c427272a", ResourceVersion:"64691", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63741731408, loc:(*time.Location)(0x770e880)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"680422666"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.2.115/32", "cni.projectcalico.org/podIPs":"100.96.2.115/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002d9ade0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002d9ae00)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002d9ae40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002d9ae80)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002d9aea0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002d9aec0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-j4mvj", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004143e80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j4mvj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j4mvj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j4mvj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00308cdd8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"dvi-conformance-1606121581-vsp1-group1-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002049880), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00308ce80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00308cea0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00308cea8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00308ceac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00317f000), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741731408, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741731408, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741731408, loc:(*time.Location)(0x770e880)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741731408, loc:(*time.Location)(0x770e880)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.8.54", PodIP:"100.96.2.115", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.2.115"}}, StartTime:(*v1.Time)(0xc002d9aee0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002049960)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0020499d0)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://c3538d1dfbaf6c2cbdbf9fda6c64a01661bb2801352054518b85bf281c1a2e02", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d9af40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d9af20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00308cf2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:52.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-625" for this suite.

• [SLOW TEST:44.345 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":195,"skipped":3248,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:52.888: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 23 12:30:53.060: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 23 12:30:53.075: INFO: Waiting for terminating namespaces to be deleted...
Nov 23 12:30:53.089: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-0 before test
Nov 23 12:30:53.115: INFO: pod-init-192cbe15-c1aa-4a27-9704-1d717b246a3f from init-container-625 started at 2020-11-23 12:30:08 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container run1 ready: false, restart count 0
Nov 23 12:30:53.116: INFO: canal-92zmd from kube-system started at 2020-11-23 09:08:12 +0000 UTC (3 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 12:30:53.116: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 12:30:53.116: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 12:30:53.116: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 12:30:53.116: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 12:30:53.116: INFO: kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0 from kube-system started at 2020-11-23 09:07:39 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container main ready: true, restart count 0
Nov 23 12:30:53.116: INFO: node-local-dns-vnlcb from kube-system started at 2020-11-23 09:08:12 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 12:30:53.116: INFO: sonobuoy from sonobuoy started at 2020-11-23 11:25:21 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 23 12:30:53.116: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 12:30:53.116: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 23 12:30:53.116: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 23 12:30:53.116: INFO: 
Logging pods the apiserver thinks is on node dvi-conformance-1606121581-vsp1-group1-1 before test
Nov 23 12:30:53.124: INFO: canal-pgdpd from kube-system started at 2020-11-23 09:08:13 +0000 UTC (3 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container calico-node ready: true, restart count 0
Nov 23 12:30:53.125: INFO: 	Container kube-flannel ready: true, restart count 0
Nov 23 12:30:53.125: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Nov 23 12:30:53.125: INFO: k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Nov 23 12:30:53.125: INFO: kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 23 12:30:53.125: INFO: kublr-node-name-reporter-c33b8d393ebe42a50f83769b2f12339f5d1f5d10dd75810de5733c313118246a-dvi-conformance-1606121581-vsp1-group1-1 from kube-system started at 2020-11-23 09:07:40 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container main ready: true, restart count 0
Nov 23 12:30:53.125: INFO: kublr-system-shell-564c49bf57-ksqmb from kube-system started at 2020-11-23 09:09:38 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container shell ready: true, restart count 0
Nov 23 12:30:53.125: INFO: metrics-server-v0.3.6-7d5754b8df-2hd8f from kube-system started at 2020-11-23 11:42:06 +0000 UTC (2 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container metrics-server ready: true, restart count 0
Nov 23 12:30:53.125: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 23 12:30:53.125: INFO: node-local-dns-74tqs from kube-system started at 2020-11-23 09:08:13 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container node-cache ready: true, restart count 0
Nov 23 12:30:53.125: INFO: kublr-operator-69c9677745-zk5lz from kublr started at 2020-11-23 11:19:03 +0000 UTC (1 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container kublr-operator ready: true, restart count 0
Nov 23 12:30:53.125: INFO: sonobuoy-e2e-job-7d6aa2c1eacf4413 from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container e2e ready: true, restart count 0
Nov 23 12:30:53.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 23 12:30:53.125: INFO: sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-sj5vn from sonobuoy started at 2020-11-23 11:25:22 +0000 UTC (2 container statuses recorded)
Nov 23 12:30:53.125: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Nov 23 12:30:53.125: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node dvi-conformance-1606121581-vsp1-group1-0
STEP: verifying the node has the label node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod pod-init-192cbe15-c1aa-4a27-9704-1d717b246a3f requesting resource cpu=100m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod canal-92zmd requesting resource cpu=40m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod canal-pgdpd requesting resource cpu=40m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-0 requesting resource cpu=1m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod k8s-api-haproxy-7d8ddb5d7f24ace91805abed92a4c2ab9af2d5464e14c9c84aec5544cda2baa4-dvi-conformance-1606121581-vsp1-group1-1 requesting resource cpu=1m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-0 requesting resource cpu=5m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod kube-proxy-7f8a7d48757ae1e6df9a864004484f48dce97c6c0dc6fe712073393c9c77ef3d-dvi-conformance-1606121581-vsp1-group1-1 requesting resource cpu=5m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod kublr-node-name-reporter-1f1e9001e550c590b9b1714d93acd7d6a397286b0968e6926043b4f58bcb498d-dvi-conformance-1606121581-vsp1-group1-0 requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod kublr-node-name-reporter-c33b8d393ebe42a50f83769b2f12339f5d1f5d10dd75810de5733c313118246a-dvi-conformance-1606121581-vsp1-group1-1 requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod kublr-system-shell-564c49bf57-ksqmb requesting resource cpu=10m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod metrics-server-v0.3.6-7d5754b8df-2hd8f requesting resource cpu=98m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod node-local-dns-74tqs requesting resource cpu=25m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod node-local-dns-vnlcb requesting resource cpu=25m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod kublr-operator-69c9677745-zk5lz requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod sonobuoy requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod sonobuoy-e2e-job-7d6aa2c1eacf4413 requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-1
Nov 23 12:30:53.207: INFO: Pod sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-dtkdd requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.207: INFO: Pod sonobuoy-systemd-logs-daemon-set-b056dc13a25543f5-sj5vn requesting resource cpu=0m on Node dvi-conformance-1606121581-vsp1-group1-1
STEP: Starting Pods to consume most of the cluster CPU.
Nov 23 12:30:53.207: INFO: Creating a pod which consumes cpu=2610m on Node dvi-conformance-1606121581-vsp1-group1-0
Nov 23 12:30:53.221: INFO: Creating a pod which consumes cpu=2604m on Node dvi-conformance-1606121581-vsp1-group1-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308.164a22f51b498a76], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8415/filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308 to dvi-conformance-1606121581-vsp1-group1-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e.164a22f54b801c10], Reason = [Created], Message = [Created container filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308.164a22f5461262e0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e.164a22f51d389183], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8415/filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e to dvi-conformance-1606121581-vsp1-group1-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308.164a22f547cb8ffb], Reason = [Created], Message = [Created container filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e.164a22f5558fa36a], Reason = [Started], Message = [Started container filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308.164a22f551ca77ae], Reason = [Started], Message = [Started container filler-pod-4f58f3be-71f8-401f-af5c-690b04e5b308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-004a1a13-a22b-4959-80c3-effba4f2679e.164a22f54978dbe5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.164a22f5954d364f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node dvi-conformance-1606121581-vsp1-group1-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node dvi-conformance-1606121581-vsp1-group1-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:30:56.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8415" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":196,"skipped":3249,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:30:56.331: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1085
STEP: creating service affinity-clusterip in namespace services-1085
STEP: creating replication controller affinity-clusterip in namespace services-1085
I1123 12:30:56.497214      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1085, replica count: 3
I1123 12:30:59.547895      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 12:30:59.555: INFO: Creating new exec pod
Nov 23 12:31:02.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov 23 12:31:04.838: INFO: rc: 1
Nov 23 12:31:04.838: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:31:05.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov 23 12:31:08.057: INFO: rc: 1
Nov 23 12:31:08.058: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:31:08.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Nov 23 12:31:09.084: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 23 12:31:09.084: INFO: stdout: ""
Nov 23 12:31:09.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c nc -zv -t -w 2 100.64.217.248 80'
Nov 23 12:31:09.352: INFO: stderr: "+ nc -zv -t -w 2 100.64.217.248 80\nConnection to 100.64.217.248 80 port [tcp/http] succeeded!\n"
Nov 23 12:31:09.352: INFO: stdout: ""
Nov 23 12:31:09.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1085 execpod-affinityj7hqh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.64.217.248:80/ ; done'
Nov 23 12:31:09.660: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.64.217.248:80/\n"
Nov 23 12:31:09.660: INFO: stdout: "\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q\naffinity-clusterip-v9m6q"
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Received response from host: affinity-clusterip-v9m6q
Nov 23 12:31:09.660: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1085, will wait for the garbage collector to delete the pods
Nov 23 12:31:09.744: INFO: Deleting ReplicationController affinity-clusterip took: 10.611889ms
Nov 23 12:31:10.444: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.301012ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:31:20.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1085" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:24.546 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":197,"skipped":3263,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:31:20.878: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7848
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 23 12:31:23.044: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7848 PodName:pod-sharedvolume-7d74ee8a-1238-4d71-a849-c5d213197ba3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:31:23.044: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:31:23.135: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:31:23.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7848" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":198,"skipped":3275,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:31:23.150: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4910
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 23 12:31:23.310: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 23 12:32:23.369: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:32:23.372: INFO: Starting informer...
STEP: Starting pods...
Nov 23 12:32:23.593: INFO: Pod1 is running on dvi-conformance-1606121581-vsp1-group1-0. Tainting Node
Nov 23 12:32:25.814: INFO: Pod2 is running on dvi-conformance-1606121581-vsp1-group1-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 23 12:32:33.112: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 23 12:32:53.330: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:32:53.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4910" for this suite.

• [SLOW TEST:90.231 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":199,"skipped":3286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:32:53.382: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Nov 23 12:32:53.547: INFO: Waiting up to 5m0s for pod "downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe" in namespace "downward-api-4362" to be "Succeeded or Failed"
Nov 23 12:32:53.560: INFO: Pod "downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.898755ms
Nov 23 12:32:55.568: INFO: Pod "downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021361007s
STEP: Saw pod success
Nov 23 12:32:55.568: INFO: Pod "downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe" satisfied condition "Succeeded or Failed"
Nov 23 12:32:55.575: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe container dapi-container: <nil>
STEP: delete the pod
Nov 23 12:32:55.613: INFO: Waiting for pod downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe to disappear
Nov 23 12:32:55.616: INFO: Pod downward-api-5576600a-a646-4359-b6b2-aa9fc2b693fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:32:55.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4362" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":200,"skipped":3316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:32:55.635: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4030
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-a4a0ec15-7059-42ae-9b7a-6d78c4af1b7e
STEP: Creating configMap with name cm-test-opt-upd-453158f9-0f7c-4a30-97e7-4da7bb870c53
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a4a0ec15-7059-42ae-9b7a-6d78c4af1b7e
STEP: Updating configmap cm-test-opt-upd-453158f9-0f7c-4a30-97e7-4da7bb870c53
STEP: Creating configMap with name cm-test-opt-create-c05eab18-71dd-4f99-a1b1-7ef29d997d64
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:32:59.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4030" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":201,"skipped":3361,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:32:59.922: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:33:00.086: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 23 12:33:00.093: INFO: Number of nodes with available pods: 0
Nov 23 12:33:00.093: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 23 12:33:00.121: INFO: Number of nodes with available pods: 0
Nov 23 12:33:00.121: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:33:01.125: INFO: Number of nodes with available pods: 0
Nov 23 12:33:01.125: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:33:02.125: INFO: Number of nodes with available pods: 1
Nov 23 12:33:02.125: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 23 12:33:02.151: INFO: Number of nodes with available pods: 1
Nov 23 12:33:02.151: INFO: Number of running nodes: 0, number of available pods: 1
Nov 23 12:33:03.155: INFO: Number of nodes with available pods: 0
Nov 23 12:33:03.155: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 23 12:33:03.170: INFO: Number of nodes with available pods: 0
Nov 23 12:33:03.170: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:33:04.173: INFO: Number of nodes with available pods: 0
Nov 23 12:33:04.173: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:33:05.221: INFO: Number of nodes with available pods: 0
Nov 23 12:33:05.221: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:33:06.174: INFO: Number of nodes with available pods: 0
Nov 23 12:33:06.174: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:33:07.174: INFO: Number of nodes with available pods: 1
Nov 23 12:33:07.174: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9828, will wait for the garbage collector to delete the pods
Nov 23 12:33:07.236: INFO: Deleting DaemonSet.extensions daemon-set took: 5.342182ms
Nov 23 12:33:07.836: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.234281ms
Nov 23 12:33:19.039: INFO: Number of nodes with available pods: 0
Nov 23 12:33:19.039: INFO: Number of running nodes: 0, number of available pods: 0
Nov 23 12:33:19.041: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9828/daemonsets","resourceVersion":"65495"},"items":null}

Nov 23 12:33:19.043: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9828/pods","resourceVersion":"65495"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:33:19.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9828" for this suite.

• [SLOW TEST:19.148 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":202,"skipped":3368,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:33:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:33:19.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6780" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":203,"skipped":3376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:33:19.273: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 23 12:33:19.419: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:33:23.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6960" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":204,"skipped":3407,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:33:23.472: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-8206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Nov 23 12:33:23.633: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 23 12:34:23.682: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Nov 23 12:34:23.714: INFO: Created pod: pod0-sched-preemption-low-priority
Nov 23 12:34:23.754: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:34:51.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8206" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:88.413 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":205,"skipped":3409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:34:51.887: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 23 12:34:52.119: INFO: Waiting up to 5m0s for pod "pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4" in namespace "emptydir-1812" to be "Succeeded or Failed"
Nov 23 12:34:52.128: INFO: Pod "pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.270063ms
Nov 23 12:34:54.132: INFO: Pod "pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012892069s
STEP: Saw pod success
Nov 23 12:34:54.132: INFO: Pod "pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4" satisfied condition "Succeeded or Failed"
Nov 23 12:34:54.135: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4 container test-container: <nil>
STEP: delete the pod
Nov 23 12:34:54.174: INFO: Waiting for pod pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4 to disappear
Nov 23 12:34:54.176: INFO: Pod pod-74dd1857-f869-4b48-8af6-7c35dc8c65c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:34:54.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1812" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:34:54.186: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-d187bfca-c269-4105-8665-4bf0fe609031
STEP: Creating a pod to test consume configMaps
Nov 23 12:34:54.344: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb" in namespace "projected-4946" to be "Succeeded or Failed"
Nov 23 12:34:54.348: INFO: Pod "pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192967ms
Nov 23 12:34:56.353: INFO: Pod "pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008732713s
STEP: Saw pod success
Nov 23 12:34:56.353: INFO: Pod "pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb" satisfied condition "Succeeded or Failed"
Nov 23 12:34:56.356: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:34:56.380: INFO: Waiting for pod pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb to disappear
Nov 23 12:34:56.389: INFO: Pod pod-projected-configmaps-2a423128-0fb6-4c94-b58f-543fbd8ebceb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:34:56.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4946" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3463,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:34:56.402: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:35:04.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9388" for this suite.

• [SLOW TEST:8.187 seconds]
[sig-apps] Job
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":208,"skipped":3483,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:35:04.589: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-93db55d4-fca1-40e5-9d96-3d6e71a19704
STEP: Creating a pod to test consume configMaps
Nov 23 12:35:04.800: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8" in namespace "configmap-5641" to be "Succeeded or Failed"
Nov 23 12:35:04.805: INFO: Pod "pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.02768ms
Nov 23 12:35:06.810: INFO: Pod "pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010398625s
STEP: Saw pod success
Nov 23 12:35:06.810: INFO: Pod "pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8" satisfied condition "Succeeded or Failed"
Nov 23 12:35:06.814: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:35:06.838: INFO: Waiting for pod pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8 to disappear
Nov 23 12:35:06.844: INFO: Pod pod-configmaps-d1b4797b-cddb-4b8c-bdda-09adb1669ea8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:35:06.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5641" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:35:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:37:07.049: INFO: Deleting pod "var-expansion-9111dfe8-4472-4cb8-8f3e-5d676cb9e0de" in namespace "var-expansion-2415"
Nov 23 12:37:07.061: INFO: Wait up to 5m0s for pod "var-expansion-9111dfe8-4472-4cb8-8f3e-5d676cb9e0de" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:11.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2415" for this suite.

• [SLOW TEST:124.222 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":210,"skipped":3519,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:11.078: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:15.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7120" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3529,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:15.280: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-26
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:37:15.425: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-26" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":212,"skipped":3533,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:16.017: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 23 12:37:18.787: INFO: Successfully updated pod "labelsupdate6238909c-fd23-4017-951e-df88a6fc130c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:22.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3020" for this suite.

• [SLOW TEST:6.809 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3549,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:22.826: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 23 12:37:25.013: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:25.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3316" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3557,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:25.041: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-4331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Nov 23 12:37:25.192: INFO: Major version: 1
STEP: Confirm minor version
Nov 23 12:37:25.192: INFO: cleanMinorVersion: 19
Nov 23 12:37:25.192: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:25.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4331" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":215,"skipped":3564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:25.204: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Nov 23 12:37:25.351: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 23 12:37:25.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6219'
Nov 23 12:37:25.788: INFO: stderr: ""
Nov 23 12:37:25.788: INFO: stdout: "service/agnhost-replica created\n"
Nov 23 12:37:25.788: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 23 12:37:25.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6219'
Nov 23 12:37:26.089: INFO: stderr: ""
Nov 23 12:37:26.089: INFO: stdout: "service/agnhost-primary created\n"
Nov 23 12:37:26.089: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 23 12:37:26.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6219'
Nov 23 12:37:26.355: INFO: stderr: ""
Nov 23 12:37:26.356: INFO: stdout: "service/frontend created\n"
Nov 23 12:37:26.356: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 23 12:37:26.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6219'
Nov 23 12:37:26.627: INFO: stderr: ""
Nov 23 12:37:26.627: INFO: stdout: "deployment.apps/frontend created\n"
Nov 23 12:37:26.627: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 23 12:37:26.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6219'
Nov 23 12:37:26.929: INFO: stderr: ""
Nov 23 12:37:26.929: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 23 12:37:26.930: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 23 12:37:26.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6219'
Nov 23 12:37:27.222: INFO: stderr: ""
Nov 23 12:37:27.222: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 23 12:37:27.222: INFO: Waiting for all frontend pods to be Running.
Nov 23 12:37:32.273: INFO: Waiting for frontend to serve content.
Nov 23 12:37:34.363: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Nov 23 12:37:39.374: INFO: Trying to add a new entry to the guestbook.
Nov 23 12:37:39.387: INFO: Verifying that added entry can be retrieved.
Nov 23 12:37:39.395: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Nov 23 12:37:44.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6219'
Nov 23 12:37:44.556: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:37:44.556: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 23 12:37:44.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6219'
Nov 23 12:37:44.699: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:37:44.699: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 23 12:37:44.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6219'
Nov 23 12:37:44.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:37:44.864: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 23 12:37:44.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6219'
Nov 23 12:37:44.973: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:37:44.973: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 23 12:37:44.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6219'
Nov 23 12:37:45.164: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:37:45.164: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 23 12:37:45.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6219'
Nov 23 12:37:45.420: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:37:45.420: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:45.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6219" for this suite.

• [SLOW TEST:20.242 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":216,"skipped":3615,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:45.446: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 23 12:37:47.754: INFO: &Pod{ObjectMeta:{send-events-3b4d0728-7e4c-4201-b9f1-c8ff71e96989  events-2293 /api/v1/namespaces/events-2293/pods/send-events-3b4d0728-7e4c-4201-b9f1-c8ff71e96989 ce8c5fbd-101d-457f-9631-f5b16ec705ab 66949 0 2020-11-23 12:37:45 +0000 UTC <nil> <nil> map[name:foo time:724415145] map[cni.projectcalico.org/podIP:100.96.2.147/32 cni.projectcalico.org/podIPs:100.96.2.147/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-11-23 12:37:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:37:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:37:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lkbgm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lkbgm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lkbgm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:37:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:37:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:37:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:37:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.147,StartTime:2020-11-23 12:37:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:37:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://0af73aa068ada6350e0a229592497329cea67f0461ffc2ef74714df3c01d78cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 23 12:37:49.763: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 23 12:37:51.767: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:51.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2293" for this suite.

• [SLOW TEST:6.353 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":217,"skipped":3623,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:51.800: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 23 12:37:51.967: INFO: Waiting up to 5m0s for pod "pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13" in namespace "emptydir-1766" to be "Succeeded or Failed"
Nov 23 12:37:51.976: INFO: Pod "pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13": Phase="Pending", Reason="", readiness=false. Elapsed: 9.149339ms
Nov 23 12:37:53.980: INFO: Pod "pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012633037s
STEP: Saw pod success
Nov 23 12:37:53.980: INFO: Pod "pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13" satisfied condition "Succeeded or Failed"
Nov 23 12:37:53.982: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13 container test-container: <nil>
STEP: delete the pod
Nov 23 12:37:54.028: INFO: Waiting for pod pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13 to disappear
Nov 23 12:37:54.035: INFO: Pod pod-2f444250-a8ca-48b7-b30d-b7eed85d4d13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:37:54.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1766" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3623,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:37:54.046: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3766
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:37:54.186: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Creating first CR 
Nov 23 12:37:54.819: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-23T12:37:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-23T12:37:54Z]] name:name1 resourceVersion:67049 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:112e8d42-d357-43e5-8e51-6adc1ab210c6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 23 12:38:04.827: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-23T12:38:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-23T12:38:04Z]] name:name2 resourceVersion:67105 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:2d09b408-dbd2-4d00-ba49-e960049bd512] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 23 12:38:14.838: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-23T12:37:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-23T12:38:14Z]] name:name1 resourceVersion:67135 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:112e8d42-d357-43e5-8e51-6adc1ab210c6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 23 12:38:24.853: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-23T12:38:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-23T12:38:24Z]] name:name2 resourceVersion:67165 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:2d09b408-dbd2-4d00-ba49-e960049bd512] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 23 12:38:34.863: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-23T12:37:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-23T12:38:14Z]] name:name1 resourceVersion:67194 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:112e8d42-d357-43e5-8e51-6adc1ab210c6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 23 12:38:44.872: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-11-23T12:38:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-11-23T12:38:24Z]] name:name2 resourceVersion:67224 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:2d09b408-dbd2-4d00-ba49-e960049bd512] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:38:55.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3766" for this suite.

• [SLOW TEST:61.359 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":219,"skipped":3633,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:38:55.405: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Nov 23 12:38:55.726: INFO: Waiting up to 5m0s for pod "client-containers-06de90cb-7970-47d6-a600-f5e4175480b1" in namespace "containers-551" to be "Succeeded or Failed"
Nov 23 12:38:55.733: INFO: Pod "client-containers-06de90cb-7970-47d6-a600-f5e4175480b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.479075ms
Nov 23 12:38:57.736: INFO: Pod "client-containers-06de90cb-7970-47d6-a600-f5e4175480b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00952328s
Nov 23 12:38:59.740: INFO: Pod "client-containers-06de90cb-7970-47d6-a600-f5e4175480b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01330225s
STEP: Saw pod success
Nov 23 12:38:59.740: INFO: Pod "client-containers-06de90cb-7970-47d6-a600-f5e4175480b1" satisfied condition "Succeeded or Failed"
Nov 23 12:38:59.742: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod client-containers-06de90cb-7970-47d6-a600-f5e4175480b1 container test-container: <nil>
STEP: delete the pod
Nov 23 12:38:59.777: INFO: Waiting for pod client-containers-06de90cb-7970-47d6-a600-f5e4175480b1 to disappear
Nov 23 12:38:59.780: INFO: Pod client-containers-06de90cb-7970-47d6-a600-f5e4175480b1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:38:59.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-551" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:38:59.788: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:39:00.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:39:03.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:03.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3449" for this suite.
STEP: Destroying namespace "webhook-3449-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":221,"skipped":3655,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:03.886: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:04.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8363" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":222,"skipped":3657,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:04.097: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Nov 23 12:39:04.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-6071'
Nov 23 12:39:04.672: INFO: stderr: ""
Nov 23 12:39:04.672: INFO: stdout: "pod/pause created\n"
Nov 23 12:39:04.672: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 23 12:39:04.672: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6071" to be "running and ready"
Nov 23 12:39:04.681: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.589195ms
Nov 23 12:39:06.686: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.013987082s
Nov 23 12:39:06.686: INFO: Pod "pause" satisfied condition "running and ready"
Nov 23 12:39:06.686: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 23 12:39:06.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 label pods pause testing-label=testing-label-value --namespace=kubectl-6071'
Nov 23 12:39:06.810: INFO: stderr: ""
Nov 23 12:39:06.810: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 23 12:39:06.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pod pause -L testing-label --namespace=kubectl-6071'
Nov 23 12:39:06.942: INFO: stderr: ""
Nov 23 12:39:06.942: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 23 12:39:06.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 label pods pause testing-label- --namespace=kubectl-6071'
Nov 23 12:39:07.082: INFO: stderr: ""
Nov 23 12:39:07.082: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 23 12:39:07.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pod pause -L testing-label --namespace=kubectl-6071'
Nov 23 12:39:07.206: INFO: stderr: ""
Nov 23 12:39:07.206: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Nov 23 12:39:07.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete --grace-period=0 --force -f - --namespace=kubectl-6071'
Nov 23 12:39:07.337: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 23 12:39:07.337: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 23 12:39:07.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get rc,svc -l name=pause --no-headers --namespace=kubectl-6071'
Nov 23 12:39:07.459: INFO: stderr: "No resources found in kubectl-6071 namespace.\n"
Nov 23 12:39:07.459: INFO: stdout: ""
Nov 23 12:39:07.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pods -l name=pause --namespace=kubectl-6071 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 23 12:39:07.577: INFO: stderr: ""
Nov 23 12:39:07.577: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:07.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6071" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":223,"skipped":3666,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:07.594: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:18.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3347" for this suite.

• [SLOW TEST:11.225 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":224,"skipped":3673,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:18.819: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 23 12:39:21.508: INFO: Successfully updated pod "annotationupdatefcf79035-9666-418d-b946-71b6075b494e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:25.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-499" for this suite.

• [SLOW TEST:6.734 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":225,"skipped":3689,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:25.553: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:39:26.185: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:39:29.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
Nov 23 12:39:30.233: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:31.350: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:32.472: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:33.566: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:34.654: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:35.782: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:36.865: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:39:37.973: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:39.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9187" for this suite.
STEP: Destroying namespace "webhook-9187-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.662 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":226,"skipped":3692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:39.215: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:39:39.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 version'
Nov 23 12:39:39.476: INFO: stderr: ""
Nov 23 12:39:39.476: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T12:50:19Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020-10-14T12:41:49Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:39.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7215" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":227,"skipped":3717,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:39.490: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 23 12:39:45.702: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:45.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1123 12:39:45.702212      23 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1123 12:39:45.702246      23 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1123 12:39:45.702256      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7537" for this suite.

• [SLOW TEST:6.229 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":228,"skipped":3730,"failed":0}
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:45.719: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:39:47.918: INFO: Waiting up to 5m0s for pod "client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d" in namespace "pods-620" to be "Succeeded or Failed"
Nov 23 12:39:47.927: INFO: Pod "client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.890869ms
Nov 23 12:39:49.931: INFO: Pod "client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01299061s
STEP: Saw pod success
Nov 23 12:39:49.931: INFO: Pod "client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d" satisfied condition "Succeeded or Failed"
Nov 23 12:39:49.933: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d container env3cont: <nil>
STEP: delete the pod
Nov 23 12:39:49.955: INFO: Waiting for pod client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d to disappear
Nov 23 12:39:49.959: INFO: Pod client-envvars-4e7da55c-daa6-40ab-9b75-ce534a99ac5d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:49.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-620" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3730,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:49.968: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-d1c6d7b4-a674-40da-b2f2-2c836cba94d1-7408
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:50.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1425" for this suite.
STEP: Destroying namespace "nspatchtest-d1c6d7b4-a674-40da-b2f2-2c836cba94d1-7408" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":230,"skipped":3739,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:50.266: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:52.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4478" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":231,"skipped":3754,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:52.482: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Nov 23 12:39:52.633: INFO: namespace kubectl-3609
Nov 23 12:39:52.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f - --namespace=kubectl-3609'
Nov 23 12:39:52.879: INFO: stderr: ""
Nov 23 12:39:52.879: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 23 12:39:53.883: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 12:39:53.883: INFO: Found 0 / 1
Nov 23 12:39:54.888: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 12:39:54.888: INFO: Found 1 / 1
Nov 23 12:39:54.888: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 23 12:39:54.896: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 23 12:39:54.896: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 23 12:39:54.896: INFO: wait on agnhost-primary startup in kubectl-3609 
Nov 23 12:39:54.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs agnhost-primary-cckg2 agnhost-primary --namespace=kubectl-3609'
Nov 23 12:39:55.049: INFO: stderr: ""
Nov 23 12:39:55.049: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 23 12:39:55.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3609'
Nov 23 12:39:55.216: INFO: stderr: ""
Nov 23 12:39:55.216: INFO: stdout: "service/rm2 exposed\n"
Nov 23 12:39:55.220: INFO: Service rm2 in namespace kubectl-3609 found.
STEP: exposing service
Nov 23 12:39:57.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3609'
Nov 23 12:39:57.373: INFO: stderr: ""
Nov 23 12:39:57.373: INFO: stdout: "service/rm3 exposed\n"
Nov 23 12:39:57.377: INFO: Service rm3 in namespace kubectl-3609 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:39:59.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3609" for this suite.

• [SLOW TEST:6.910 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":232,"skipped":3765,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:39:59.392: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-631639cf-c3ab-49cc-887d-e3308055a78e in namespace container-probe-4619
Nov 23 12:40:01.546: INFO: Started pod liveness-631639cf-c3ab-49cc-887d-e3308055a78e in namespace container-probe-4619
STEP: checking the pod's current state and verifying that restartCount is present
Nov 23 12:40:01.552: INFO: Initial restart count of pod liveness-631639cf-c3ab-49cc-887d-e3308055a78e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:02.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4619" for this suite.

• [SLOW TEST:242.728 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":3767,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:02.121: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:18.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3951" for this suite.

• [SLOW TEST:16.309 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":234,"skipped":3784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:18.431: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Nov 23 12:44:21.141: INFO: Successfully updated pod "labelsupdatefafad526-6876-47bc-8bab-d3e96ffc4135"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:25.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7524" for this suite.

• [SLOW TEST:6.762 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3835,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:25.193: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:36.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1627" for this suite.

• [SLOW TEST:11.235 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":236,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:36.429: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Nov 23 12:44:38.131: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1123 12:44:38.131223      23 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1123 12:44:38.131248      23 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1123 12:44:38.131254      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:38.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4054" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":237,"skipped":3893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:38.141: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7844
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 23 12:44:38.317: INFO: Waiting up to 5m0s for pod "pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2" in namespace "emptydir-7844" to be "Succeeded or Failed"
Nov 23 12:44:38.326: INFO: Pod "pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.953619ms
Nov 23 12:44:40.330: INFO: Pod "pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012681515s
STEP: Saw pod success
Nov 23 12:44:40.330: INFO: Pod "pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2" satisfied condition "Succeeded or Failed"
Nov 23 12:44:40.332: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2 container test-container: <nil>
STEP: delete the pod
Nov 23 12:44:40.365: INFO: Waiting for pod pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2 to disappear
Nov 23 12:44:40.368: INFO: Pod pod-43f7588b-8fe5-42a7-9f18-3e7ebf9142a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:40.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7844" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":3923,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2937
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-022e9d41-37c5-40a8-9686-d9392a4046dd
STEP: Creating configMap with name cm-test-opt-upd-a3e6d876-ab0a-4f23-bcef-736e20335229
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-022e9d41-37c5-40a8-9686-d9392a4046dd
STEP: Updating configmap cm-test-opt-upd-a3e6d876-ab0a-4f23-bcef-736e20335229
STEP: Creating configMap with name cm-test-opt-create-8db0938b-31f6-4581-b1fe-1fa9a182dad9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:44.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2937" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":3935,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:44.671: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 23 12:44:44.875: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4047 /api/v1/namespaces/watch-4047/configmaps/e2e-watch-test-label-changed 1cc8eee4-f595-4049-ad75-855e933c0f77 69147 0 2020-11-23 12:44:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-23 12:44:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 12:44:44.875: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4047 /api/v1/namespaces/watch-4047/configmaps/e2e-watch-test-label-changed 1cc8eee4-f595-4049-ad75-855e933c0f77 69148 0 2020-11-23 12:44:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-23 12:44:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 12:44:44.875: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4047 /api/v1/namespaces/watch-4047/configmaps/e2e-watch-test-label-changed 1cc8eee4-f595-4049-ad75-855e933c0f77 69149 0 2020-11-23 12:44:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-23 12:44:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 23 12:44:54.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4047 /api/v1/namespaces/watch-4047/configmaps/e2e-watch-test-label-changed 1cc8eee4-f595-4049-ad75-855e933c0f77 69212 0 2020-11-23 12:44:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-23 12:44:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 12:44:54.906: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4047 /api/v1/namespaces/watch-4047/configmaps/e2e-watch-test-label-changed 1cc8eee4-f595-4049-ad75-855e933c0f77 69213 0 2020-11-23 12:44:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-23 12:44:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 23 12:44:54.906: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4047 /api/v1/namespaces/watch-4047/configmaps/e2e-watch-test-label-changed 1cc8eee4-f595-4049-ad75-855e933c0f77 69214 0 2020-11-23 12:44:44 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-11-23 12:44:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4047" for this suite.

• [SLOW TEST:10.244 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":240,"skipped":3950,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:54.915: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-da602380-d340-4f1b-bff6-3b49980c07f9
STEP: Creating a pod to test consume secrets
Nov 23 12:44:55.078: INFO: Waiting up to 5m0s for pod "pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34" in namespace "secrets-3201" to be "Succeeded or Failed"
Nov 23 12:44:55.081: INFO: Pod "pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.019518ms
Nov 23 12:44:57.087: INFO: Pod "pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008326618s
STEP: Saw pod success
Nov 23 12:44:57.087: INFO: Pod "pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34" satisfied condition "Succeeded or Failed"
Nov 23 12:44:57.090: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34 container secret-env-test: <nil>
STEP: delete the pod
Nov 23 12:44:57.132: INFO: Waiting for pod pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34 to disappear
Nov 23 12:44:57.140: INFO: Pod pod-secrets-94e24d6f-626d-4676-99d9-5e1237f8db34 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:57.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3201" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3953,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:57.154: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:44:57.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6470" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":242,"skipped":3972,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:44:57.358: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:44:57.520: INFO: Creating deployment "webserver-deployment"
Nov 23 12:44:57.524: INFO: Waiting for observed generation 1
Nov 23 12:44:59.534: INFO: Waiting for all required pods to come up
Nov 23 12:44:59.540: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 23 12:45:01.555: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 23 12:45:01.563: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 23 12:45:01.572: INFO: Updating deployment webserver-deployment
Nov 23 12:45:01.572: INFO: Waiting for observed generation 2
Nov 23 12:45:03.579: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 23 12:45:03.583: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 23 12:45:03.586: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 23 12:45:03.595: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 23 12:45:03.595: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 23 12:45:03.598: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 23 12:45:03.603: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 23 12:45:03.603: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 23 12:45:03.611: INFO: Updating deployment webserver-deployment
Nov 23 12:45:03.611: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 23 12:45:03.630: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 23 12:45:03.641: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 23 12:45:03.680: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/deployments/webserver-deployment 3d6c8c27-19f1-47da-a60d-5b257c0ea2eb 69500 3 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007311ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-11-23 12:45:01 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-23 12:45:03 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 23 12:45:03.727: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/replicasets/webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 69488 3 2020-11-23 12:45:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3d6c8c27-19f1-47da-a60d-5b257c0ea2eb 0xc003b10937 0xc003b10938}] []  [{kube-controller-manager Update apps/v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d6c8c27-19f1-47da-a60d-5b257c0ea2eb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b109b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:45:03.727: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 23 12:45:03.727: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-9066 /apis/apps/v1/namespaces/deployment-9066/replicasets/webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 69486 3 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3d6c8c27-19f1-47da-a60d-5b257c0ea2eb 0xc003b10a17 0xc003b10a18}] []  [{kube-controller-manager Update apps/v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d6c8c27-19f1-47da-a60d-5b257c0ea2eb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b10a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:45:03.769: INFO: Pod "webserver-deployment-795d758f88-clqqs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-clqqs webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-clqqs c80c314e-7f9c-42bb-afd1-fbb70a132f51 69501 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005167897 0xc005167898}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.769: INFO: Pod "webserver-deployment-795d758f88-cvxm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cvxm6 webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-cvxm6 50b77aa2-65e0-47bf-aa35-686f25e65d3b 69519 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc0051679c0 0xc0051679c1}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.770: INFO: Pod "webserver-deployment-795d758f88-dpqzp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dpqzp webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-dpqzp 3ff2e8c0-e260-4715-9335-46d968522666 69518 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005167af0 0xc005167af1}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:,StartTime:2020-11-23 12:45:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.770: INFO: Pod "webserver-deployment-795d758f88-f7lsq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f7lsq webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-f7lsq 3787498c-6419-4334-b203-ead6230139f7 69482 0 2020-11-23 12:45:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.2.175/32 cni.projectcalico.org/podIPs:100.96.2.175/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005167ca7 0xc005167ca8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-23 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:,StartTime:2020-11-23 12:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.770: INFO: Pod "webserver-deployment-795d758f88-n54m6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n54m6 webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-n54m6 2a5750f7-4e5b-4902-b3da-8d2984e54a2f 69472 0 2020-11-23 12:45:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.105/32 cni.projectcalico.org/podIPs:100.96.1.105/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005167ea7 0xc005167ea8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-23 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:,StartTime:2020-11-23 12:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.771: INFO: Pod "webserver-deployment-795d758f88-pwszp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pwszp webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-pwszp d1944f20-3641-42b6-bf70-8b1285e7c3eb 69523 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18057 0xc005c18058}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.772: INFO: Pod "webserver-deployment-795d758f88-qgt82" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qgt82 webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-qgt82 84eae959-2bc0-435a-b271-02ca0f40656b 69479 0 2020-11-23 12:45:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.106/32 cni.projectcalico.org/podIPs:100.96.1.106/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18187 0xc005c18188}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-23 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:,StartTime:2020-11-23 12:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.772: INFO: Pod "webserver-deployment-795d758f88-v7j2q" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v7j2q webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-v7j2q c8ca091b-0e1e-44ea-bf26-f18f0d055555 69528 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18337 0xc005c18338}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.774: INFO: Pod "webserver-deployment-795d758f88-v7wpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v7wpr webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-v7wpr 5b53d471-99a2-4dbe-81d6-5d70a001a652 69467 0 2020-11-23 12:45:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.2.173/32 cni.projectcalico.org/podIPs:100.96.2.173/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18480 0xc005c18481}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-23 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:,StartTime:2020-11-23 12:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.774: INFO: Pod "webserver-deployment-795d758f88-w5vf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w5vf7 webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-w5vf7 1b5e485a-9588-4da8-914f-6867571c5630 69508 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18637 0xc005c18638}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.775: INFO: Pod "webserver-deployment-795d758f88-x7fw7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x7fw7 webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-x7fw7 17bb0c66-fd51-4b60-8925-0c5349ae240d 69512 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18777 0xc005c18778}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.775: INFO: Pod "webserver-deployment-795d758f88-xn25s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xn25s webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-xn25s 876a20d3-b380-4fe5-9b82-a81eefd8dd24 69476 0 2020-11-23 12:45:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.2.174/32 cni.projectcalico.org/podIPs:100.96.2.174/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c188a7 0xc005c188a8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-11-23 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:,StartTime:2020-11-23 12:45:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.776: INFO: Pod "webserver-deployment-795d758f88-zgdlc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zgdlc webserver-deployment-795d758f88- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-795d758f88-zgdlc 62a16174-3cb5-4ffa-91a3-cf0c631fa40b 69511 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e65fa99f-da34-4515-a434-648c466c381d 0xc005c18a57 0xc005c18a58}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e65fa99f-da34-4515-a434-648c466c381d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.776: INFO: Pod "webserver-deployment-dd94f59b7-45nkb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-45nkb webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-45nkb 164ea295-893e-42cf-9685-82f5f97ac905 69382 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.2.170/32 cni.projectcalico.org/podIPs:100.96.2.170/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c18b87 0xc005c18b88}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.170,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f9327a5eef53587c2d790cb4ae76b87fe36708dd1a5e2b7d747f276334b37a7a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.776: INFO: Pod "webserver-deployment-dd94f59b7-9g879" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9g879 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-9g879 7f2275e6-68e7-42cc-bb44-d7427d884c74 69514 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c18d37 0xc005c18d38}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:,StartTime:2020-11-23 12:45:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.779: INFO: Pod "webserver-deployment-dd94f59b7-bvf8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bvf8z webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-bvf8z d945b3a3-c275-40ce-821d-64fe83022713 69517 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c18eb7 0xc005c18eb8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.779: INFO: Pod "webserver-deployment-dd94f59b7-f86c8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-f86c8 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-f86c8 6fe78c09-bb80-4216-9222-b0d43295da39 69351 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.104/32 cni.projectcalico.org/podIPs:100.96.1.104/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c18ff0 0xc005c18ff1}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:100.96.1.104,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0aec69c77f8d911cb4a02f69334ca35a2212e1a354ead4032b21fd0f92df320d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.780: INFO: Pod "webserver-deployment-dd94f59b7-fg28l" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fg28l webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-fg28l 7e57cfb9-ff80-42d0-a4bd-3286fbe355f6 69393 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.2.167/32 cni.projectcalico.org/podIPs:100.96.2.167/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c191d7 0xc005c191d8}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:45:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.167,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://67dab5cc7a23d860818274db5c7999313f40c722688c40066c49ebb7f5d1975f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.780: INFO: Pod "webserver-deployment-dd94f59b7-h6fx8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h6fx8 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-h6fx8 11a9272f-69eb-4a49-963f-c392d12f1f39 69509 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19387 0xc005c19388}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.780: INFO: Pod "webserver-deployment-dd94f59b7-h79t7" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h79t7 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-h79t7 b372222e-314b-4698-8e70-8cb6877ef783 69529 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19487 0xc005c19488}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.780: INFO: Pod "webserver-deployment-dd94f59b7-l9htv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l9htv webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-l9htv 467a62db-4bb2-4cc2-9a4f-386ba179aa10 69356 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.102/32 cni.projectcalico.org/podIPs:100.96.1.102/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c195c0 0xc005c195c1}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:100.96.1.102,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4e5dc21a248eaf3826fde380444f794be890a3e764f72a355f6820da349a3606,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.781: INFO: Pod "webserver-deployment-dd94f59b7-nkktz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nkktz webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-nkktz 1f2a71a4-de00-4dda-8eef-c78ce3651f89 69522 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19787 0xc005c19788}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.782: INFO: Pod "webserver-deployment-dd94f59b7-p2w6b" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p2w6b webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-p2w6b 0859b0e0-ec47-4e66-8fce-2a5679972ba2 69504 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19887 0xc005c19888}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.782: INFO: Pod "webserver-deployment-dd94f59b7-rhfx9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rhfx9 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-rhfx9 e12a64d8-72de-4147-9e16-b84fc61d21ca 69526 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c199a0 0xc005c199a1}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.782: INFO: Pod "webserver-deployment-dd94f59b7-rmv8k" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rmv8k webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-rmv8k 9a0b70ab-d2bd-440c-9d13-f567faa351dd 69521 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19aa7 0xc005c19aa8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.782: INFO: Pod "webserver-deployment-dd94f59b7-rtn9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rtn9w webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-rtn9w 82237261-dfe7-4b3b-a8d4-8c89f6348b4f 69524 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19ba7 0xc005c19ba8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.783: INFO: Pod "webserver-deployment-dd94f59b7-tfdkp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tfdkp webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-tfdkp 8b097c36-38b7-4691-bbb0-f4bac8b909f0 69527 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19cb7 0xc005c19cb8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.783: INFO: Pod "webserver-deployment-dd94f59b7-v9xd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v9xd6 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-v9xd6 60862be7-5174-4adb-8a41-876d7eb7117c 69515 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19dd0 0xc005c19dd1}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.783: INFO: Pod "webserver-deployment-dd94f59b7-w4nz8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w4nz8 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-w4nz8 b4b57f30-4543-4b4d-a8ae-a48127a11853 69371 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.2.168/32 cni.projectcalico.org/podIPs:100.96.2.168/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc005c19f10 0xc005c19f11}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.168,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9d1b83e22b1ca8018557a449af9d5c2eb0123910e8c07277c83cd3b5b2831378,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.783: INFO: Pod "webserver-deployment-dd94f59b7-w5zc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w5zc4 webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-w5zc4 68b3b0ef-8b08-401c-b93c-7a834947d977 69525 0 2020-11-23 12:45:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc003ba60e7 0xc003ba60e8}] []  [{kube-controller-manager Update v1 2020-11-23 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.783: INFO: Pod "webserver-deployment-dd94f59b7-x72zq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x72zq webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-x72zq 199f4417-2161-4487-aaca-27894b7ace6c 69379 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.101/32 cni.projectcalico.org/podIPs:100.96.1.101/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc003ba6217 0xc003ba6218}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:100.96.1.101,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d292161af351b519b6b0ff87ff60b30d61d96c6d03ec7113c5dd723a67ee9630,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.783: INFO: Pod "webserver-deployment-dd94f59b7-xqm5j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xqm5j webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-xqm5j 9e1dfaac-2b11-44b9-a1c0-f7c2ea7dcab2 69364 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.1.103/32 cni.projectcalico.org/podIPs:100.96.1.103/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc003ba6427 0xc003ba6428}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.34,PodIP:100.96.1.103,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c7ea08e020082752e8c8bb7df18ca5335c1f53686640c46923275d5331a83b53,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 23 12:45:03.784: INFO: Pod "webserver-deployment-dd94f59b7-xvb7m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xvb7m webserver-deployment-dd94f59b7- deployment-9066 /api/v1/namespaces/deployment-9066/pods/webserver-deployment-dd94f59b7-xvb7m 287d56f6-4ab8-41cc-8e96-5918282c8ada 69369 0 2020-11-23 12:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:100.96.2.169/32 cni.projectcalico.org/podIPs:100.96.2.169/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 d7cedf5e-e824-4b8e-a4d7-992c734bbd22 0xc003ba6617 0xc003ba6618}] []  [{kube-controller-manager Update v1 2020-11-23 12:44:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7cedf5e-e824-4b8e-a4d7-992c734bbd22\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-11-23 12:44:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-11-23 12:44:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-924zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-924zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-924zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:100.96.2.169,StartTime:2020-11-23 12:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-11-23 12:44:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/httpd:2.4.38-alpine,ImageID:docker-pullable://docker.io/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c3f8a2a304562b0bf3b4a76e864ef47cfaac5c1129d4d7049fef36e24b05ca04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9066" for this suite.

• [SLOW TEST:6.468 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":243,"skipped":3977,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:03.827: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1534
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 23 12:45:04.095: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 23 12:45:04.150: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:45:06.155: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:45:08.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:45:10.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:45:12.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:45:14.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:45:16.154: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 23 12:45:16.157: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 23 12:45:18.162: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 23 12:45:20.161: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 23 12:45:22.179: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.186:8080/dial?request=hostname&protocol=udp&host=100.96.2.183&port=8081&tries=1'] Namespace:pod-network-test-1534 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:45:22.179: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:45:22.289: INFO: Waiting for responses: map[]
Nov 23 12:45:22.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.2.186:8080/dial?request=hostname&protocol=udp&host=100.96.1.114&port=8081&tries=1'] Namespace:pod-network-test-1534 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:45:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:45:22.395: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:22.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1534" for this suite.

• [SLOW TEST:18.577 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":3981,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:22.404: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:45:22.544: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:24.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1629" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3987,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:24.588: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Nov 23 12:45:24.748: INFO: Waiting up to 5m0s for pod "client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403" in namespace "containers-5381" to be "Succeeded or Failed"
Nov 23 12:45:24.753: INFO: Pod "client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403": Phase="Pending", Reason="", readiness=false. Elapsed: 4.258365ms
Nov 23 12:45:26.759: INFO: Pod "client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010477418s
STEP: Saw pod success
Nov 23 12:45:26.759: INFO: Pod "client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403" satisfied condition "Succeeded or Failed"
Nov 23 12:45:26.763: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403 container test-container: <nil>
STEP: delete the pod
Nov 23 12:45:26.785: INFO: Waiting for pod client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403 to disappear
Nov 23 12:45:26.789: INFO: Pod client-containers-c3dadbc7-8875-4109-9c1e-fc0aab5d3403 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:26.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5381" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":246,"skipped":3988,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:26.800: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5521
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Nov 23 12:45:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:51.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5521" for this suite.

• [SLOW TEST:24.865 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":247,"skipped":3989,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:51.666: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3164
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-8452ba15-a691-4ca6-a62f-e7eb0b7af8d3
STEP: Creating secret with name s-test-opt-upd-5c2c2716-a038-4da1-92ca-f4f9fee3869d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8452ba15-a691-4ca6-a62f-e7eb0b7af8d3
STEP: Updating secret s-test-opt-upd-5c2c2716-a038-4da1-92ca-f4f9fee3869d
STEP: Creating secret with name s-test-opt-create-c941de8a-cf5d-4912-ae7a-3e717a6c6de9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:55.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3164" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":4009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:55.958: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:45:56.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4283" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":249,"skipped":4042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:45:56.140: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-2af878d5-e734-44fc-98f8-f0049974235c
STEP: Creating a pod to test consume secrets
Nov 23 12:45:56.321: INFO: Waiting up to 5m0s for pod "pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043" in namespace "secrets-3288" to be "Succeeded or Failed"
Nov 23 12:45:56.324: INFO: Pod "pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478277ms
Nov 23 12:45:58.327: INFO: Pod "pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043": Phase="Running", Reason="", readiness=true. Elapsed: 2.006449642s
Nov 23 12:46:00.331: INFO: Pod "pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010822935s
STEP: Saw pod success
Nov 23 12:46:00.331: INFO: Pod "pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043" satisfied condition "Succeeded or Failed"
Nov 23 12:46:00.335: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-1 pod pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043 container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:46:00.358: INFO: Waiting for pod pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043 to disappear
Nov 23 12:46:00.364: INFO: Pod pod-secrets-7c5a8308-b0a9-499f-93ec-a97b88c1a043 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:46:00.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3288" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":4081,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:46:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-t5bck in namespace proxy-5716
I1123 12:46:00.543344      23 runners.go:190] Created replication controller with name: proxy-service-t5bck, namespace: proxy-5716, replica count: 1
I1123 12:46:01.593694      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1123 12:46:02.593948      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:03.594617      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:04.594869      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:05.595108      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:06.595422      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:07.595840      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:08.596114      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1123 12:46:09.596372      23 runners.go:190] proxy-service-t5bck Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 12:46:09.600: INFO: setup took 9.086104038s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 23 12:46:09.618: INFO: (0) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 17.243439ms)
Nov 23 12:46:09.618: INFO: (0) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 17.108662ms)
Nov 23 12:46:09.618: INFO: (0) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 17.050509ms)
Nov 23 12:46:09.618: INFO: (0) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 17.621693ms)
Nov 23 12:46:09.621: INFO: (0) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 20.919789ms)
Nov 23 12:46:09.621: INFO: (0) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 20.798325ms)
Nov 23 12:46:09.621: INFO: (0) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 20.598407ms)
Nov 23 12:46:09.621: INFO: (0) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 20.652318ms)
Nov 23 12:46:09.621: INFO: (0) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 20.966394ms)
Nov 23 12:46:09.621: INFO: (0) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 20.532421ms)
Nov 23 12:46:09.623: INFO: (0) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 22.692858ms)
Nov 23 12:46:09.623: INFO: (0) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 22.766969ms)
Nov 23 12:46:09.623: INFO: (0) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 23.003861ms)
Nov 23 12:46:09.625: INFO: (0) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 24.965402ms)
Nov 23 12:46:09.626: INFO: (0) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 25.65168ms)
Nov 23 12:46:09.627: INFO: (0) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 25.712636ms)
Nov 23 12:46:09.644: INFO: (1) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 17.248477ms)
Nov 23 12:46:09.644: INFO: (1) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 17.24903ms)
Nov 23 12:46:09.644: INFO: (1) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 17.444703ms)
Nov 23 12:46:09.644: INFO: (1) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 17.565373ms)
Nov 23 12:46:09.645: INFO: (1) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 17.916567ms)
Nov 23 12:46:09.645: INFO: (1) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 17.590736ms)
Nov 23 12:46:09.645: INFO: (1) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 18.25964ms)
Nov 23 12:46:09.645: INFO: (1) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 18.059579ms)
Nov 23 12:46:09.648: INFO: (1) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 20.825887ms)
Nov 23 12:46:09.650: INFO: (1) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 22.687109ms)
Nov 23 12:46:09.650: INFO: (1) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 23.113255ms)
Nov 23 12:46:09.650: INFO: (1) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 22.399041ms)
Nov 23 12:46:09.650: INFO: (1) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 22.758592ms)
Nov 23 12:46:09.651: INFO: (1) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 23.507711ms)
Nov 23 12:46:09.651: INFO: (1) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 23.824557ms)
Nov 23 12:46:09.652: INFO: (1) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 24.897834ms)
Nov 23 12:46:09.658: INFO: (2) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 6.013072ms)
Nov 23 12:46:09.659: INFO: (2) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 6.570649ms)
Nov 23 12:46:09.661: INFO: (2) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 8.471298ms)
Nov 23 12:46:09.661: INFO: (2) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 8.881906ms)
Nov 23 12:46:09.661: INFO: (2) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 8.612971ms)
Nov 23 12:46:09.662: INFO: (2) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 9.158816ms)
Nov 23 12:46:09.662: INFO: (2) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 9.156156ms)
Nov 23 12:46:09.662: INFO: (2) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 8.928428ms)
Nov 23 12:46:09.664: INFO: (2) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 11.733554ms)
Nov 23 12:46:09.666: INFO: (2) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 13.640746ms)
Nov 23 12:46:09.667: INFO: (2) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 14.682088ms)
Nov 23 12:46:09.667: INFO: (2) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 14.225853ms)
Nov 23 12:46:09.667: INFO: (2) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 15.126577ms)
Nov 23 12:46:09.668: INFO: (2) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 14.478804ms)
Nov 23 12:46:09.668: INFO: (2) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 14.78883ms)
Nov 23 12:46:09.668: INFO: (2) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 15.300836ms)
Nov 23 12:46:09.675: INFO: (3) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 6.072199ms)
Nov 23 12:46:09.675: INFO: (3) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 6.253107ms)
Nov 23 12:46:09.681: INFO: (3) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 12.291275ms)
Nov 23 12:46:09.682: INFO: (3) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 13.537593ms)
Nov 23 12:46:09.682: INFO: (3) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 13.39239ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 16.468529ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 15.69709ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 15.919893ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 16.037247ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 16.219975ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 16.022154ms)
Nov 23 12:46:09.685: INFO: (3) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 16.11427ms)
Nov 23 12:46:09.688: INFO: (3) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 18.691349ms)
Nov 23 12:46:09.688: INFO: (3) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 18.927826ms)
Nov 23 12:46:09.688: INFO: (3) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 18.736895ms)
Nov 23 12:46:09.688: INFO: (3) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 19.253337ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 10.886969ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 11.089697ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 11.37199ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 11.771445ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 11.568109ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 12.026676ms)
Nov 23 12:46:09.700: INFO: (4) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 11.917405ms)
Nov 23 12:46:09.701: INFO: (4) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 11.706822ms)
Nov 23 12:46:09.701: INFO: (4) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 12.016615ms)
Nov 23 12:46:09.701: INFO: (4) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 12.730247ms)
Nov 23 12:46:09.702: INFO: (4) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 13.638342ms)
Nov 23 12:46:09.702: INFO: (4) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 13.717811ms)
Nov 23 12:46:09.704: INFO: (4) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 14.573197ms)
Nov 23 12:46:09.704: INFO: (4) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 15.270463ms)
Nov 23 12:46:09.704: INFO: (4) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 15.219607ms)
Nov 23 12:46:09.705: INFO: (4) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 15.450032ms)
Nov 23 12:46:09.715: INFO: (5) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 9.641063ms)
Nov 23 12:46:09.715: INFO: (5) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 9.85533ms)
Nov 23 12:46:09.715: INFO: (5) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 9.184892ms)
Nov 23 12:46:09.715: INFO: (5) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 9.668861ms)
Nov 23 12:46:09.715: INFO: (5) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 9.982844ms)
Nov 23 12:46:09.716: INFO: (5) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 10.988629ms)
Nov 23 12:46:09.717: INFO: (5) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 11.608942ms)
Nov 23 12:46:09.717: INFO: (5) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 11.871166ms)
Nov 23 12:46:09.717: INFO: (5) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 12.072223ms)
Nov 23 12:46:09.719: INFO: (5) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 13.968953ms)
Nov 23 12:46:09.720: INFO: (5) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 14.235356ms)
Nov 23 12:46:09.720: INFO: (5) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 14.533078ms)
Nov 23 12:46:09.720: INFO: (5) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 14.312029ms)
Nov 23 12:46:09.721: INFO: (5) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 15.382311ms)
Nov 23 12:46:09.721: INFO: (5) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 15.263706ms)
Nov 23 12:46:09.721: INFO: (5) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 15.517444ms)
Nov 23 12:46:09.730: INFO: (6) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 8.104794ms)
Nov 23 12:46:09.730: INFO: (6) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 8.304833ms)
Nov 23 12:46:09.730: INFO: (6) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 8.787893ms)
Nov 23 12:46:09.730: INFO: (6) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 8.479805ms)
Nov 23 12:46:09.731: INFO: (6) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 9.949765ms)
Nov 23 12:46:09.731: INFO: (6) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 9.469636ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 11.740302ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 12.583644ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 12.095594ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 11.929073ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 12.20709ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 11.885589ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 11.927935ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 12.061122ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 11.874041ms)
Nov 23 12:46:09.734: INFO: (6) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 12.043917ms)
Nov 23 12:46:09.742: INFO: (7) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 8.279316ms)
Nov 23 12:46:09.742: INFO: (7) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 8.19503ms)
Nov 23 12:46:09.746: INFO: (7) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 11.536434ms)
Nov 23 12:46:09.746: INFO: (7) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 11.412934ms)
Nov 23 12:46:09.746: INFO: (7) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 11.370623ms)
Nov 23 12:46:09.746: INFO: (7) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 12.016785ms)
Nov 23 12:46:09.746: INFO: (7) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 11.416205ms)
Nov 23 12:46:09.746: INFO: (7) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 11.819613ms)
Nov 23 12:46:09.747: INFO: (7) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 12.498048ms)
Nov 23 12:46:09.747: INFO: (7) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 12.841111ms)
Nov 23 12:46:09.747: INFO: (7) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 12.858325ms)
Nov 23 12:46:09.747: INFO: (7) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 13.523867ms)
Nov 23 12:46:09.748: INFO: (7) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 13.7064ms)
Nov 23 12:46:09.748: INFO: (7) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 13.926796ms)
Nov 23 12:46:09.749: INFO: (7) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 14.369379ms)
Nov 23 12:46:09.749: INFO: (7) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 14.334439ms)
Nov 23 12:46:09.760: INFO: (8) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 10.431165ms)
Nov 23 12:46:09.761: INFO: (8) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 10.597813ms)
Nov 23 12:46:09.761: INFO: (8) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 10.770342ms)
Nov 23 12:46:09.761: INFO: (8) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 11.544108ms)
Nov 23 12:46:09.761: INFO: (8) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 11.770597ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 14.40629ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 14.392045ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 14.685724ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 14.598978ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 14.570308ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 14.411101ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 14.507005ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 15.096606ms)
Nov 23 12:46:09.764: INFO: (8) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 14.657248ms)
Nov 23 12:46:09.765: INFO: (8) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 15.354848ms)
Nov 23 12:46:09.767: INFO: (8) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 18.364763ms)
Nov 23 12:46:09.778: INFO: (9) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 10.56376ms)
Nov 23 12:46:09.778: INFO: (9) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 10.23329ms)
Nov 23 12:46:09.778: INFO: (9) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 10.723529ms)
Nov 23 12:46:09.778: INFO: (9) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 10.608608ms)
Nov 23 12:46:09.779: INFO: (9) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 11.263082ms)
Nov 23 12:46:09.779: INFO: (9) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 11.210453ms)
Nov 23 12:46:09.784: INFO: (9) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 15.61481ms)
Nov 23 12:46:09.784: INFO: (9) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 16.00067ms)
Nov 23 12:46:09.784: INFO: (9) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 16.277142ms)
Nov 23 12:46:09.784: INFO: (9) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 16.327084ms)
Nov 23 12:46:09.785: INFO: (9) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 17.109266ms)
Nov 23 12:46:09.785: INFO: (9) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 16.832399ms)
Nov 23 12:46:09.786: INFO: (9) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 17.544621ms)
Nov 23 12:46:09.787: INFO: (9) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 19.018784ms)
Nov 23 12:46:09.787: INFO: (9) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 19.092468ms)
Nov 23 12:46:09.787: INFO: (9) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 19.203023ms)
Nov 23 12:46:09.799: INFO: (10) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 11.304574ms)
Nov 23 12:46:09.799: INFO: (10) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 11.117378ms)
Nov 23 12:46:09.800: INFO: (10) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 12.148389ms)
Nov 23 12:46:09.800: INFO: (10) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 12.149831ms)
Nov 23 12:46:09.800: INFO: (10) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 12.240739ms)
Nov 23 12:46:09.800: INFO: (10) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 12.145524ms)
Nov 23 12:46:09.801: INFO: (10) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 12.728818ms)
Nov 23 12:46:09.801: INFO: (10) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 12.649486ms)
Nov 23 12:46:09.801: INFO: (10) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 12.703022ms)
Nov 23 12:46:09.801: INFO: (10) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 12.948622ms)
Nov 23 12:46:09.804: INFO: (10) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 16.356022ms)
Nov 23 12:46:09.804: INFO: (10) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 16.616184ms)
Nov 23 12:46:09.805: INFO: (10) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 16.791535ms)
Nov 23 12:46:09.805: INFO: (10) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 17.554751ms)
Nov 23 12:46:09.805: INFO: (10) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 17.230941ms)
Nov 23 12:46:09.805: INFO: (10) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 17.025724ms)
Nov 23 12:46:09.810: INFO: (11) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 4.71239ms)
Nov 23 12:46:09.811: INFO: (11) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 5.7573ms)
Nov 23 12:46:09.811: INFO: (11) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 5.570555ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 14.373994ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 14.451119ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 14.660628ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 14.354955ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 14.887223ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 14.653888ms)
Nov 23 12:46:09.820: INFO: (11) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 14.637853ms)
Nov 23 12:46:09.826: INFO: (11) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 20.961995ms)
Nov 23 12:46:09.828: INFO: (11) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 22.739719ms)
Nov 23 12:46:09.828: INFO: (11) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 22.675453ms)
Nov 23 12:46:09.829: INFO: (11) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 22.924767ms)
Nov 23 12:46:09.830: INFO: (11) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 24.199843ms)
Nov 23 12:46:09.830: INFO: (11) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 24.197939ms)
Nov 23 12:46:09.844: INFO: (12) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 13.255162ms)
Nov 23 12:46:09.844: INFO: (12) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 13.568964ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 13.999969ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 14.778379ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 14.49741ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 14.570657ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 14.766279ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 14.478821ms)
Nov 23 12:46:09.845: INFO: (12) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 14.753815ms)
Nov 23 12:46:09.846: INFO: (12) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 15.105342ms)
Nov 23 12:46:09.846: INFO: (12) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 15.088945ms)
Nov 23 12:46:09.846: INFO: (12) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 15.784006ms)
Nov 23 12:46:09.849: INFO: (12) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 18.511095ms)
Nov 23 12:46:09.849: INFO: (12) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 18.67664ms)
Nov 23 12:46:09.850: INFO: (12) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 19.019089ms)
Nov 23 12:46:09.850: INFO: (12) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 18.917178ms)
Nov 23 12:46:09.864: INFO: (13) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 13.579807ms)
Nov 23 12:46:09.864: INFO: (13) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 14.649513ms)
Nov 23 12:46:09.868: INFO: (13) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 17.66489ms)
Nov 23 12:46:09.868: INFO: (13) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 17.646894ms)
Nov 23 12:46:09.869: INFO: (13) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 17.864217ms)
Nov 23 12:46:09.869: INFO: (13) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 18.558906ms)
Nov 23 12:46:09.869: INFO: (13) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 18.308588ms)
Nov 23 12:46:09.873: INFO: (13) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 22.072112ms)
Nov 23 12:46:09.873: INFO: (13) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 22.296527ms)
Nov 23 12:46:09.873: INFO: (13) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 22.395044ms)
Nov 23 12:46:09.874: INFO: (13) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 23.398348ms)
Nov 23 12:46:09.876: INFO: (13) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 25.945864ms)
Nov 23 12:46:09.876: INFO: (13) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 26.488301ms)
Nov 23 12:46:09.876: INFO: (13) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 25.517259ms)
Nov 23 12:46:09.877: INFO: (13) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 26.872089ms)
Nov 23 12:46:09.877: INFO: (13) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 26.395129ms)
Nov 23 12:46:09.897: INFO: (14) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 19.186052ms)
Nov 23 12:46:09.897: INFO: (14) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 18.914474ms)
Nov 23 12:46:09.897: INFO: (14) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 18.797973ms)
Nov 23 12:46:09.897: INFO: (14) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 18.989183ms)
Nov 23 12:46:09.903: INFO: (14) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 24.746659ms)
Nov 23 12:46:09.904: INFO: (14) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 26.108214ms)
Nov 23 12:46:09.904: INFO: (14) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 26.8324ms)
Nov 23 12:46:09.904: INFO: (14) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 26.490031ms)
Nov 23 12:46:09.904: INFO: (14) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 26.691258ms)
Nov 23 12:46:09.905: INFO: (14) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 27.061056ms)
Nov 23 12:46:09.919: INFO: (14) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 41.131879ms)
Nov 23 12:46:09.929: INFO: (14) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 50.924442ms)
Nov 23 12:46:09.929: INFO: (14) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 51.549828ms)
Nov 23 12:46:09.929: INFO: (14) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 51.160053ms)
Nov 23 12:46:09.929: INFO: (14) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 51.439792ms)
Nov 23 12:46:09.930: INFO: (14) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 51.838443ms)
Nov 23 12:46:09.950: INFO: (15) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 19.914764ms)
Nov 23 12:46:09.950: INFO: (15) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 20.27719ms)
Nov 23 12:46:09.951: INFO: (15) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 21.363959ms)
Nov 23 12:46:09.951: INFO: (15) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 21.281953ms)
Nov 23 12:46:09.951: INFO: (15) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 21.13217ms)
Nov 23 12:46:09.952: INFO: (15) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 21.358716ms)
Nov 23 12:46:09.952: INFO: (15) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 21.616733ms)
Nov 23 12:46:09.952: INFO: (15) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 22.332676ms)
Nov 23 12:46:09.952: INFO: (15) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 22.445424ms)
Nov 23 12:46:09.952: INFO: (15) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 22.205629ms)
Nov 23 12:46:09.959: INFO: (15) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 28.27972ms)
Nov 23 12:46:09.959: INFO: (15) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 28.779469ms)
Nov 23 12:46:09.959: INFO: (15) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 28.897603ms)
Nov 23 12:46:09.959: INFO: (15) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 28.772028ms)
Nov 23 12:46:09.960: INFO: (15) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 29.569289ms)
Nov 23 12:46:09.960: INFO: (15) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 29.613995ms)
Nov 23 12:46:09.975: INFO: (16) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 15.387694ms)
Nov 23 12:46:09.980: INFO: (16) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 19.725261ms)
Nov 23 12:46:09.983: INFO: (16) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 22.472393ms)
Nov 23 12:46:09.983: INFO: (16) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 22.641233ms)
Nov 23 12:46:09.983: INFO: (16) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 22.177742ms)
Nov 23 12:46:09.985: INFO: (16) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 23.446982ms)
Nov 23 12:46:09.985: INFO: (16) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 23.707997ms)
Nov 23 12:46:09.989: INFO: (16) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 27.924644ms)
Nov 23 12:46:09.989: INFO: (16) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 28.06792ms)
Nov 23 12:46:09.990: INFO: (16) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 29.39012ms)
Nov 23 12:46:09.991: INFO: (16) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 30.628268ms)
Nov 23 12:46:09.991: INFO: (16) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 30.155641ms)
Nov 23 12:46:09.991: INFO: (16) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 31.247441ms)
Nov 23 12:46:09.991: INFO: (16) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 29.98623ms)
Nov 23 12:46:09.991: INFO: (16) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 30.133889ms)
Nov 23 12:46:09.991: INFO: (16) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 30.356769ms)
Nov 23 12:46:10.001: INFO: (17) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 9.327576ms)
Nov 23 12:46:10.009: INFO: (17) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 16.902269ms)
Nov 23 12:46:10.009: INFO: (17) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 16.791387ms)
Nov 23 12:46:10.009: INFO: (17) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 17.060755ms)
Nov 23 12:46:10.009: INFO: (17) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 17.275722ms)
Nov 23 12:46:10.009: INFO: (17) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 17.113436ms)
Nov 23 12:46:10.010: INFO: (17) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 18.211088ms)
Nov 23 12:46:10.010: INFO: (17) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 18.08883ms)
Nov 23 12:46:10.010: INFO: (17) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 17.996639ms)
Nov 23 12:46:10.016: INFO: (17) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 24.502193ms)
Nov 23 12:46:10.018: INFO: (17) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 26.537251ms)
Nov 23 12:46:10.018: INFO: (17) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 26.551889ms)
Nov 23 12:46:10.018: INFO: (17) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 26.691257ms)
Nov 23 12:46:10.019: INFO: (17) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 26.969397ms)
Nov 23 12:46:10.019: INFO: (17) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 27.311551ms)
Nov 23 12:46:10.019: INFO: (17) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 27.268799ms)
Nov 23 12:46:10.025: INFO: (18) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 5.714807ms)
Nov 23 12:46:10.025: INFO: (18) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 5.983842ms)
Nov 23 12:46:10.027: INFO: (18) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 7.875582ms)
Nov 23 12:46:10.027: INFO: (18) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 7.689061ms)
Nov 23 12:46:10.027: INFO: (18) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 7.820745ms)
Nov 23 12:46:10.028: INFO: (18) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 8.203071ms)
Nov 23 12:46:10.036: INFO: (18) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 16.517585ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 23.607586ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 23.145825ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 23.767814ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 23.050104ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 23.6312ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 23.5908ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 23.019831ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 23.395703ms)
Nov 23 12:46:10.043: INFO: (18) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 23.804398ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:460/proxy/: tls baz (200; 18.130724ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 18.264921ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 19.049861ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname2/proxy/: tls qux (200; 18.880389ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:162/proxy/: bar (200; 18.296875ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname2/proxy/: bar (200; 18.714824ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">test<... (200; 18.58975ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname2/proxy/: bar (200; 18.235186ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:462/proxy/: tls qux (200; 18.471326ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/https:proxy-service-t5bck-sg5t2:443/proxy/tlsrewritem... (200; 18.895713ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:160/proxy/: foo (200; 18.581198ms)
Nov 23 12:46:10.062: INFO: (19) /api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/proxy-service-t5bck-sg5t2/proxy/rewriteme">test</a> (200; 18.278605ms)
Nov 23 12:46:10.065: INFO: (19) /api/v1/namespaces/proxy-5716/services/proxy-service-t5bck:portname1/proxy/: foo (200; 20.855083ms)
Nov 23 12:46:10.065: INFO: (19) /api/v1/namespaces/proxy-5716/services/https:proxy-service-t5bck:tlsportname1/proxy/: tls baz (200; 20.940463ms)
Nov 23 12:46:10.066: INFO: (19) /api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5716/pods/http:proxy-service-t5bck-sg5t2:1080/proxy/rewriteme">... (200; 21.501986ms)
Nov 23 12:46:10.066: INFO: (19) /api/v1/namespaces/proxy-5716/services/http:proxy-service-t5bck:portname1/proxy/: foo (200; 21.676658ms)
STEP: deleting ReplicationController proxy-service-t5bck in namespace proxy-5716, will wait for the garbage collector to delete the pods
Nov 23 12:46:10.134: INFO: Deleting ReplicationController proxy-service-t5bck took: 8.405479ms
Nov 23 12:46:10.735: INFO: Terminating ReplicationController proxy-service-t5bck pods took: 601.130559ms
[AfterEach] version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:46:20.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5716" for this suite.

• [SLOW TEST:20.474 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":251,"skipped":4097,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:46:20.849: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4847.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4847.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 12:46:25.034: INFO: DNS probes using dns-test-0e946504-6770-4089-87c5-3939ca6f4933 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4847.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4847.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 12:46:27.094: INFO: File wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:27.097: INFO: File jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:27.097: INFO: Lookups using dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 failed for: [wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local]

Nov 23 12:46:32.104: INFO: File wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:32.108: INFO: File jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:32.108: INFO: Lookups using dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 failed for: [wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local]

Nov 23 12:46:37.102: INFO: File wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:37.105: INFO: File jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:37.105: INFO: Lookups using dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 failed for: [wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local]

Nov 23 12:46:42.103: INFO: File wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:42.108: INFO: File jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:42.109: INFO: Lookups using dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 failed for: [wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local]

Nov 23 12:46:47.102: INFO: File wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:47.105: INFO: File jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:47.106: INFO: Lookups using dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 failed for: [wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local]

Nov 23 12:46:52.104: INFO: File wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:52.126: INFO: File jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local from pod  dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 23 12:46:52.126: INFO: Lookups using dns-4847/dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 failed for: [wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local]

Nov 23 12:46:57.106: INFO: DNS probes using dns-test-60f0029b-fb53-4dfd-9267-a8a5dd598413 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4847.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4847.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4847.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4847.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 12:46:59.183: INFO: DNS probes using dns-test-cfeba6e8-760b-4c1b-a665-6c463d2ce1a3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:46:59.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4847" for this suite.

• [SLOW TEST:38.399 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":252,"skipped":4114,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:46:59.249: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-c482dea0-055b-4e10-81f4-21bf5fd86a84
STEP: Creating a pod to test consume secrets
Nov 23 12:46:59.458: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a" in namespace "projected-8781" to be "Succeeded or Failed"
Nov 23 12:46:59.473: INFO: Pod "pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.36247ms
Nov 23 12:47:01.476: INFO: Pod "pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01760791s
STEP: Saw pod success
Nov 23 12:47:01.476: INFO: Pod "pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a" satisfied condition "Succeeded or Failed"
Nov 23 12:47:01.478: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:47:01.500: INFO: Waiting for pod pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a to disappear
Nov 23 12:47:01.505: INFO: Pod pod-projected-secrets-bc55019b-9b46-4c14-92ef-67aa5f596c4a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:47:01.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8781" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:47:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:47:01.677: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 23 12:47:01.687: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:01.690: INFO: Number of nodes with available pods: 0
Nov 23 12:47:01.690: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:47:02.696: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:02.699: INFO: Number of nodes with available pods: 0
Nov 23 12:47:02.699: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:47:03.695: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:03.698: INFO: Number of nodes with available pods: 2
Nov 23 12:47:03.698: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 23 12:47:03.722: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:03.722: INFO: Wrong image for pod: daemon-set-hssl2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:03.735: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:04.740: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:04.740: INFO: Wrong image for pod: daemon-set-hssl2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:04.746: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:05.741: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:05.741: INFO: Wrong image for pod: daemon-set-hssl2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:05.748: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:06.793: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:06.793: INFO: Wrong image for pod: daemon-set-hssl2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:06.793: INFO: Pod daemon-set-hssl2 is not available
Nov 23 12:47:06.800: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:07.742: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:07.742: INFO: Pod daemon-set-wf7fv is not available
Nov 23 12:47:07.746: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:08.738: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:08.742: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:09.739: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:09.739: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:09.742: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:10.739: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:10.739: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:10.744: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:11.740: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:11.740: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:11.744: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:12.739: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:12.739: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:12.743: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:13.739: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:13.739: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:13.742: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:14.740: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:14.740: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:14.744: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:15.739: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:15.739: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:15.745: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:16.740: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:16.740: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:16.744: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:17.740: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:17.740: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:17.745: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:18.738: INFO: Wrong image for pod: daemon-set-42qtb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Nov 23 12:47:18.738: INFO: Pod daemon-set-42qtb is not available
Nov 23 12:47:18.742: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:19.739: INFO: Pod daemon-set-g5fnz is not available
Nov 23 12:47:19.742: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 23 12:47:19.746: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:19.750: INFO: Number of nodes with available pods: 1
Nov 23 12:47:19.750: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:47:20.759: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:47:20.761: INFO: Number of nodes with available pods: 2
Nov 23 12:47:20.762: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4855, will wait for the garbage collector to delete the pods
Nov 23 12:47:20.834: INFO: Deleting DaemonSet.extensions daemon-set took: 5.119433ms
Nov 23 12:47:21.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.222979ms
Nov 23 12:47:29.038: INFO: Number of nodes with available pods: 0
Nov 23 12:47:29.038: INFO: Number of running nodes: 0, number of available pods: 0
Nov 23 12:47:29.040: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4855/daemonsets","resourceVersion":"70768"},"items":null}

Nov 23 12:47:29.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4855/pods","resourceVersion":"70768"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:47:29.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4855" for this suite.

• [SLOW TEST:27.547 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":254,"skipped":4180,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:47:29.060: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-da2ca926-2f37-47a6-a889-0807bd24d078 in namespace container-probe-2804
Nov 23 12:47:31.213: INFO: Started pod liveness-da2ca926-2f37-47a6-a889-0807bd24d078 in namespace container-probe-2804
STEP: checking the pod's current state and verifying that restartCount is present
Nov 23 12:47:31.215: INFO: Initial restart count of pod liveness-da2ca926-2f37-47a6-a889-0807bd24d078 is 0
Nov 23 12:47:53.275: INFO: Restart count of pod container-probe-2804/liveness-da2ca926-2f37-47a6-a889-0807bd24d078 is now 1 (22.059555091s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:47:53.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2804" for this suite.

• [SLOW TEST:24.276 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4181,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:47:53.336: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Nov 23 12:48:33.532: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 23 12:48:33.532: INFO: Deleting pod "simpletest.rc-2lz9q" in namespace "gc-9135"
W1123 12:48:33.532881      23 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1123 12:48:33.532915      23 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1123 12:48:33.532920      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Nov 23 12:48:33.543: INFO: Deleting pod "simpletest.rc-2m6xx" in namespace "gc-9135"
Nov 23 12:48:33.566: INFO: Deleting pod "simpletest.rc-6fcgx" in namespace "gc-9135"
Nov 23 12:48:33.593: INFO: Deleting pod "simpletest.rc-8lr2f" in namespace "gc-9135"
Nov 23 12:48:33.611: INFO: Deleting pod "simpletest.rc-b2lnp" in namespace "gc-9135"
Nov 23 12:48:33.636: INFO: Deleting pod "simpletest.rc-dz6lz" in namespace "gc-9135"
Nov 23 12:48:33.654: INFO: Deleting pod "simpletest.rc-ftgjf" in namespace "gc-9135"
Nov 23 12:48:33.678: INFO: Deleting pod "simpletest.rc-hj7b6" in namespace "gc-9135"
Nov 23 12:48:33.704: INFO: Deleting pod "simpletest.rc-k99wq" in namespace "gc-9135"
Nov 23 12:48:33.722: INFO: Deleting pod "simpletest.rc-zttds" in namespace "gc-9135"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:48:33.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9135" for this suite.

• [SLOW TEST:40.422 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":256,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:48:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Nov 23 12:48:33.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-769'
Nov 23 12:48:34.287: INFO: stderr: ""
Nov 23 12:48:34.287: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 23 12:48:34.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 get pod e2e-test-httpd-pod -o json --namespace=kubectl-769'
Nov 23 12:48:34.467: INFO: stderr: ""
Nov 23 12:48:34.467: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-11-23T12:48:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-23T12:48:34Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-11-23T12:48:34Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-769\",\n        \"resourceVersion\": \"71199\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-769/pods/e2e-test-httpd-pod\",\n        \"uid\": \"f7a0e9d8-99d4-4ed0-8e11-e565d3ab7c50\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-j2hts\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"dvi-conformance-1606121581-vsp1-group1-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-j2hts\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-j2hts\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:48:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:48:34Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:48:34Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-11-23T12:48:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.8.54\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-11-23T12:48:34Z\"\n    }\n}\n"
Nov 23 12:48:34.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 replace -f - --dry-run server --namespace=kubectl-769'
Nov 23 12:48:34.961: INFO: stderr: "W1123 12:48:34.561549    1525 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Nov 23 12:48:34.961: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Nov 23 12:48:34.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete pods e2e-test-httpd-pod --namespace=kubectl-769'
Nov 23 12:48:48.947: INFO: stderr: ""
Nov 23 12:48:48.947: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:48:48.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-769" for this suite.

• [SLOW TEST:15.202 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":257,"skipped":4219,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:48:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:48:50.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:48:53.117: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:48:53.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1332" for this suite.
STEP: Destroying namespace "webhook-1332-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":258,"skipped":4231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:48:53.231: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6790
STEP: Creating secret with name secret-test-403a0408-a878-4ed7-a5fc-e562a736407b
STEP: Creating a pod to test consume secrets
Nov 23 12:48:53.546: INFO: Waiting up to 5m0s for pod "pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f" in namespace "secrets-9327" to be "Succeeded or Failed"
Nov 23 12:48:53.559: INFO: Pod "pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.106675ms
Nov 23 12:48:55.563: INFO: Pod "pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017254549s
STEP: Saw pod success
Nov 23 12:48:55.563: INFO: Pod "pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f" satisfied condition "Succeeded or Failed"
Nov 23 12:48:55.567: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f container secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:48:55.597: INFO: Waiting for pod pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f to disappear
Nov 23 12:48:55.603: INFO: Pod pod-secrets-c14b4c1e-d846-40dd-8368-6b574860cf5f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:48:55.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9327" for this suite.
STEP: Destroying namespace "secret-namespace-6790" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4269,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:48:55.621: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:48:55.814: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:48:57.817: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:48:59.818: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:01.818: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:03.818: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:05.818: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:07.819: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:09.818: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:11.819: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:13.819: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:15.818: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = false)
Nov 23 12:49:17.819: INFO: The status of Pod test-webserver-70dbb71e-6b4e-405b-9b63-6e8e3697bca7 is Running (Ready = true)
Nov 23 12:49:17.823: INFO: Container started at 2020-11-23 12:48:56 +0000 UTC, pod became ready at 2020-11-23 12:49:17 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:49:17.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2881" for this suite.

• [SLOW TEST:22.210 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4280,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:49:17.832: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Nov 23 12:49:17.971: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:49:21.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9662" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":261,"skipped":4288,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:49:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3952
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3952
STEP: Creating statefulset with conflicting port in namespace statefulset-3952
STEP: Waiting until pod test-pod will start running in namespace statefulset-3952
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3952
Nov 23 12:49:26.073: INFO: Observed stateful pod in namespace: statefulset-3952, name: ss-0, uid: ab67232b-ecb7-46c9-8514-31318811c213, status phase: Pending. Waiting for statefulset controller to delete.
Nov 23 12:49:26.242: INFO: Observed stateful pod in namespace: statefulset-3952, name: ss-0, uid: ab67232b-ecb7-46c9-8514-31318811c213, status phase: Failed. Waiting for statefulset controller to delete.
Nov 23 12:49:26.252: INFO: Observed stateful pod in namespace: statefulset-3952, name: ss-0, uid: ab67232b-ecb7-46c9-8514-31318811c213, status phase: Failed. Waiting for statefulset controller to delete.
Nov 23 12:49:26.259: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3952
STEP: Removing pod with conflicting port in namespace statefulset-3952
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3952 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Nov 23 12:49:30.319: INFO: Deleting all statefulset in ns statefulset-3952
Nov 23 12:49:30.325: INFO: Scaling statefulset ss to 0
Nov 23 12:49:40.346: INFO: Waiting for statefulset status.replicas updated to 0
Nov 23 12:49:40.349: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:49:40.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3952" for this suite.

• [SLOW TEST:18.531 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":262,"skipped":4290,"failed":0}
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:49:40.381: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Nov 23 12:49:40.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-5336 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 23 12:49:40.645: INFO: stderr: ""
Nov 23 12:49:40.645: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Nov 23 12:49:40.645: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 23 12:49:40.645: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5336" to be "running and ready, or succeeded"
Nov 23 12:49:40.652: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079737ms
Nov 23 12:49:42.656: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010630552s
Nov 23 12:49:42.656: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 23 12:49:42.656: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 23 12:49:42.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs logs-generator logs-generator --namespace=kubectl-5336'
Nov 23 12:49:42.796: INFO: stderr: ""
Nov 23 12:49:42.796: INFO: stdout: "I1123 12:49:41.526144       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/xk5h 374\nI1123 12:49:41.726301       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/vlrk 201\nI1123 12:49:41.926296       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/pqm 257\nI1123 12:49:42.126303       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/7sd 515\nI1123 12:49:42.326314       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/gkd 216\nI1123 12:49:42.526269       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/2fj 527\nI1123 12:49:42.726255       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/p2lh 237\n"
STEP: limiting log lines
Nov 23 12:49:42.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs logs-generator logs-generator --namespace=kubectl-5336 --tail=1'
Nov 23 12:49:42.964: INFO: stderr: ""
Nov 23 12:49:42.964: INFO: stdout: "I1123 12:49:42.926344       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/cjt 256\n"
Nov 23 12:49:42.964: INFO: got output "I1123 12:49:42.926344       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/cjt 256\n"
STEP: limiting log bytes
Nov 23 12:49:42.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs logs-generator logs-generator --namespace=kubectl-5336 --limit-bytes=1'
Nov 23 12:49:43.102: INFO: stderr: ""
Nov 23 12:49:43.102: INFO: stdout: "I"
Nov 23 12:49:43.102: INFO: got output "I"
STEP: exposing timestamps
Nov 23 12:49:43.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs logs-generator logs-generator --namespace=kubectl-5336 --tail=1 --timestamps'
Nov 23 12:49:43.230: INFO: stderr: ""
Nov 23 12:49:43.230: INFO: stdout: "2020-11-23T12:49:43.126721780Z I1123 12:49:43.126451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/bdc 452\n"
Nov 23 12:49:43.230: INFO: got output "2020-11-23T12:49:43.126721780Z I1123 12:49:43.126451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/bdc 452\n"
STEP: restricting to a time range
Nov 23 12:49:45.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs logs-generator logs-generator --namespace=kubectl-5336 --since=1s'
Nov 23 12:49:45.886: INFO: stderr: ""
Nov 23 12:49:45.886: INFO: stdout: "I1123 12:49:44.926223       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/btp 497\nI1123 12:49:45.126262       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/tvjx 239\nI1123 12:49:45.326239       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/wqzd 249\nI1123 12:49:45.526315       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/f2l 467\nI1123 12:49:45.726277       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/p97 584\n"
Nov 23 12:49:45.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 logs logs-generator logs-generator --namespace=kubectl-5336 --since=24h'
Nov 23 12:49:46.030: INFO: stderr: ""
Nov 23 12:49:46.030: INFO: stdout: "I1123 12:49:41.526144       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/xk5h 374\nI1123 12:49:41.726301       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/vlrk 201\nI1123 12:49:41.926296       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/pqm 257\nI1123 12:49:42.126303       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/7sd 515\nI1123 12:49:42.326314       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/gkd 216\nI1123 12:49:42.526269       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/2fj 527\nI1123 12:49:42.726255       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/p2lh 237\nI1123 12:49:42.926344       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/cjt 256\nI1123 12:49:43.126451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/bdc 452\nI1123 12:49:43.326306       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/pqgs 461\nI1123 12:49:43.526287       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/rd6 408\nI1123 12:49:43.726444       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/5bkf 418\nI1123 12:49:43.926296       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/cjd 432\nI1123 12:49:44.126248       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/k4b 477\nI1123 12:49:44.326243       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/ntjc 512\nI1123 12:49:44.526328       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/tzx 296\nI1123 12:49:44.726198       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mkps 231\nI1123 12:49:44.926223       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/btp 497\nI1123 12:49:45.126262       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/tvjx 239\nI1123 12:49:45.326239       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/wqzd 249\nI1123 12:49:45.526315       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/f2l 467\nI1123 12:49:45.726277       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/p97 584\nI1123 12:49:45.926251       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/tdw4 383\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Nov 23 12:49:46.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete pod logs-generator --namespace=kubectl-5336'
Nov 23 12:49:58.945: INFO: stderr: ""
Nov 23 12:49:58.945: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:49:58.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5336" for this suite.

• [SLOW TEST:18.575 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":263,"skipped":4290,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:49:58.956: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-02febf72-10c9-4316-afe7-b44975792093 in namespace container-probe-9839
Nov 23 12:50:01.130: INFO: Started pod busybox-02febf72-10c9-4316-afe7-b44975792093 in namespace container-probe-9839
STEP: checking the pod's current state and verifying that restartCount is present
Nov 23 12:50:01.135: INFO: Initial restart count of pod busybox-02febf72-10c9-4316-afe7-b44975792093 is 0
Nov 23 12:50:55.267: INFO: Restart count of pod container-probe-9839/busybox-02febf72-10c9-4316-afe7-b44975792093 is now 1 (54.131906682s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:50:55.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9839" for this suite.

• [SLOW TEST:56.336 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4291,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:50:55.293: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Nov 23 12:50:55.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 create -f -'
Nov 23 12:50:55.675: INFO: stderr: ""
Nov 23 12:50:55.675: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 23 12:50:55.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 diff -f -'
Nov 23 12:50:56.079: INFO: rc: 1
Nov 23 12:50:56.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 delete -f -'
Nov 23 12:50:56.207: INFO: stderr: ""
Nov 23 12:50:56.207: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:50:56.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6159" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":265,"skipped":4298,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:50:56.224: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-69358bce-f3dd-4037-a3ac-53ac65d60334
STEP: Creating a pod to test consume secrets
Nov 23 12:50:56.385: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5" in namespace "projected-8930" to be "Succeeded or Failed"
Nov 23 12:50:56.390: INFO: Pod "pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.522956ms
Nov 23 12:50:58.394: INFO: Pod "pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009327356s
STEP: Saw pod success
Nov 23 12:50:58.394: INFO: Pod "pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5" satisfied condition "Succeeded or Failed"
Nov 23 12:50:58.396: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:50:58.417: INFO: Waiting for pod pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5 to disappear
Nov 23 12:50:58.423: INFO: Pod pod-projected-secrets-e565fe86-9006-4d8e-ab8a-606014cecef5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:50:58.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8930" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4304,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:50:58.432: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:51:58.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3552" for this suite.

• [SLOW TEST:60.189 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:51:58.622: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-3883
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3883 to expose endpoints map[]
Nov 23 12:51:58.783: INFO: successfully validated that service multi-endpoint-test in namespace services-3883 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3883
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3883 to expose endpoints map[pod1:[100]]
Nov 23 12:52:00.816: INFO: successfully validated that service multi-endpoint-test in namespace services-3883 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3883
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3883 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 23 12:52:03.847: INFO: successfully validated that service multi-endpoint-test in namespace services-3883 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-3883
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3883 to expose endpoints map[pod2:[101]]
Nov 23 12:52:03.875: INFO: successfully validated that service multi-endpoint-test in namespace services-3883 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3883
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3883 to expose endpoints map[]
Nov 23 12:52:03.902: INFO: successfully validated that service multi-endpoint-test in namespace services-3883 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:52:03.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3883" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.324 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":268,"skipped":4348,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:52:03.946: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 23 12:52:07.141: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:52:08.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1819" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":269,"skipped":4357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:52:08.172: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 23 12:52:12.378: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 23 12:52:12.383: INFO: Pod pod-with-prestop-http-hook still exists
Nov 23 12:52:14.383: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 23 12:52:14.391: INFO: Pod pod-with-prestop-http-hook still exists
Nov 23 12:52:16.383: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 23 12:52:16.387: INFO: Pod pod-with-prestop-http-hook still exists
Nov 23 12:52:18.383: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 23 12:52:18.387: INFO: Pod pod-with-prestop-http-hook still exists
Nov 23 12:52:20.384: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 23 12:52:20.388: INFO: Pod pod-with-prestop-http-hook still exists
Nov 23 12:52:22.383: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 23 12:52:22.387: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:52:22.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1576" for this suite.

• [SLOW TEST:14.231 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4383,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:52:22.403: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Nov 23 12:52:22.552: INFO: Waiting up to 5m0s for pod "var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c" in namespace "var-expansion-4096" to be "Succeeded or Failed"
Nov 23 12:52:22.558: INFO: Pod "var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574768ms
Nov 23 12:52:24.562: INFO: Pod "var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010143558s
STEP: Saw pod success
Nov 23 12:52:24.562: INFO: Pod "var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c" satisfied condition "Succeeded or Failed"
Nov 23 12:52:24.567: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c container dapi-container: <nil>
STEP: delete the pod
Nov 23 12:52:24.597: INFO: Waiting for pod var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c to disappear
Nov 23 12:52:24.602: INFO: Pod var-expansion-eee9ff26-fa6e-43a1-bdbf-26060453fb7c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:52:24.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4096" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4398,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:52:24.622: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7300
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:52:24.795: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:52:25.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7300" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":272,"skipped":4402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:52:25.840: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4585.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4585.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4585.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4585.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4585.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 153.31.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.31.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.31.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.31.153_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4585.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4585.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4585.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4585.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4585.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4585.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 153.31.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.31.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.31.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.31.153_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 23 12:52:28.092: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.098: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.102: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.107: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.129: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.133: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.137: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.140: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:28.158: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:52:33.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.169: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.182: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.228: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.232: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.237: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.241: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:33.281: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:52:38.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.169: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.174: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.215: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.220: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.231: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.235: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:38.256: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:52:43.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.179: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.187: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.229: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.237: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.242: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.247: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:43.282: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:52:48.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.171: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.208: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.211: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.214: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.217: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:48.235: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:52:53.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.168: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.172: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.204: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.212: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:53.235: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:52:58.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.171: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.174: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.203: INFO: Unable to read jessie_udp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.214: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.219: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local from pod dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d: the server could not find the requested resource (get pods dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d)
Nov 23 12:52:58.245: INFO: Lookups using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d failed for: [wheezy_udp@dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@dns-test-service.dns-4585.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_udp@dns-test-service.dns-4585.svc.cluster.local jessie_tcp@dns-test-service.dns-4585.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4585.svc.cluster.local]

Nov 23 12:53:03.243: INFO: DNS probes using dns-4585/dns-test-7a92a7f6-d615-4c39-8a37-69b95ebc044d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:03.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4585" for this suite.

• [SLOW TEST:37.753 seconds]
[sig-network] DNS
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":273,"skipped":4443,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:03.592: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-2568/configmap-test-04122f89-e010-436c-9c26-a2085ec9fee6
STEP: Creating a pod to test consume configMaps
Nov 23 12:53:03.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a" in namespace "configmap-2568" to be "Succeeded or Failed"
Nov 23 12:53:03.806: INFO: Pod "pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.13073ms
Nov 23 12:53:05.811: INFO: Pod "pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012052713s
STEP: Saw pod success
Nov 23 12:53:05.811: INFO: Pod "pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a" satisfied condition "Succeeded or Failed"
Nov 23 12:53:05.814: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a container env-test: <nil>
STEP: delete the pod
Nov 23 12:53:05.840: INFO: Waiting for pod pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a to disappear
Nov 23 12:53:05.843: INFO: Pod pod-configmaps-5e991569-a4d7-45df-b956-25d1c177231a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:05.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2568" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4446,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:05.854: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:53:06.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1" in namespace "downward-api-5327" to be "Succeeded or Failed"
Nov 23 12:53:06.012: INFO: Pod "downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.158801ms
Nov 23 12:53:08.016: INFO: Pod "downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009070223s
STEP: Saw pod success
Nov 23 12:53:08.016: INFO: Pod "downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1" satisfied condition "Succeeded or Failed"
Nov 23 12:53:08.018: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1 container client-container: <nil>
STEP: delete the pod
Nov 23 12:53:08.034: INFO: Waiting for pod downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1 to disappear
Nov 23 12:53:08.037: INFO: Pod downwardapi-volume-1f30c359-1f6f-4bd3-b9ea-fe32f0d069e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:08.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5327" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4465,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:08.046: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-4391/configmap-test-848adcdd-d0d1-4a0c-96c1-2846df181dd0
STEP: Creating a pod to test consume configMaps
Nov 23 12:53:08.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d" in namespace "configmap-4391" to be "Succeeded or Failed"
Nov 23 12:53:08.238: INFO: Pod "pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.291699ms
Nov 23 12:53:10.242: INFO: Pod "pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008380768s
STEP: Saw pod success
Nov 23 12:53:10.242: INFO: Pod "pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d" satisfied condition "Succeeded or Failed"
Nov 23 12:53:10.245: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d container env-test: <nil>
STEP: delete the pod
Nov 23 12:53:10.274: INFO: Waiting for pod pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d to disappear
Nov 23 12:53:10.281: INFO: Pod pod-configmaps-cee9b382-f68f-4907-bb61-78815583215d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:10.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4391" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4478,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:10.296: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:23.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9324" for this suite.

• [SLOW TEST:13.267 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":277,"skipped":4481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:23.564: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 23 12:53:23.736: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:23.741: INFO: Number of nodes with available pods: 0
Nov 23 12:53:23.741: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:53:24.753: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:24.758: INFO: Number of nodes with available pods: 0
Nov 23 12:53:24.758: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:53:25.746: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:25.750: INFO: Number of nodes with available pods: 2
Nov 23 12:53:25.750: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 23 12:53:25.767: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:25.773: INFO: Number of nodes with available pods: 1
Nov 23 12:53:25.773: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:26.779: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:26.782: INFO: Number of nodes with available pods: 1
Nov 23 12:53:26.782: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:27.778: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:27.782: INFO: Number of nodes with available pods: 1
Nov 23 12:53:27.782: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:28.778: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:28.781: INFO: Number of nodes with available pods: 1
Nov 23 12:53:28.781: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:29.779: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:29.782: INFO: Number of nodes with available pods: 1
Nov 23 12:53:29.782: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:30.778: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:30.781: INFO: Number of nodes with available pods: 1
Nov 23 12:53:30.781: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:31.779: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:31.782: INFO: Number of nodes with available pods: 1
Nov 23 12:53:31.782: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:53:32.778: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:53:32.782: INFO: Number of nodes with available pods: 2
Nov 23 12:53:32.782: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3628, will wait for the garbage collector to delete the pods
Nov 23 12:53:32.846: INFO: Deleting DaemonSet.extensions daemon-set took: 8.431715ms
Nov 23 12:53:33.446: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.333349ms
Nov 23 12:53:40.850: INFO: Number of nodes with available pods: 0
Nov 23 12:53:40.850: INFO: Number of running nodes: 0, number of available pods: 0
Nov 23 12:53:40.853: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3628/daemonsets","resourceVersion":"73103"},"items":null}

Nov 23 12:53:40.855: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3628/pods","resourceVersion":"73103"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:40.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3628" for this suite.

• [SLOW TEST:17.307 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":278,"skipped":4512,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:40.871: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1189
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1189
I1123 12:53:41.044645      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1189, replica count: 2
Nov 23 12:53:44.095: INFO: Creating new exec pod
I1123 12:53:44.095224      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 12:53:47.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1189 execpodtl8rh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 23 12:53:49.346: INFO: rc: 1
Nov 23 12:53:49.346: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1189 execpodtl8rh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:53:50.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1189 execpodtl8rh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 23 12:53:52.559: INFO: rc: 1
Nov 23 12:53:52.559: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1189 execpodtl8rh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:53:53.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1189 execpodtl8rh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 23 12:53:53.581: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 23 12:53:53.581: INFO: stdout: ""
Nov 23 12:53:53.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-1189 execpodtl8rh -- /bin/sh -x -c nc -zv -t -w 2 100.66.178.219 80'
Nov 23 12:53:53.814: INFO: stderr: "+ nc -zv -t -w 2 100.66.178.219 80\nConnection to 100.66.178.219 80 port [tcp/http] succeeded!\n"
Nov 23 12:53:53.814: INFO: stdout: ""
Nov 23 12:53:53.814: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:53.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1189" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.015 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":279,"skipped":4517,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:53.886: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:53:54.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0" in namespace "downward-api-274" to be "Succeeded or Failed"
Nov 23 12:53:54.062: INFO: Pod "downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.165874ms
Nov 23 12:53:56.066: INFO: Pod "downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.009329138s
Nov 23 12:53:58.070: INFO: Pod "downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012678732s
STEP: Saw pod success
Nov 23 12:53:58.070: INFO: Pod "downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0" satisfied condition "Succeeded or Failed"
Nov 23 12:53:58.072: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0 container client-container: <nil>
STEP: delete the pod
Nov 23 12:53:58.093: INFO: Waiting for pod downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0 to disappear
Nov 23 12:53:58.100: INFO: Pod downwardapi-volume-b9cd1b0b-e27f-43d9-9958-42eac3a81fd0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:53:58.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-274" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4521,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:53:58.113: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-cc261306-828d-49ba-acd6-1b05e530ca25
STEP: Creating a pod to test consume secrets
Nov 23 12:53:58.280: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c" in namespace "projected-4301" to be "Succeeded or Failed"
Nov 23 12:53:58.290: INFO: Pod "pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.929058ms
Nov 23 12:54:00.294: INFO: Pod "pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c": Phase="Running", Reason="", readiness=true. Elapsed: 2.014099885s
Nov 23 12:54:02.306: INFO: Pod "pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026112912s
STEP: Saw pod success
Nov 23 12:54:02.306: INFO: Pod "pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c" satisfied condition "Succeeded or Failed"
Nov 23 12:54:02.309: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 23 12:54:02.332: INFO: Waiting for pod pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c to disappear
Nov 23 12:54:02.336: INFO: Pod pod-projected-secrets-a81c538e-4bac-4cd7-8f4b-507abb32910c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:02.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4301" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":281,"skipped":4539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:02.351: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Nov 23 12:54:03.044: INFO: created pod pod-service-account-defaultsa
Nov 23 12:54:03.044: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 23 12:54:03.058: INFO: created pod pod-service-account-mountsa
Nov 23 12:54:03.058: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 23 12:54:03.069: INFO: created pod pod-service-account-nomountsa
Nov 23 12:54:03.069: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 23 12:54:03.081: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 23 12:54:03.081: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 23 12:54:03.107: INFO: created pod pod-service-account-mountsa-mountspec
Nov 23 12:54:03.107: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 23 12:54:03.124: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 23 12:54:03.124: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 23 12:54:03.133: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 23 12:54:03.133: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 23 12:54:03.147: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 23 12:54:03.147: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 23 12:54:03.173: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 23 12:54:03.173: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:03.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7062" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":282,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3851
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:54:03.447: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 23 12:54:08.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3851 create -f -'
Nov 23 12:54:09.252: INFO: stderr: ""
Nov 23 12:54:09.252: INFO: stdout: "e2e-test-crd-publish-openapi-6926-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 23 12:54:09.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3851 delete e2e-test-crd-publish-openapi-6926-crds test-cr'
Nov 23 12:54:09.416: INFO: stderr: ""
Nov 23 12:54:09.416: INFO: stdout: "e2e-test-crd-publish-openapi-6926-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 23 12:54:09.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3851 apply -f -'
Nov 23 12:54:09.752: INFO: stderr: ""
Nov 23 12:54:09.752: INFO: stdout: "e2e-test-crd-publish-openapi-6926-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 23 12:54:09.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3851 delete e2e-test-crd-publish-openapi-6926-crds test-cr'
Nov 23 12:54:09.936: INFO: stderr: ""
Nov 23 12:54:09.936: INFO: stdout: "e2e-test-crd-publish-openapi-6926-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 23 12:54:09.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-6926-crds'
Nov 23 12:54:10.264: INFO: stderr: ""
Nov 23 12:54:10.264: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6926-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:14.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3851" for this suite.

• [SLOW TEST:11.623 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":283,"skipped":4628,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:14.856: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:54:15.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:54:18.436: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Nov 23 12:54:19.478: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:20.598: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:21.719: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:22.811: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:23.898: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:25.017: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:26.102: INFO: Waiting for webhook configuration to be ready...
Nov 23 12:54:27.190: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:28.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-716" for this suite.
STEP: Destroying namespace "webhook-716-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.561 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":284,"skipped":4628,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:28.417: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 23 12:54:31.600: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4029" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4639,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:31.628: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1443
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:54:31.768: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:32.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1443" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":286,"skipped":4644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:33.122: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-8102
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8102 to expose endpoints map[]
Nov 23 12:54:33.396: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 23 12:54:34.405: INFO: successfully validated that service endpoint-test2 in namespace services-8102 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8102
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8102 to expose endpoints map[pod1:[80]]
Nov 23 12:54:36.512: INFO: successfully validated that service endpoint-test2 in namespace services-8102 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-8102
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8102 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 23 12:54:38.557: INFO: successfully validated that service endpoint-test2 in namespace services-8102 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-8102
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8102 to expose endpoints map[pod2:[80]]
Nov 23 12:54:38.587: INFO: successfully validated that service endpoint-test2 in namespace services-8102 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-8102
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8102 to expose endpoints map[]
Nov 23 12:54:39.608: INFO: successfully validated that service endpoint-test2 in namespace services-8102 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:39.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8102" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.544 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":287,"skipped":4672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:39.666: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7732
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 23 12:54:39.810: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 23 12:54:39.848: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:54:41.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:54:43.852: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:54:45.852: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:54:47.851: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:54:49.853: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:54:51.852: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:54:53.851: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 23 12:54:53.856: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 23 12:54:55.890: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.2.235:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7732 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:54:55.891: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:54:55.991: INFO: Found all expected endpoints: [netserver-0]
Nov 23 12:54:55.994: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.135:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7732 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:54:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:54:56.096: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:54:56.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7732" for this suite.

• [SLOW TEST:16.446 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":288,"skipped":4694,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:54:56.112: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 23 12:54:56.334: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:54:56.337: INFO: Number of nodes with available pods: 0
Nov 23 12:54:56.337: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:54:57.347: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:54:57.351: INFO: Number of nodes with available pods: 0
Nov 23 12:54:57.351: INFO: Node dvi-conformance-1606121581-vsp1-group1-0 is running more than one daemon pod
Nov 23 12:54:58.344: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:54:58.348: INFO: Number of nodes with available pods: 2
Nov 23 12:54:58.348: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 23 12:54:58.367: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:54:58.379: INFO: Number of nodes with available pods: 1
Nov 23 12:54:58.379: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:54:59.384: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:54:59.387: INFO: Number of nodes with available pods: 1
Nov 23 12:54:59.388: INFO: Node dvi-conformance-1606121581-vsp1-group1-1 is running more than one daemon pod
Nov 23 12:55:00.387: INFO: DaemonSet pods can't tolerate node dvi-conformance-1606121581-vsp1-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 23 12:55:00.390: INFO: Number of nodes with available pods: 2
Nov 23 12:55:00.390: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4332, will wait for the garbage collector to delete the pods
Nov 23 12:55:00.456: INFO: Deleting DaemonSet.extensions daemon-set took: 8.047752ms
Nov 23 12:55:00.556: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.268817ms
Nov 23 12:55:10.859: INFO: Number of nodes with available pods: 0
Nov 23 12:55:10.859: INFO: Number of running nodes: 0, number of available pods: 0
Nov 23 12:55:10.861: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4332/daemonsets","resourceVersion":"74012"},"items":null}

Nov 23 12:55:10.864: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4332/pods","resourceVersion":"74012"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:55:10.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4332" for this suite.

• [SLOW TEST:14.770 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":289,"skipped":4697,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:55:10.882: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Nov 23 12:55:11.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7" in namespace "projected-8671" to be "Succeeded or Failed"
Nov 23 12:55:11.044: INFO: Pod "downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.082576ms
Nov 23 12:55:13.048: INFO: Pod "downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015026917s
STEP: Saw pod success
Nov 23 12:55:13.048: INFO: Pod "downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7" satisfied condition "Succeeded or Failed"
Nov 23 12:55:13.051: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7 container client-container: <nil>
STEP: delete the pod
Nov 23 12:55:13.075: INFO: Waiting for pod downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7 to disappear
Nov 23 12:55:13.079: INFO: Pod downwardapi-volume-3b660d68-2487-4a47-8e26-460d9e2c81d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:55:13.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8671" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:55:13.094: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Nov 23 12:55:23.284: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:55:23.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1123 12:55:23.284544      23 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1123 12:55:23.284579      23 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1123 12:55:23.284588      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-9978" for this suite.

• [SLOW TEST:10.205 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":291,"skipped":4740,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:55:23.299: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2092
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 23 12:55:23.460: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 23 12:55:40.046: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:55:44.391: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:00.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2092" for this suite.

• [SLOW TEST:37.100 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":292,"skipped":4758,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:00.399: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 23 12:56:00.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 23 12:56:02.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741732960, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741732960, loc:(*time.Location)(0x770e880)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63741732960, loc:(*time.Location)(0x770e880)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63741732960, loc:(*time.Location)(0x770e880)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 23 12:56:05.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:06.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6801" for this suite.
STEP: Destroying namespace "webhook-6801-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.729 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":293,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:06.129: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-9427/secret-test-47db0876-8bd1-4f03-8303-5f8ea7639739
STEP: Creating a pod to test consume secrets
Nov 23 12:56:06.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c" in namespace "secrets-9427" to be "Succeeded or Failed"
Nov 23 12:56:06.291: INFO: Pod "pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166756ms
Nov 23 12:56:08.295: INFO: Pod "pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00730042s
STEP: Saw pod success
Nov 23 12:56:08.295: INFO: Pod "pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c" satisfied condition "Succeeded or Failed"
Nov 23 12:56:08.303: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c container env-test: <nil>
STEP: delete the pod
Nov 23 12:56:08.321: INFO: Waiting for pod pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c to disappear
Nov 23 12:56:08.326: INFO: Pod pod-configmaps-2162f536-cd77-4764-8e00-60578e979c6c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:08.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9427" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":294,"skipped":4784,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:08.334: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Nov 23 12:56:08.485: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-710016297 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:08.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8909" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":295,"skipped":4803,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:08.600: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:56:08.750: INFO: Creating deployment "test-recreate-deployment"
Nov 23 12:56:08.754: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 23 12:56:08.764: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 23 12:56:10.770: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 23 12:56:10.773: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 23 12:56:10.784: INFO: Updating deployment test-recreate-deployment
Nov 23 12:56:10.784: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Nov 23 12:56:10.898: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5337 /apis/apps/v1/namespaces/deployment-5337/deployments/test-recreate-deployment 5b164dc4-c53c-4569-9388-c4d2f4b538c0 74474 2 2020-11-23 12:56:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-11-23 12:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-11-23 12:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049b8418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-11-23 12:56:10 +0000 UTC,LastTransitionTime:2020-11-23 12:56:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-11-23 12:56:10 +0000 UTC,LastTransitionTime:2020-11-23 12:56:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 23 12:56:10.901: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5337 /apis/apps/v1/namespaces/deployment-5337/replicasets/test-recreate-deployment-f79dd4667 8812363d-177c-4b12-83bb-363a6b0423c9 74470 1 2020-11-23 12:56:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5b164dc4-c53c-4569-9388-c4d2f4b538c0 0xc0049778e0 0xc0049778e1}] []  [{kube-controller-manager Update apps/v1 2020-11-23 12:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b164dc4-c53c-4569-9388-c4d2f4b538c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049779a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:56:10.901: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 23 12:56:10.901: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-5337 /apis/apps/v1/namespaces/deployment-5337/replicasets/test-recreate-deployment-c96cf48f 86ac44d7-bfda-436a-8675-d3006b3747af 74462 2 2020-11-23 12:56:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5b164dc4-c53c-4569-9388-c4d2f4b538c0 0xc00497772f 0xc004977750}] []  [{kube-controller-manager Update apps/v1 2020-11-23 12:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b164dc4-c53c-4569-9388-c4d2f4b538c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004977808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 23 12:56:10.905: INFO: Pod "test-recreate-deployment-f79dd4667-ksb8z" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-ksb8z test-recreate-deployment-f79dd4667- deployment-5337 /api/v1/namespaces/deployment-5337/pods/test-recreate-deployment-f79dd4667-ksb8z cb4ecf4d-c055-433f-9954-78137c4b3f01 74475 0 2020-11-23 12:56:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 8812363d-177c-4b12-83bb-363a6b0423c9 0xc0049e60a0 0xc0049e60a1}] []  [{kube-controller-manager Update v1 2020-11-23 12:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8812363d-177c-4b12-83bb-363a6b0423c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-11-23 12:56:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jgnnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jgnnh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jgnnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:dvi-conformance-1606121581-vsp1-group1-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:56:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:56:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:56:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-11-23 12:56:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.54,PodIP:,StartTime:2020-11-23 12:56:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:10.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5337" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":296,"skipped":4816,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:10.915: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-c5209956-1399-4aa8-acd6-468edfecb9ac
STEP: Creating a pod to test consume configMaps
Nov 23 12:56:11.088: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65" in namespace "configmap-5394" to be "Succeeded or Failed"
Nov 23 12:56:11.108: INFO: Pod "pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65": Phase="Pending", Reason="", readiness=false. Elapsed: 20.217322ms
Nov 23 12:56:13.112: INFO: Pod "pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65": Phase="Running", Reason="", readiness=true. Elapsed: 2.024198986s
Nov 23 12:56:15.116: INFO: Pod "pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028391158s
STEP: Saw pod success
Nov 23 12:56:15.116: INFO: Pod "pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65" satisfied condition "Succeeded or Failed"
Nov 23 12:56:15.120: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:56:15.150: INFO: Waiting for pod pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65 to disappear
Nov 23 12:56:15.154: INFO: Pod pod-configmaps-2ca28825-5397-45c2-beee-3808f89a3a65 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:15.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5394" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4833,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:15.169: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 23 12:56:15.350: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:28.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9124" for this suite.

• [SLOW TEST:13.806 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":4840,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:28.975: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Nov 23 12:56:29.808: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1123 12:56:29.808321      23 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1123 12:56:29.808356      23 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1123 12:56:29.808362      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:29.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1176" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":299,"skipped":4857,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:29.827: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8556
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 23 12:56:29.987: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 23 12:56:30.041: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 23 12:56:32.046: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:56:34.045: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:56:36.045: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:56:38.045: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:56:40.047: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:56:42.048: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 23 12:56:44.045: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 23 12:56:44.051: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Nov 23 12:56:46.084: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.2.247 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8556 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:56:46.084: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:56:47.170: INFO: Found all expected endpoints: [netserver-0]
Nov 23 12:56:47.173: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8556 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 23 12:56:47.173: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
Nov 23 12:56:48.261: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:56:48.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8556" for this suite.

• [SLOW TEST:18.447 seconds]
[sig-network] Networking
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4869,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:56:48.274: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6511
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6511
I1123 12:56:48.460497      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6511, replica count: 2
Nov 23 12:56:51.510: INFO: Creating new exec pod
I1123 12:56:51.510907      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 23 12:56:54.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 23 12:56:56.823: INFO: rc: 1
Nov 23 12:56:56.823: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:56:57.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 23 12:57:00.045: INFO: rc: 1
Nov 23 12:57:00.045: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Nov 23 12:57:00.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Nov 23 12:57:01.052: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 23 12:57:01.052: INFO: stdout: ""
Nov 23 12:57:01.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 100.66.209.15 80'
Nov 23 12:57:01.297: INFO: stderr: "+ nc -zv -t -w 2 100.66.209.15 80\nConnection to 100.66.209.15 80 port [tcp/http] succeeded!\n"
Nov 23 12:57:01.297: INFO: stdout: ""
Nov 23 12:57:01.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 31655'
Nov 23 12:57:01.521: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 31655\nConnection to 192.168.8.54 31655 port [tcp/31655] succeeded!\n"
Nov 23 12:57:01.522: INFO: stdout: ""
Nov 23 12:57:01.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 31655'
Nov 23 12:57:01.785: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 31655\nConnection to 192.168.8.34 31655 port [tcp/31655] succeeded!\n"
Nov 23 12:57:01.785: INFO: stdout: ""
Nov 23 12:57:01.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.54 31655'
Nov 23 12:57:02.018: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.54 31655\nConnection to 192.168.8.54 31655 port [tcp/31655] succeeded!\n"
Nov 23 12:57:02.018: INFO: stdout: ""
Nov 23 12:57:02.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 exec --namespace=services-6511 execpod6jc4h -- /bin/sh -x -c nc -zv -t -w 2 192.168.8.34 31655'
Nov 23 12:57:02.289: INFO: stderr: "+ nc -zv -t -w 2 192.168.8.34 31655\nConnection to 192.168.8.34 31655 port [tcp/31655] succeeded!\n"
Nov 23 12:57:02.289: INFO: stdout: ""
Nov 23 12:57:02.289: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:57:02.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6511" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.071 seconds]
[sig-network] Services
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":301,"skipped":4876,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:57:02.345: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3479
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Nov 23 12:57:02.496: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 23 12:57:06.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3479 create -f -'
Nov 23 12:57:07.380: INFO: stderr: ""
Nov 23 12:57:07.380: INFO: stdout: "e2e-test-crd-publish-openapi-3937-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 23 12:57:07.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3479 delete e2e-test-crd-publish-openapi-3937-crds test-cr'
Nov 23 12:57:07.528: INFO: stderr: ""
Nov 23 12:57:07.529: INFO: stdout: "e2e-test-crd-publish-openapi-3937-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 23 12:57:07.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3479 apply -f -'
Nov 23 12:57:07.881: INFO: stderr: ""
Nov 23 12:57:07.881: INFO: stdout: "e2e-test-crd-publish-openapi-3937-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 23 12:57:07.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 --namespace=crd-publish-openapi-3479 delete e2e-test-crd-publish-openapi-3937-crds test-cr'
Nov 23 12:57:08.007: INFO: stderr: ""
Nov 23 12:57:08.007: INFO: stdout: "e2e-test-crd-publish-openapi-3937-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 23 12:57:08.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-710016297 explain e2e-test-crd-publish-openapi-3937-crds'
Nov 23 12:57:08.240: INFO: stderr: ""
Nov 23 12:57:08.240: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3937-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:57:12.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3479" for this suite.

• [SLOW TEST:9.970 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":302,"skipped":4879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:57:12.315: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:57:28.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-943" for this suite.

• [SLOW TEST:16.293 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":303,"skipped":4911,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:57:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 23 12:57:28.768: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 23 12:57:33.773: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:57:34.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6459" for this suite.

• [SLOW TEST:6.210 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":304,"skipped":4922,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Nov 23 12:57:34.819: INFO: >>> kubeConfig: /tmp/kubeconfig-710016297
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-eb57eba2-db8d-4ca3-88bf-9afd73e7a4cd
STEP: Creating a pod to test consume configMaps
Nov 23 12:57:34.979: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f" in namespace "projected-9760" to be "Succeeded or Failed"
Nov 23 12:57:34.984: INFO: Pod "pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.855855ms
Nov 23 12:57:36.988: INFO: Pod "pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009229997s
STEP: Saw pod success
Nov 23 12:57:36.988: INFO: Pod "pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f" satisfied condition "Succeeded or Failed"
Nov 23 12:57:36.992: INFO: Trying to get logs from node dvi-conformance-1606121581-vsp1-group1-0 pod pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 23 12:57:37.021: INFO: Waiting for pod pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f to disappear
Nov 23 12:57:37.025: INFO: Pod pod-projected-configmaps-85533cb7-ad01-4c5b-865f-d0a40a87431f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.3-rc.0.69+37babbd0e76c11/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Nov 23 12:57:37.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9760" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":4923,"failed":0}
SSSSSNov 23 12:57:37.043: INFO: Running AfterSuite actions on all nodes
Nov 23 12:57:37.043: INFO: Running AfterSuite actions on node 1
Nov 23 12:57:37.043: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4928,"failed":0}

Ran 305 of 5233 Specs in 5516.571 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4928 Skipped
PASS

Ginkgo ran 1 suite in 1h31m58.508731872s
Test Suite Passed
